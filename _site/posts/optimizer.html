<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Maxime Rivest">
<meta name="dcterms.date" content="2025-07-17">
<meta name="description" content="I limited deep research from OpenAI to only the DSPy repository, and it did an awesome job documenting DSPy’s optimizers">

<title>DSPy Optimizers – Parameter Structure Analysis (by deep research) – Maxime Rivest</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-532FMB8ETS"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-532FMB8ETS', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../custom.css">
<meta name="twitter:title" content="DSPy Optimizers – Parameter Structure Analysis (by deep research) – Maxime Rivest">
<meta name="twitter:description" content="I limited deep research from OpenAI to only the DSPy repository, and it did an awesome job documenting DSPy’s optimizers">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Maxime Rivest</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/maximerivest"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/@maximerivest"> <i class="bi bi-youtube" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/maximerivest"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../posts.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header">
<h1 class="title display-7">DSPy Optimizers – Parameter Structure Analysis (by deep research)</h1>
<p class="author">Maxime Rivest</p>

<p class="date">2025-07-17</p>
</header>


<p><img src="images/dr_opt.png" class="img-fluid"></p>
<p>One of my favorite things about deep research from OpenAI is that it was fine-tuned to produce long reports, so I like to use it to produce long reports almost more than to do deep research, and one of the things I just discovered that is very useful is to not give it the internet but give it a GitHub connector to a repo and then ask a question or ask it to document your repository and it will write a very long report about that.</p>
<p>I just discovered that limiting deep research from OpenAI to only a repository (using the connectors) is very effective at focusing deep research on making a complete report about your code. On this page I am sharing with you the results I got applying this to DSPy’s optimizers. Fun fact: deep research was finetuned to write longer output, so it is quite a ‘different’ model than others you would find out there. I like it for this application.</p>
<section id="summary-table" class="level2">
<h2 class="anchored" data-anchor-id="summary-table">Summary Table</h2>
<p>Below is an overview of each optimizer class in the <code>dspy.teleprompter</code> module, including their constructor (<code>__init__</code>) parameters and primary optimization method (usually <code>compile</code>) with a breakdown of positional vs.&nbsp;keyword-only arguments:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 1%">
<col style="width: 47%">
<col style="width: 51%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Optimizer Class</strong></th>
<th><strong><code>__init__</code> Parameters</strong> (positional vs.&nbsp;keyword-only)</th>
<th><strong>Core Method &amp; Parameters</strong> (positional vs.&nbsp;keyword-only)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Teleprompter</strong> (base class)</td>
<td><strong><code>__init__(self)</code></strong> – <em>no parameters</em> (just <code>self</code>).</td>
<td><strong><code>compile(self, student, *, trainset, teacher=None, valset=None)</code></strong> – <code>student</code> is positional; <code>trainset</code> is required keyword-only; <code>teacher</code> and <code>valset</code> are optional keyword-only.</td>
</tr>
<tr class="even">
<td><strong>LabeledFewShot</strong></td>
<td><strong><code>__init__(self, k=16)</code></strong> – one parameter <code>k</code> (int) with default 16 (may be given positionally or by name).</td>
<td><strong><code>compile(self, student, *, trainset, sample=True)</code></strong> – <code>student</code> is positional; <code>trainset</code> required keyword-only; <code>sample</code> optional keyword-only (default True).</td>
</tr>
<tr class="odd">
<td><strong>BootstrapFewShot</strong></td>
<td><strong><code>__init__(self, metric=None, metric_threshold=None, teacher_settings={}, max_bootstrapped_demos=4, max_labeled_demos=16, max_rounds=1, max_errors=5)</code></strong> – all parameters have defaults (callable <code>metric</code> and optional <code>metric_threshold</code> for success cutoff; <code>teacher_settings</code> dict for teacher LM config; numeric defaults for demos and rounds; <code>max_errors</code> tolerates errors). These can be passed as keywords (order is not enforced by <code>*</code> in the constructor).</td>
<td><strong><code>compile(self, student, *, teacher=None, trainset, valset=None)</code></strong> – <code>student</code> positional; <code>trainset</code> required keyword-only; <code>teacher</code> optional (default None, keyword-only); <code>valset</code> optional keyword-only.</td>
</tr>
<tr class="even">
<td><strong>BootstrapFewShotWithRandomSearch</strong></td>
<td><strong><code>__init__(self, metric, teacher_settings=None, max_bootstrapped_demos=4, max_labeled_demos=16, max_rounds=1, num_candidate_programs=16, num_threads=None, max_errors=None, stop_at_score=None, metric_threshold=None)</code></strong> – extends BootstrapFewShot with additional parameters for random search. <code>metric</code> (callable) is required (no default); others are optional (teacher_settings default None; defaults for demos and rounds as in BootstrapFewShot; <code>num_candidate_programs</code> controls number of candidate prompt sets; optional <code>num_threads</code> for parallelism; <code>max_errors</code> default None uses global setting; <code>stop_at_score</code> optional early stopping threshold; <code>metric_threshold</code> optional filter threshold).</td>
<td><strong><code>compile(self, student, *, teacher=None, trainset, valset=None, restrict=None, labeled_sample=True)</code></strong> – <code>student</code> positional; <code>trainset</code> required keyword-only; <code>teacher</code> optional keyword-only; <code>valset</code> optional keyword-only; <code>restrict</code> optional keyword-only (to restrict which candidate seeds to run); <code>labeled_sample</code> optional keyword-only (default True, whether to sample labeled demos in candidate generation).</td>
</tr>
<tr class="odd">
<td><strong>Ensemble</strong></td>
<td><strong><code>__init__(self, *, reduce_fn=None, size=None, deterministic=False)</code></strong> – all arguments are keyword-only (enforced by <code>*</code>). <code>reduce_fn</code> is a function to combine outputs (e.g.&nbsp;majority vote) defaulting to None; <code>size</code> is an optional int to sample subset of programs; <code>deterministic</code> is a bool (must be False for now, as deterministic mode not implemented).</td>
<td><strong><code>compile(self, programs)</code></strong> – takes a list of programs as a single positional argument. No trainset or metric is used here; the method returns an ensembled program that calls all (or a sampled subset of) given programs and reduces their outputs.</td>
</tr>
<tr class="even">
<td><strong>FinetuneTeleprompter</strong> (base for fine-tuning optimizers)</td>
<td><strong><code>__init__(self, train_kwargs=None)</code></strong> – one optional parameter <code>train_kwargs</code> which can be a dict of training arguments (or a dict mapping specific LM objects to their training args). Defaults to None (internally converted to a default dict). This base class doesn’t implement compile itself (inherits Teleprompter.compile which raises NotImplemented) – it is meant to be subclassed for fine-tuning behavior.</td>
<td><em>No direct <code>compile</code> method in this base class</em> – subclasses implement the optimization logic. (It inherits the abstract <code>compile</code> signature from Teleprompter but does not override it, so it cannot be used standalone.)</td>
</tr>
<tr class="odd">
<td><strong>BootstrapFinetune</strong></td>
<td><strong><code>__init__(self, metric=None, multitask=True, train_kwargs=None, adapter=None, exclude_demos=False, num_threads=None)</code></strong> – extends FinetuneTeleprompter. All arguments have defaults: <code>metric</code> (evaluation metric, default None), <code>multitask</code> (bool, True to fine-tune on combined data vs.&nbsp;per-predictor), <code>train_kwargs</code> (dict for training hyperparams, default None), <code>adapter</code> (optional Adapter or mapping for fine-tuning, default None), <code>exclude_demos</code> (bool, default False, whether to clear prompt demos after fine-tuning), <code>num_threads</code> (int, default None for using global default threads). These can be given as keywords or positionally (no <code>*</code> in signature).</td>
<td><strong><code>compile(self, student, trainset, teacher=None, valset=None, target=\"t5-large\", bsize=12, accumsteps=1, lr=5e-5, epochs=1, bf16=False, int8=False, peft=False, path_prefix=None)</code></strong> – <code>student</code> and <code>trainset</code> are accepted as positional args (unlike others, this method does not strictly enforce keyword-only for <code>trainset</code> in the code). <code>teacher</code> is optional (default None, can be passed by name); <code>valset</code> optional (default None); and a series of fine-tuning hyperparameters are provided as keyword options with defaults (<code>target</code> model name, batch size <code>bsize</code>, gradient accumulation steps <code>accumsteps</code>, learning rate <code>lr</code>, <code>epochs</code>, and flags for bf16, int8, PEFT usage, plus <code>path_prefix</code> for saving checkpoints). <em>(In practice, these would be passed as keywords; the lack of <code>*</code> means <code>trainset</code> and <code>teacher</code> could technically be given positionally, which is an inconsistency in interface.)</em></td>
</tr>
<tr class="even">
<td><strong>COPRO</strong> (Co-Prompt Optimization)</td>
<td><strong><code>__init__(self, prompt_model=None, metric=None, breadth=10, depth=3, init_temperature=1.4, track_stats=False)</code></strong> – all parameters have defaults. <code>prompt_model</code> is an LM used to generate prompt variations (defaults to the globally configured LM if None); <code>metric</code> is the evaluation metric (default None, meaning it will optimize without a specific metric filter unless provided); <code>breadth</code> (int) is how many new prompt candidates to generate per iteration (default 10); <code>depth</code> is how many iterations of prompt refinement to perform (default 3); <code>init_temperature</code> (float) for prompt generation randomness (default 1.4); <code>track_stats</code> (bool) whether to record optimization statistics (default False).</td>
<td><strong><code>compile(self, student, *, trainset, eval_kwargs)</code></strong> – <code>student</code> program is positional; <code>trainset</code> is required keyword-only; <code>eval_kwargs</code> is also required keyword-only (a dict of extra arguments for evaluation). <em>No <code>teacher</code> parameter in this optimizer</em> – instead it uses <code>prompt_model</code> internally for generating new instructions, and evaluates the student on <code>trainset</code> using the provided <code>metric</code> and eval settings.</td>
</tr>
<tr class="odd">
<td><strong>MIPROv2</strong> (Multi-Iteration Prompt Optimizer)</td>
<td><strong><code>__init__(self, metric, prompt_model=None, task_model=None, teacher_settings=None, max_bootstrapped_demos=4, max_labeled_demos=4, auto=\"light\", num_candidates=None, num_threads=None, max_errors=None, seed=9, init_temperature=0.5, verbose=False, track_stats=True, log_dir=None, metric_threshold=None)</code></strong> – a large number of parameters. Notably, <code>metric</code> is required (no default) – the primary evaluation metric. <code>prompt_model</code> and <code>task_model</code> are optional LM instances (if None, defaults to global settings for prompt generation and for executing the task, respectively). <code>teacher_settings</code> is an optional dict of LM settings for any teacher model usage (default None -&gt; <code>{}</code>). <code>max_bootstrapped_demos</code> and <code>max_labeled_demos</code> default to 4 each (controls how many few-shot examples of each type to use initially). <code>auto</code> can be <code>"light"</code>, <code>"medium"</code>, <code>"heavy"</code> or None, controlling an automatic configuration of search effort (default “light”). <code>num_candidates</code> (int, optional) specifies how many candidate prompt variations to generate (if <code>auto</code> is None, this must be set along with <code>num_trials</code>). <code>num_threads</code> optional (for parallel eval, default None). <code>max_errors</code> optional (max allowed errors during eval, default None to use global). <code>seed</code> default 9 (random seed for reproducibility). <code>init_temperature</code> (float) default 0.5 for initial prompt variation. <code>verbose</code> (bool) default False for logging. <code>track_stats</code> default True to record detailed stats. <code>log_dir</code> optional path for logging. <code>metric_threshold</code> optional float to early-discard prompts below this score threshold.</td>
<td><strong><code>compile(self, student, *, trainset, teacher=None, valset=None, num_trials=None, max_bootstrapped_demos=None, max_labeled_demos=None, seed=None, minibatch=True, minibatch_size=35, minibatch_full_eval_steps=5, program_aware_proposer=True, data_aware_proposer=True, view_data_batch_size=10, tip_aware_proposer=True, fewshot_aware_proposer=True, requires_permission_to_run=True, provide_traceback=None)</code></strong> – <code>student</code> is positional; all other parameters are keyword-only. <code>trainset</code> (list of examples) is required; <code>teacher</code> optional (defaults None, a teacher program/LM for bootstrapping if needed); <code>valset</code> optional (if provided, used for evaluation phases). This method exposes many tuning knobs: <code>num_trials</code> (total search iterations, required if <code>auto</code> mode is None), the ability to override <code>max_bootstrapped_demos</code>/<code>max_labeled_demos</code> for this run, a <code>seed</code> (if not given, uses the seed from init), and several boolean flags controlling different proposer strategies (<code>minibatch</code> evaluation vs full dataset, with <code>minibatch_size</code> and how often to fully evaluate <code>minibatch_full_eval_steps</code>; whether the prompt proposal is aware of the program structure, data distribution, etc. via <code>program_aware_proposer</code>, <code>data_aware_proposer</code>, <code>tip_aware_proposer</code>, <code>fewshot_aware_proposer</code> – all True by default). <code>view_data_batch_size</code> (int, default 10) controls how much data a proposal sees at once. <code>requires_permission_to_run</code> (bool, default True) will prompt the user before a potentially expensive run. <code>provide_traceback</code> (bool or None) toggles including stack traces in logged errors. All of these are meant to be supplied as keywords when needed (there is a <code>*</code> enforcing keyword-only) to fine-tune the search behavior.</td>
</tr>
</tbody>
</table>
<p><strong>Table Legend:</strong> Positional parameters are those that must be supplied in order (or by name), before any <code>*</code>. Keyword-only parameters (shown after <code>*</code>) can only be supplied by name (and have default values if not marked required). Defaults are shown where applicable. Each class’s core method (usually <code>compile</code>) is listed with its signature and the nature of its arguments.</p>
</section>
<section id="detailed-method-argument-analysis" class="level2">
<h2 class="anchored" data-anchor-id="detailed-method-argument-analysis">Detailed Method Argument Analysis</h2>
<p>Below we provide a class-by-class breakdown of the constructor and primary method parameters, explaining each argument, default values, and usage conventions:</p>
<section id="teleprompter-base-class" class="level3">
<h3 class="anchored" data-anchor-id="teleprompter-base-class">Teleprompter (Base Class)</h3>
<ul>
<li><p><strong>Constructor <code>Teleprompter.__init__</code>:</strong> Takes no arguments besides <code>self</code> (no parameters to configure). It’s essentially an abstract base, so no initialization parameters are needed.</p></li>
<li><p><strong>Method <code>compile(self, student, *, trainset, teacher=None, valset=None)</code>:</strong> This is meant to be overridden by subclasses. It accepts a <code>student</code> program (the DSPy program to optimize) as a positional argument. The datasets are keyword-only:</p>
<ul>
<li><code>trainset</code> (<strong>required</strong>, list of <code>Example</code>): the training examples on which to optimize.</li>
<li><code>teacher</code> (optional, default <code>None</code>): an optional teacher program used to guide optimization (if not provided, many optimizers default to using the student itself or an internal strategy).</li>
<li><code>valset</code> (optional, default <code>None</code>): an optional validation set of examples to evaluate generalization or for early stopping. All parameters after <code>student</code> are marked with <code>*</code> in the signature, making them keyword-only for clarity. The base implementation raises <code>NotImplementedError</code> (since Teleprompter itself doesn’t define a specific optimization strategy).</li>
</ul></li>
<li><p><strong>Method <code>get_params(self)</code>:</strong> (Minor utility) Returns a dictionary of the Teleprompter’s internal attributes (simply <code>self.__dict__</code>). This is a common interface to retrieve the configuration of any Teleprompter.</p></li>
</ul>
</section>
<section id="labeledfewshot" class="level3">
<h3 class="anchored" data-anchor-id="labeledfewshot">LabeledFewShot</h3>
<ul>
<li><p><strong>Constructor <code>LabeledFewShot.__init__(self, k=16)</code>:</strong> This optimizer’s only parameter is <code>k</code> – the number of examples from the trainset to label (i.e.&nbsp;use as demonstrations) per predictor. It defaults to 16. This parameter is positional-or-keyword (not forced to keyword-only), so one could call <code>LabeledFewShot(10)</code> to use 10 examples, or <code>LabeledFewShot(k=10)</code>. The value of <code>k</code> sets an upper bound on how many examples will be taken from the training data to insert as prompt demonstrations.</p></li>
<li><p><strong>Method <code>compile(self, student, *, trainset, sample=True)</code>:</strong> Optimizes the given <code>student</code> program by attaching labeled examples to it:</p>
<ul>
<li><code>student</code> – the program to optimize (positional).</li>
<li><code>trainset</code> – <strong>required</strong> keyword-only list of examples to draw demonstrations from.</li>
<li><code>sample</code> – keyword-only bool (default <code>True</code>): if True, it randomly samples <code>min(k, len(trainset))</code> examples for each predictor in the student; if False, it simply takes the first <code>k</code> examples (in order) from the trainset.</li>
</ul>
<p>The <code>compile</code> method returns a new compiled program where each predictor in the student has up to <code>k</code> example demos in its prompt. If the <code>trainset</code> is empty, it returns the student unchanged. This optimizer does not use any “teacher” or iterative improvement – it’s a one-step assignment of labeled data. All arguments after <code>student</code> are keyword-only as indicated by the <code>*</code> in the signature.</p></li>
</ul>
</section>
<section id="bootstrapfewshot" class="level3">
<h3 class="anchored" data-anchor-id="bootstrapfewshot">BootstrapFewShot</h3>
<ul>
<li><p><strong>Constructor <code>BootstrapFewShot.__init__</code>:</strong> This optimizer automatically “bootstraps” new prompt demonstrations by having the program attempt the task and collecting successful outputs as examples. Its constructor accepts several parameters, all with defaults:</p>
<ul>
<li><code>metric</code> (callable, default <code>None</code>): A function to judge success on an example (takes e.g.&nbsp;<code>(gold_example, prediction, trace)</code> and returns True/False or a score). If <code>None</code>, any output is considered a success for bootstrapping purposes.</li>
<li><code>metric_threshold</code> (float, default <code>None</code>): A score threshold for the metric – if provided, a prediction must meet or exceed this threshold to count as a successful example. (If <code>metric</code> is boolean-returning, this may not be used.) This parameter allows filtering which outputs become demonstrations.</li>
<li><code>teacher_settings</code> (dict, default <code>{}</code>): Settings to configure the behavior of the teacher model (e.g., a different language model or different decoding parameters). These settings (like temperature) will be applied to the teacher when generating outputs.</li>
<li><code>max_bootstrapped_demos</code> (int, default 4): The maximum number of <strong>bootstrapped</strong> demos (new examples generated from the model itself) to add per predictor.</li>
<li><code>max_labeled_demos</code> (int, default 16): The maximum number of <strong>labeled</strong> demos (original trainset examples) to use per predictor. This sets an upper bound on using ground-truth examples in addition to bootstrapped ones.</li>
<li><code>max_rounds</code> (int, default 1): How many bootstrapping rounds to perform. Each round can attempt to gather new demos from the model’s outputs.</li>
<li><code>max_errors</code> (int, default 5 in some implementations, or <code>None</code>): The maximum number of errors to tolerate during bootstrapping (e.g., if the student or teacher throws exceptions). If the number of errors exceeds this, the process will halt or raise. In some versions, if set to None, it may fall back on a global setting.</li>
</ul>
<p>All these parameters have default values, meaning the constructor can be called with no arguments (it will bootstrap using default settings). They are not declared as keyword-only in the signature (no leading <code>*</code> in the <code>__init__</code>), but in practice they are almost always passed by keyword for clarity.</p></li>
<li><p><strong>Method <code>compile(self, student, *, teacher=None, trainset, valset=None)</code>:</strong> This performs the bootstrapping process:</p>
<ul>
<li><code>student</code> – the program to optimize (positional). The student should initially be “uncompiled” (no demos attached).</li>
<li><code>teacher</code> – optional keyword-only. If provided, this is a separate program or model to act as the “coach” producing outputs; if None, the student itself (or a copy) is used as the teacher by default. The teacher is typically a copy of the student (or a version with different settings) that generates candidate outputs.</li>
<li><code>trainset</code> – required keyword-only list of examples for training. The teleprompter will run each example through the teacher (or student) to see if it can get a correct output.</li>
<li><code>valset</code> – optional keyword-only list of examples for validation (default None). If provided, it may be used after bootstrapping to evaluate or select prompts (in the basic <code>BootstrapFewShot</code>, it’s not heavily used; it often defaults to using any remaining train examples not successfully bootstrapped as a validation list).</li>
</ul>
<p><strong>Process:</strong> The compile method will:</p>
<ol type="1">
<li>Make a fresh copy of the <code>student</code> (ensuring the original remains unchanged) and also prepare a <code>teacher</code> copy.</li>
<li>If <code>max_labeled_demos &gt; 0</code> and the teacher program isn’t already compiled with demos, it first uses a <code>LabeledFewShot</code> teleprompter to supply up to <code>max_labeled_demos</code> ground-truth examples to the teacher (so the teacher starts with some baseline demos).</li>
<li>It then iterates through the <code>trainset</code>, using the teacher to generate predictions. For each example, if the prediction is “successful” according to the <code>metric</code> (or if no metric provided), it will extract the input/output pair from the execution trace and add it as a new demo example (a bootstrapped demo) for the student’s corresponding predictor.</li>
<li>It stops once it has collected <code>max_bootstrapped_demos</code> successful demos or has exhausted the training data (or completed <code>max_rounds</code> passes). Any training examples not “bootstrapped” successfully may remain as a validation set.</li>
<li>Finally, it calls an internal <code>_train()</code> which assembles the final set of demos for each predictor: it takes the bootstrapped demos collected and, if there’s still room (up to <code>max_labeled_demos</code> total), it may fill in some of the original trainset examples as well. The resulting student (with demos attached) is marked as compiled and returned.</li>
</ol>
<p>All arguments after <code>student</code> are keyword-only, enforcing calls like <code>teleprompter.compile(student=prog, trainset=data)</code> for clarity. This is consistent with the base Teleprompter signature. The presence of both <code>teacher_settings</code> in the constructor and an optional <code>teacher</code> in compile means you configure how the teacher behaves up front (e.g., use a different model or temperature via settings), and you can also supply a specific teacher program if desired at compile time.</p></li>
</ul>
</section>
<section id="bootstrapfewshotwithrandomsearch" class="level3">
<h3 class="anchored" data-anchor-id="bootstrapfewshotwithrandomsearch">BootstrapFewShotWithRandomSearch</h3>
<ul>
<li><p><strong>Constructor <code>BootstrapFewShotWithRandomSearch.__init__</code>:</strong> This class builds on <code>BootstrapFewShot</code> to not only bootstrap demos but also perform a random search over multiple candidate prompt sets. It inherits from <code>Teleprompter</code> (and in newer versions, it extends <code>BootstrapFewShot</code>) and introduces additional parameters:</p>
<ul>
<li><code>metric</code> (callable, <strong>no default</strong> in signature): Similar to BootstrapFewShot, this is the evaluation metric. In this class, <code>metric</code> is effectively required – the absence of a default indicates the user should supply one (the random search needs a way to compare programs). (If not provided, it might default to using the truthy evaluation of outputs if the metric function is None, but typically one provides a metric).</li>
<li><code>teacher_settings</code> (dict, default None): Same role as in BootstrapFewShot – configuration for the teacher’s LM behavior. If None, an empty dict is used internally.</li>
<li><code>max_bootstrapped_demos</code> (int, default 4), <code>max_labeled_demos</code> (int, default 16), <code>max_rounds</code> (int, default 1): Same meaning as in BootstrapFewShot (limits on demos and bootstrap iterations).</li>
<li><code>num_candidate_programs</code> (int, default 16): The number of candidate programs (prompt configurations) to evaluate in the random search. This class will generate and test up to this many variations of prompts.</li>
<li><code>num_threads</code> (int, default None): If set, this can be used to parallelize evaluation of candidates (e.g., number of threads for the Evaluate calls). If None, it might default to a global setting or single-threaded evaluation.</li>
<li><code>max_errors</code> (int, default None): Maximum errors tolerated (similar to BootstrapFewShot; if None, use global setting). This applies during each candidate evaluation as well.</li>
<li><code>stop_at_score</code> (float, default None): If provided, the search will stop early if it finds a candidate with a metric score greater or equal to this threshold.</li>
<li><code>metric_threshold</code> (float, default None): A threshold applied during the bootstrapping phase for considering a trace successful (similar to BootstrapFewShot’s metric_threshold).</li>
</ul>
<p>All these arguments have defaults except <code>metric</code>, and they are typically passed by keyword. In the code, none are forced keyword-only at <strong>init</strong>, but practically one would use keywords for clarity due to the number of parameters.</p></li>
<li><p><strong>Method <code>compile(self, student, *, teacher=None, trainset, valset=None, restrict=None, labeled_sample=True)</code>:</strong> This performs an extended random search on top of bootstrapping:</p>
<ul>
<li><code>student</code> – the program to optimize (positional).</li>
<li><code>teacher</code> – optional keyword-only teacher program (default None) as in BootstrapFewShot.</li>
<li><code>trainset</code> – required keyword-only training examples.</li>
<li><code>valset</code> – optional keyword-only validation set (defaults to using <code>trainset</code> if not provided, as seen in code where <code>self.valset = valset or trainset</code>).</li>
<li><code>restrict</code> – optional keyword-only (default None). This can be used to restrict which candidate indices/seeds to run. Internally, this optimizer uses different random seeds (including some special values like -3, -2, -1 for baseline variants) to generate candidate prompt sets; the <code>restrict</code> parameter can specify a subset of these seeds to actually evaluate (useful for debugging or partial searches).</li>
<li><code>labeled_sample</code> – optional keyword-only bool (default True). This is passed into the LabeledFewShot step for the seed that uses labeled examples only. If <code>True</code>, it randomly samples labeled demos; if <code>False</code>, it takes the first examples (just as in LabeledFewShot’s compile).</li>
</ul>
<p><strong>Process:</strong> The compile method goes through a sequence of candidate evaluations (using different <code>seed</code> values to shuffle the trainset and vary the demos):</p>
<ol type="1">
<li><p>It considers a set of candidate prompt configurations:</p>
<ul>
<li><code>seed = -3</code>: a zero-shot baseline (no demos at all).</li>
<li><code>seed = -2</code>: a baseline with labeled examples only (uses <code>LabeledFewShot</code> to attach up to <code>max_labeled_demos</code> demos).</li>
<li><code>seed = -1</code>: an “unshuffled” few-shot bootstrap (runs BootstrapFewShot with the trainset in given order).</li>
<li><code>seed &gt;= 0</code>: a number of random shuffles. For each seed from 0 up to <code>num_candidate_programs-1</code>, it shuffles a copy of the trainset and picks a random number of bootstrapped demos (between 1 and <code>max_bootstrapped_demos</code>) to gather, then runs BootstrapFewShot with those settings.</li>
</ul></li>
<li><p>For each candidate, it uses <code>Evaluate</code> to compute the overall metric score on either the <code>valset</code> or training set for that compiled program. It keeps track of the scores.</p></li>
<li><p>It applies adjustments for any assertion-based failures (specific to DSPy, e.g., if the program has internal assertion checks) – see the section subtracting for <code>_suggest_failures</code> and zeroing out if <code>_assert_failures</code>.</p></li>
<li><p>It identifies the best-scoring program and can stop early if <code>stop_at_score</code> was specified and achieved.</p></li>
<li><p>Finally, it attaches a list of all candidate programs and their scores to the best program (<code>best_program.candidate_programs</code>) for reference, and returns the best program.</p></li>
</ol>
<p>All parameters after <code>student</code> are keyword-only, aligning with the interface of BootstrapFewShot (trainset must be named, etc.). This optimizer’s interface is more complex, but the use of keyword-only helps avoid confusion when calling <code>compile</code> with many optional settings. One idiosyncrasy: the <code>compile</code> method itself uses the internal <code>BootstrapFewShot</code> class for seeds -1 and &gt;=0, thereby inheriting any parameters set in the constructor like <code>metric_threshold</code> or <code>teacher_settings</code> and reusing them for each candidate search.</p></li>
</ul>
</section>
<section id="ensemble" class="level3">
<h3 class="anchored" data-anchor-id="ensemble">Ensemble</h3>
<ul>
<li><p><strong>Constructor <code>Ensemble.__init__(self, *, reduce_fn=None, size=None, deterministic=False)</code>:</strong> The Ensemble teleprompter does not deal with datasets or metrics at all – instead, it creates an ensemble from multiple programs. All its parameters are keyword-only (notice the leading <code>*,</code> in the signature):</p>
<ul>
<li><code>reduce_fn</code> (callable, default None): A function that takes a list of outputs (one from each program in the ensemble) and reduces them to a single output. For example, DSPy provides <code>dspy.majority</code> to pick the most common answer, which is a typical choice for classification tasks. If <code>reduce_fn</code> is None, the ensemble’s <code>forward</code> will return the list of all outputs.</li>
<li><code>size</code> (int, default None): If set, the ensemble will randomly select <code>size</code> programs out of the provided list each time it is called, rather than using all programs. If None, it uses all programs each time.</li>
<li><code>deterministic</code> (bool, default False): If True, the ensemble would aim to produce deterministic behavior (e.g., always pick the same subset for a given input). Currently, this is not implemented (the code asserts that <code>deterministic is False</code>).</li>
</ul>
<p>These parameters allow controlling how the ensemble combines multiple models’ outputs. All must be passed by keyword, e.g., <code>Ensemble(reduce_fn=dspy.majority, size=5)</code>.</p></li>
<li><p><strong>Method <code>compile(self, programs)</code>:</strong> Instead of optimizing prompts, this teleprompter combines programs. The <code>programs</code> argument is a list of DSPy programs to ensemble, passed as a single positional argument. There are no trainset or metric arguments. The method returns a new <code>EnsembledProgram</code> (constructed internally) which, when called, will:</p>
<ul>
<li>If <code>size</code> is specified, randomly sample that many programs from the list; otherwise use all programs.</li>
<li>Invoke each selected program’s <code>__call__</code> (or <code>forward</code>) on the given inputs.</li>
<li>Collect their outputs, and then either apply the <code>reduce_fn</code> if provided or return the list of outputs as-is.</li>
</ul>
<p>The <code>compile</code> here is straightforward: it doesn’t “learn” or modify the programs, just wraps them. Notably, there is no keyword-only enforcement in this signature, because it only takes one argument (<code>programs</code>). The usage is simply <code>ensemble_teleprompter.compile([prog1, prog2, ...])</code>. This class is an outlier in that it doesn’t use any of the training data or metric infrastructure – it’s purely a structural optimizer.</p></li>
</ul>
</section>
<section id="finetuneteleprompter-base-class-for-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="finetuneteleprompter-base-class-for-fine-tuning">FinetuneTeleprompter (Base Class for Fine-tuning)</h3>
<ul>
<li><p><strong>Constructor <code>FinetuneTeleprompter.__init__(self, train_kwargs=None)</code>:</strong> This base class is designed for optimizers that fine-tune language model weights. It introduces a single configuration parameter:</p>
<ul>
<li><code>train_kwargs</code> (dict or dict-of-dicts, default None): Training arguments for fine-tuning. It can be one dictionary applied to all LMs, or a mapping from specific <code>LM</code> objects to their respective parameter dicts. For example, this might include learning rate, number of epochs, etc. If None, it defaults to an empty configuration. Internally, the constructor converts this into a standard form (using <code>convert_to_lm_dict</code>) where each LM maps to its own settings (even if the same settings are used for all).</li>
</ul>
<p>This class does not take a metric in its constructor – because often fine-tuning might use the training loss as implicit metric, or the metric can be applied on a validation set externally. It primarily encapsulates how to call the underlying LM’s fine-tune method. <code>FinetuneTeleprompter</code> doesn’t implement a new <code>compile</code> itself – it relies on child classes to implement the strategy. After construction, it holds a <code>train_kwargs</code> mapping that will be used during fine-tune calls.</p></li>
<li><p><strong>No direct <code>compile</code> method:</strong> <code>FinetuneTeleprompter</code> inherits the abstract <code>compile</code> from Teleprompter but does not override it, so it can’t be used on its own. Subclasses (like <code>BootstrapFinetune</code>) will implement the actual compile logic. Essentially, <code>FinetuneTeleprompter</code> serves to store training configurations and provide utility methods (in the DSPy code, e.g., <code>finetune_lms</code> static method in the newer implementation, or <code>convert_to_lm_dict</code>). Think of it as an abstract base similar to Teleprompter, but specifically for fine-tuning optimizers, ensuring they handle <code>train_kwargs</code> uniformly.</p></li>
</ul>
</section>
<section id="bootstrapfinetune" class="level3">
<h3 class="anchored" data-anchor-id="bootstrapfinetune">BootstrapFinetune</h3>
<ul>
<li><p><strong>Constructor <code>BootstrapFinetune.__init__</code>:</strong> This class combines bootstrapping with actual fine-tuning of an LM. It inherits from <code>FinetuneTeleprompter</code>. Its parameters are as follows:</p>
<ul>
<li><code>metric</code> (callable, default None): An optional metric function to evaluate model outputs (similar to other teleprompters). If provided, it can be used to judge which outputs are “successful” when bootstrapping data or to guide the selection of fine-tuning data. If None, all outputs might be considered or a default (like always True) is used.</li>
<li><code>multitask</code> (bool, default True): Whether to fine-tune on all tasks/predictors jointly (<code>True</code>) or separately (<code>False</code>). If <code>multitask=True</code>, all data from all predictors might be combined to fine-tune a single model (or one model per unique LM); if False, it will fine-tune separate models for each predictor (the code sets data indices accordingly).</li>
<li><code>train_kwargs</code> (dict or dict-of-LM dicts, default None): Passed to the base FinetuneTeleprompter to configure fine-tuning (learning rate, epochs, etc.). If a plain dict is given, the same settings apply to all language models; a more granular mapping can specify different hyperparameters per LM.</li>
<li><code>adapter</code> (Adapter or dict of LMs to Adapter, default None): An optional specification of an <em>adapter</em> to use for fine-tuning (e.g., for parameter-efficient fine-tuning). If provided, this indicates which fine-tuning method or adapter to use for each LM. Internally converted to a dict mapping each LM to an Adapter (using a similar technique to <code>train_kwargs</code>).</li>
<li><code>exclude_demos</code> (bool, default False): If True, after fine-tuning it will clear out any prompt demonstrations in the predictors (perhaps under the assumption that the model has learned from them and they are no longer needed). If False, it leaves any demos in place. In the code, after fine-tuning, they actually set <code>pred.demos = []</code> if <code>exclude_demos</code> is True.</li>
<li><code>num_threads</code> (int, default None): Number of threads for parallel fine-tuning jobs. If you have multiple predictors to fine-tune (e.g., multitask=False scenario or multiple LMs in a program), this sets how many can run in parallel. It defaults to None, which means use the global default (or 1 if not set).</li>
</ul>
<p>All these parameters have defaults, so you can call <code>BootstrapFinetune()</code> with none, and it will use a multitask approach with whatever global LM is configured. The signature does not enforce keyword-only, but given the number of parameters, using keywords is strongly recommended for clarity (e.g., <code>BootstrapFinetune(metric=my_metric, epochs=2)</code> etc., though <code>epochs</code> would actually go inside <code>train_kwargs</code> in this design).</p></li>
<li><p><strong>Method <code>compile(self, student, trainset, teacher=None, valset=None, target="t5-large", bsize=12, accumsteps=1, lr=5e-5, epochs=1, bf16=False, int8=False, peft=False, path_prefix=None)</code>:</strong> This is a two-phase optimizer: it first bootstraps prompt examples, then fine-tunes the model on those examples. Its signature is notably different in that it does not strictly require <code>trainset</code> to be passed as a keyword (there is no <code>*</code> before <code>trainset</code> in the current implementation’s signature, meaning <code>student</code> and <code>trainset</code> could be given positionally). However, to avoid confusion, it’s often called with keywords for clarity. The parameters are:</p>
<ul>
<li><p><code>student</code> – the program to optimize (positional).</p></li>
<li><p><code>trainset</code> – the list of examples to train on (positional or keyword). These will be used both for bootstrapping prompts and as the fine-tuning dataset.</p></li>
<li><p><code>teacher</code> – optional (default None). A teacher program or list of programs. If provided, those will be used to bootstrap examples; if None, it will issue a warning that it’s using an uncompiled student as teacher. Often, one might pass a copy of the student or a differently configured model as the teacher for the bootstrap step.</p></li>
<li><p><code>valset</code> – optional validation set (default None). Not extensively used inside the compile method for Bootstrapping (the code primarily uses <code>trainset</code> for bootstrapping and doesn’t explicitly use <code>valset</code> in fine-tuning, though it could be used to evaluate during training or after).</p></li>
<li><p><strong>Fine-tuning hyperparameters:</strong> These are all optional with defaults, and they mirror typical HuggingFace/transformers fine-tuning settings:</p>
<ul>
<li><code>target</code> (str, default <code>"t5-large"</code>): The model name or identifier to fine-tune. This class may instantiate a fresh model of this type for fine-tuning or use it as an identifier to save the fine-tuned weights.</li>
<li><code>bsize</code> (int, default 12): Batch size for fine-tuning.</li>
<li><code>accumsteps</code> (int, default 1): Gradient accumulation steps.</li>
<li><code>lr</code> (float, default 5e-5): Learning rate for fine-tuning.</li>
<li><code>epochs</code> (int, default 1): Number of fine-tuning epochs.</li>
<li><code>bf16</code> (bool, default False): Whether to use bfloat16 precision.</li>
<li><code>int8</code> (bool, default False): Whether to use int8 quantization for fine-tuning (likely requires an adapter that supports it).</li>
<li><code>peft</code> (bool, default False): Whether to use a PEFT (Parameter-Efficient Fine Tuning) method (like LoRA). If True, the fine-tuning will use an adapter method rather than full model tuning.</li>
<li><code>path_prefix</code> (str, default None): An optional prefix path for saving fine-tuned model checkpoints. If provided, the fine-tuned model weights are saved under this path with a generated name.</li>
</ul></li>
</ul>
<p>The compile process is as follows:</p>
<ol type="1">
<li><strong>Bootstrap Phase:</strong> It uses an internal <code>self.teleprompter</code>, which is a <code>BootstrapFewShot</code> instance configured in <code>__init__</code> (with <code>max_bootstrapped_demos</code> very high and <code>max_labeled_demos=0</code> by default in some implementations), to compile the student (or teacher) with bootstrapped demonstrations. Essentially, it generates a set of demonstrations by running the teacher (or student) on the trainset and collecting successful outputs (using the given <code>metric</code> if provided). This yields a compiled program with demos.</li>
<li>It then prepares fine-tuning data: for each predictor in the compiled program, it takes all the demos (input-output pairs) and formats them into prompt-completion training examples appropriate for the language model fine-tuning. The code constructs prompt text and target text from each demo using the predictor’s signature/template, accumulating them in a list.</li>
<li>It shuffles the fine-tuning data and writes it to disk as a <code>.jsonl</code> file (or multiple files if multitask vs per-predictor).</li>
<li><strong>Fine-tuning Phase:</strong> It invokes a fine-tuning routine (likely <code>finetune_hf</code> for HuggingFace models) on the prepared data for the specified <code>target</code> model, with the given hyperparameters (<code>batch_size</code>, <code>epochs</code>, <code>lr</code>, etc.). This produces fine-tuned model checkpoint(s).</li>
<li>It loads these fine-tuned weights into the student’s predictors – replacing their <code>lm</code> with the fine-tuned model(s). If <code>multitask=True</code>, typically one model is fine-tuned for all (assuming a shared LM); if False, each predictor might get its own fine-tuned model. The code ensures the structure matches and assigns the new LMs.</li>
<li>If <code>exclude_demos=True</code>, it clears the <code>demos</code> for each predictor (since the model is now supposed to handle the task without needing prompt examples).</li>
<li>The method marks the program as compiled and returns the fine-tuned compiled program.</li>
</ol>
<p>Key points: The <code>trainset</code> here is used both to bootstrap examples and to generate the fine-tuning dataset, effectively turning successful model outputs into training data (this is a form of self-training). The presence of both metric-based bootstrapping and actual gradient descent is unique to this optimizer. The interface inconsistency is that <code>trainset</code> is not forced to keyword-only (likely an oversight), whereas most others require naming it. Best practice is to call it as <code>teleprompter.compile(student, trainset=..., teacher=..., epochs=..., lr=..., ...)</code> for clarity. All the fine-tuning hyperparameters are keyword-only by position (they come after the required args and <code>*</code> in the function definition), meaning in code you must call them as named arguments (which is natural for these settings).</p></li>
</ul>
</section>
<section id="copro-co-prompt-optimizer" class="level3">
<h3 class="anchored" data-anchor-id="copro-co-prompt-optimizer">COPRO (Co-Prompt Optimizer)</h3>
<ul>
<li><p><strong>Constructor <code>COPRO.__init__</code>:</strong> COPRO aims to optimize the <em>instructions</em> in a prompt by iterative generation and testing. Its parameters:</p>
<ul>
<li><code>prompt_model</code> (LM client, default None): The language model used to propose new instructions. If None, the system likely defaults to the same model as the student (or whatever is set in global settings). By providing a separate <code>prompt_model</code>, you could use a larger or more creative model to generate prompt variants while using a different <code>task_model</code> (the student) for execution.</li>
<li><code>metric</code> (callable, default None): The metric to evaluate the student’s performance. If None, COPRO can still run, but it might not have a quantitative way to compare prompts – in practice, a metric should be supplied so it can choose the best prompt.</li>
<li><code>breadth</code> (int, default 10): The number of new prompt candidates to generate at each iteration (each “depth”). Essentially, in each round COPRO will produce this many alternative instructions via the <code>prompt_model</code>.</li>
<li><code>depth</code> (int, default 3): The number of iterations (rounds of prompt generation and evaluation) to perform. A depth of 3 means it will generate new instructions 3 times, each time possibly building on or replacing previous ones.</li>
<li><code>init_temperature</code> (float, default 1.4): The temperature setting for the prompt generation model in the initial generation round (higher temperature means more randomness/creativity). This influences the diversity of prompts generated. In the code, this temperature might be used for <code>prompt_model</code> when sampling instructions.</li>
<li><code>track_stats</code> (bool, default False): Whether to collect statistics about the optimization process. If True, COPRO will record details such as the distribution of scores for prompts at each iteration (min, max, avg, std of top prompts, etc.). These stats would be stored in attributes like <code>results_best</code>, <code>results_latest</code>, etc., on the returned program for analysis.</li>
</ul>
<p>All of these parameters are keyword-only by design (note the <code>*,</code> in the <code>__init__</code> signature in code) – meaning you must call, for example, <code>COPRO(metric=..., breadth=20)</code>. This enforces clarity given the number of optional arguments.</p></li>
<li><p><strong>Method <code>compile(self, student, *, trainset, eval_kwargs)</code>:</strong> COPRO’s compile differs from previous ones in that it doesn’t attach demos or fine-tune weights, but instead <em>alters the prompt instructions</em> of the student’s predictors. Parameters:</p>
<ul>
<li><code>student</code> – the program to optimize (positional). This program likely contains one or more predictors with an instruction (prompt template) that we want to improve.</li>
<li><code>trainset</code> – required keyword-only list of examples. These will be used to evaluate the quality of instructions. Essentially, for each candidate prompt, COPRO will run the student on the trainset and measure performance.</li>
<li><code>eval_kwargs</code> – required keyword-only dict of arguments for evaluation. This is passed to DSPy’s <code>Evaluate</code> to evaluate the student on the trainset. For example, <code>eval_kwargs</code> might specify <code>num_threads</code> for parallel evaluation or <code>display_progress</code> flags. It’s mandatory to provide (the code does not have a default), ensuring the user is explicit about how to evaluate (e.g., <code>eval_kwargs={"display_progress": False}</code> or with specific settings).</li>
</ul>
<p><strong>Process:</strong> In simplified terms, COPRO will:</p>
<ol type="1">
<li><p>Make a deepcopy of the <code>student</code> to work on (so as not to modify the original mid-process).</p></li>
<li><p>Evaluate the initial student on the trainset to get a baseline score (not explicitly shown in snippet, but likely done implicitly as part of loop or for stats tracking).</p></li>
<li><p>For each iteration (up to <code>depth</code>):</p>
<ul>
<li><p>Use the <code>prompt_model</code> to generate <code>breadth</code> new candidate instructions for each predictor. The generation likely uses one of two Signature classes defined in the code:</p>
<ul>
<li><code>BasicGenerateInstruction</code> if it’s the first round (which just takes the original instruction and asks for an improved one).</li>
<li><code>GenerateInstructionGivenAttempts</code> if it’s after the first round (which provides some of the previously tried instructions and their scores to the prompt model, so it can propose a better one).</li>
</ul></li>
<li><p>For each predictor in the student program, replace its instruction with each of the candidate instructions one at a time and evaluate the program on the trainset using the metric (via <code>Evaluate</code> with <code>eval_kwargs</code>).</p></li>
<li><p>Track the performance of each candidate. If <code>track_stats</code> is True, record the stats of these candidates (min, max, etc.).</p></li>
<li><p>Possibly filter out duplicate or very similar instructions (the code has <code>_drop_duplicates</code> to eliminate repeated candidates that yield the same results).</p></li>
<li><p>Select the top-performing instruction(s) to carry forward. Likely it keeps the best one as the new base instruction (and possibly uses others for context in subsequent rounds).</p></li>
</ul></li>
<li><p>Repeat for the specified number of depths. By the end, ideally, the student’s predictors have improved instructions that yield better metric performance on the trainset.</p></li>
<li><p>Return the optimized program (with its instruction updated to the best found). If <code>track_stats</code> was True, the returned program might have attributes like <code>results_best</code> and <code>results_latest</code> containing the recorded statistics.</p></li>
</ol>
<p>All parameters after <code>student</code> are keyword-only, so one would call <code>teleprompter.compile(student=prog, trainset=data, eval_kwargs=eval_args)</code>. The absence of a <code>teacher</code> parameter here is notable – COPRO doesn’t use a separate teacher model to generate outputs for evaluation; instead, it uses a separate <code>prompt_model</code> to generate <em>prompts</em> (instructions), and the original program (or its LM, possibly configured via <code>teacher_settings</code> if any) to evaluate those prompts. Essentially, COPRO is searching in prompt/instruction space, guided by metric evaluations on the trainset.</p></li>
</ul>
</section>
<section id="miprov2" class="level3">
<h3 class="anchored" data-anchor-id="miprov2">MIPROv2</h3>
<ul>
<li><p><strong>Constructor <code>MIPROv2.__init__</code>:</strong> MIPRO (“Mixed Initiative Prompt Optimization”, perhaps) is one of the most complex teleprompters, combining few-shot bootstrapping, instruction proposal, and hyperparameter search. Its initialization has many parameters, mostly optional, to cover various aspects of the search:</p>
<ul>
<li><code>metric</code> (callable, <strong>required</strong>): The evaluation metric to maximize. Unlike many others, MIPROv2 does not default <code>metric</code> to None – you must provide a metric function. This makes sense given the complexity: it needs a quantitative measure to drive the optimization.</li>
<li><code>prompt_model</code> (LM, default None): Similar to COPRO, an optional separate model used to propose instructions or other prompt components. If None, defaults to the globally configured LM (or the student’s LM).</li>
<li><code>task_model</code> (LM, default None): If the student program uses a particular LM, <code>task_model</code> can override or specify it. If None, it uses <code>dspy.settings.lm</code> (the globally configured default LM) as the model to actually run the task. Essentially, <code>task_model</code> is the model that executes the prompts (the “student’s LM”), and <code>prompt_model</code> is the model that generates new prompt candidates; they could be different.</li>
<li><code>teacher_settings</code> (dict, default None): Similar to earlier teleprompters, this can hold settings for any teacher or evaluation model usage. MIPRO does some bootstrapping internally, so this could configure how that’s done. Internally, if None, it stores as an empty <code>{}</code>.</li>
<li><code>max_bootstrapped_demos</code> (int, default 4): The initial number of bootstrapped few-shot examples to gather (per predictor) for use in prompts.</li>
<li><code>max_labeled_demos</code> (int, default 4): The initial number of labeled (ground-truth) examples to include per predictor. (Notice this default is 4, smaller than the 16 used in simpler teleprompters, possibly to limit scope for the automated search).</li>
<li><code>auto</code> (Literal “light”/“medium”/“heavy” or None, default “light”): This is a high-level switch to configure how exhaustive the search should be. If set to “medium” or “heavy”, the teleprompter will automatically set or override other parameters (like number of trials, etc.) to spend more effort. If <code>auto=None</code>, the user must manually specify certain parameters (like <code>num_trials</code>). The allowed values are enforced; any other string would raise an error.</li>
<li><code>num_candidates</code> (int, default None): The number of candidate solutions (e.g., prompt combinations) to consider in the search. If <code>auto</code> is None, this must be provided (along with <code>num_trials</code>) or an error is raised. If <code>auto</code> is set, <code>num_candidates</code> should not be provided (it would be overridden by the auto settings).</li>
<li><code>num_threads</code> (int, default None): Number of threads for parallel operations (like evaluation). If None, falls back to global setting.</li>
<li><code>max_errors</code> (int, default None): Max errors to tolerate; if None, use global setting (similar usage as before).</li>
<li><code>seed</code> (int, default 9): Random seed for reproducibility. Used for shuffling and any stochastic decisions.</li>
<li><code>init_temperature</code> (float, default 0.5): Initial temperature for any prompt generation or sampling (lower than COPRO’s default, implying more conservative generation).</li>
<li><code>verbose</code> (bool, default False): If True, provides more logging info during the process.</li>
<li><code>track_stats</code> (bool, default True): Whether to collect and store statistics of the optimization (like how COPRO does). By default True, so it will track performance of trials, etc.</li>
<li><code>log_dir</code> (str, default None): If provided, the directory path to save logs or intermediate results (like candidate programs, evaluations).</li>
<li><code>metric_threshold</code> (float, default None): Similar to earlier, a threshold for the metric to perhaps prune or consider a trial successful. If set, any candidate with metric below this might be discarded or considered failing.</li>
</ul>
<p>The constructor sets a lot of these into internal attributes and does some validation: e.g., ensures if <code>auto</code> is not None, the user hasn’t also set <code>num_candidates</code> or <code>num_trials</code> (to avoid conflict), and if <code>auto</code> is None, then both <code>num_candidates</code> and <code>num_trials</code> must be specified by the user. It also immediately converts <code>teacher_settings</code> to an empty dict if None and assigns default models if <code>prompt_model</code> or <code>task_model</code> are None. All parameters except <code>metric</code> have defaults, but given their number, they are meant to be given by keyword (the signature includes no <code>*</code> here, but practically one would hardly pass 15 args positionally in order). The ordering places <code>metric</code> first (required), then the two models, then other settings.</p></li>
<li><p><strong>Method <code>compile(self, student, *, trainset, teacher=None, valset=None, num_trials=None, max_bootstrapped_demos=None, max_labeled_demos=None, seed=None, minibatch=True, minibatch_size=35, minibatch_full_eval_steps=5, program_aware_proposer=True, data_aware_proposer=True, view_data_batch_size=10, tip_aware_proposer=True, fewshot_aware_proposer=True, requires_permission_to_run=True, provide_traceback=None)</code>:</strong> This signature is expansive, but all arguments after <code>student</code> are keyword-only (enforced by the <code>*</code>). Here’s what they mean:</p>
<ul>
<li><p><code>student</code> – the program to optimize (positional).</p></li>
<li><p><code>trainset</code> – required keyword-only list of examples to train/optimize on.</p></li>
<li><p><code>teacher</code> – optional keyword-only (default None). If provided, used during the bootstrap of few-shot examples (similar to BootstrapFewShot’s teacher). If None, the student (or rather its <code>task_model</code>) is used to bootstrap itself.</p></li>
<li><p><code>valset</code> – optional keyword-only list of examples for validation (default None). MIPRO uses a validation set to evaluate candidate prompts (distinct from trainset if provided) and for final evaluation of each trial. If not provided, it may split the trainset or use part of it for validation implicitly.</p></li>
<li><p><code>num_trials</code> – optional keyword-only (int). The number of search trials to run. If <code>auto</code> is None, this must be set (and should correspond roughly to <code>num_candidates</code> and the effort desired). If <code>auto</code> is “light”/“medium”/“heavy<code>,</code>num_trials` will be determined internally (and providing it will raise an error).</p></li>
<li><p><code>max_bootstrapped_demos</code>, <code>max_labeled_demos</code> – optional ints to override the defaults for this compile run. If provided, they will update the internal <code>max_bootstrapped_demos</code>/<code>max_labeled_demos</code> before running. Otherwise, it uses the values from the constructor (which might have been set via auto mode).</p></li>
<li><p><code>seed</code> – optional int to override the random seed for this run (if not provided, uses the seed from init). This allows one to repeat the search with different seeds or ensure reproducibility.</p></li>
<li><p><code>minibatch</code> (bool, default True): Whether to use minibatch evaluation when scoring prompts. If True, and the validation set is large, MIPRO will evaluate in batches rather than all at once (to speed up or simulate iterative evaluation). If False, it evaluates on the full <code>valset</code> every time.</p></li>
<li><p><code>minibatch_size</code> (int, default 35): The number of examples to use in each minibatch evaluation if <code>minibatch</code> is True. It will evaluate candidate programs on chunks of this many examples and possibly use an average or intermediate pruning strategy.</p></li>
<li><p><code>minibatch_full_eval_steps</code> (int, default 5): If using minibatch mode, this could indicate how frequently (in terms of trial count or iterations) a full evaluation on the entire <code>valset</code> is done, or how many minibatch steps constitute a “full” eval for logging. (This parameter’s use is a bit advanced; it might define after how many partial batches to do a full evaluation or something similar.)</p></li>
<li><p>The next several are boolean flags controlling <em>proposers</em> – these determine what aspects of the prompt the algorithm is allowed to propose changes for:</p>
<ul>
<li><code>program_aware_proposer</code> (default True): If True, the optimizer will propose modifications aware of the program’s structure (likely meaning it can consider changes to instructions in context of entire program).</li>
<li><code>data_aware_proposer</code> (default True): If True, proposals might take into account the data distribution or particularities of examples (perhaps by examining some examples during instruction proposals).</li>
<li><code>view_data_batch_size</code> (int, default 10): Possibly the number of examples the proposers can look at at once when generating suggestions (if data-aware).</li>
<li><code>tip_aware_proposer</code> (default True): “Tip” could refer to a part of prompt (like a prefix or a suffix). If True, the proposer can adjust the “tip” (maybe the output field prefix or few-shot separators).</li>
<li><code>fewshot_aware_proposer</code> (default True): If True, the proposer can adjust few-shot examples or how they’re used (since MIPRO also handles bootstrapped demos).</li>
</ul></li>
<li><p><code>requires_permission_to_run</code> (bool, default True): If True, the compile will prompt the user for confirmation before running a potentially expensive search (especially in heavy mode). If set to False, it will run to completion without interactive confirmation.</p></li>
<li><p><code>provide_traceback</code> (bool or None, default None): If True, any errors encountered might include tracebacks in the logs; if False, suppress tracebacks; if None, use a default setting (perhaps false). This is mainly for debugging if something goes wrong during evaluation, which can be helpful when <code>verbose</code> logging.</p></li>
</ul>
<p><strong>Process:</strong> MIPROv2’s compile is very comprehensive. Summarizing:</p>
<ol type="1">
<li><p><strong>Few-shot Bootstrapping:</strong> It likely begins by ensuring the student has some initial demos. There is a call <code>demo_candidates = self._bootstrap_fewshot_examples(program, trainset, seed, teacher)</code> which presumably uses <code>max_bootstrapped_demos</code> and <code>max_labeled_demos</code> to produce a set of demonstration candidates (similar to BootstrapFewShot but perhaps generating multiple sets).</p></li>
<li><p><strong>Instruction Proposal:</strong> Then it calls <code>_propose_instructions(...)</code> which uses the <code>prompt_model</code> to propose new instructions, possibly taking into account the current program, the data, and the demo candidates. The parameters like <code>view_data_batch_size</code>, <code>program_aware_proposer</code>, etc., influence this step – e.g., it might generate instructions while seeing a batch of <code>view_data_batch_size</code> examples or not.</p></li>
<li><p>If zero-shot optimization is indicated (no demos allowed, <code>zeroshot_opt</code>), it may discard demos to focus purely on instructions.</p></li>
<li><p><strong>Prompt Parameter Optimization:</strong> It then calls <code>_optimize_prompt_parameters(...)</code> – this likely orchestrates the main search over trials (<code>num_trials</code>). In each trial, it might:</p>
<ul>
<li>Choose a set of demos (from <code>demo_candidates</code>, possibly none if zero-shot) and an instruction (from <code>instruction_candidates</code> proposed) to form a candidate program (a specific configuration of prompts).</li>
<li>Evaluate that program on the <code>valset</code> using the metric (the code uses an <code>Evaluate</code> instance for the <code>valset</code> with the given metric and threads).</li>
<li>Use something like Optuna (since the code imports <code>optuna</code> if available) to intelligently choose the next combination of parameters to try (the “Bayesian” or guided search aspect).</li>
<li>Possibly prune low-performing trials early (since the code has integration for pruning via intermediate minibatch evaluation).</li>
<li>Repeat until <code>num_trials</code> are done or the search converges.</li>
</ul></li>
<li><p>It likely uses the <code>auto</code> setting to determine <code>num_trials</code> and possibly adjust <code>minibatch</code> usage. For example, “heavy” auto might set a large number of trials and larger validation set size.</p></li>
<li><p>If <code>requires_permission_to_run=True</code>, before starting the full search, it will print an estimate of how many LM calls or how long it might take and prompt the user to continue. If the user declines, it aborts and returns the original student unchanged.</p></li>
<li><p>Throughout, it tracks the best program found. At the end, it returns the optimized program (with improved instructions and possibly with selected demos attached). It also attaches logs like <code>trial_logs</code> containing the score of each trial and the parameters used, as well as possibly storing in <code>student._compiled = True</code>.</p></li>
</ol>
<p>The key feature of MIPROv2 is that it integrates multiple dimensions: it can optimize the instruction text (like COPRO), the selection of few-shot examples (like BootstrapFewShot), and even other prompt parameters (e.g., it might experiment with presence or absence of demos – that’s why it has both <code>fewshot_aware_proposer</code> and code logic for zero-shot vs few-shot). It effectively generalizes and combines ideas from the simpler teleprompters. Because of this, its interface is the most complex. All those boolean flags allow turning on/off certain aspects of the search:</p>
<ul>
<li>e.g., one could run it with <code>program_aware_proposer=False</code> to ignore program structure differences when proposing instructions, or <code>minibatch=False</code> to always evaluate on full validation set (safer but slower).</li>
</ul>
<p>As with other teleprompters, <code>trainset</code> and other main parameters are keyword-only to prevent mix-ups. The <code>compile</code> method is clearly intended to be called with named arguments for anything beyond the basics (e.g., <code>teleprompter.compile(student=prog, trainset=data, valset=dev, num_trials=50, fewshot_aware_proposer=False, requires_permission_to_run=False)</code>). The consistency in using keyword-only here is welcome given how many tuning knobs exist.</p></li>
</ul>
</section>
</section>
<section id="patterns-and-idiosyncrasies" class="level2">
<h2 class="anchored" data-anchor-id="patterns-and-idiosyncrasies">Patterns and Idiosyncrasies</h2>
<p>Examining all these optimizers, we can observe several <strong>patterns</strong> in how parameters are structured, as well as some inconsistencies or outliers:</p>
<ul>
<li><p><strong>Common Structure – “compile” with trainset:</strong> Almost every optimizer uses a <code>compile(student, *, ... trainset ..., ...)</code> method to perform the optimization on a given program and dataset. Requiring <code>trainset</code> as a keyword-only argument is a common design (seen in Teleprompter base, LabeledFewShot, BootstrapFewShot, RandomSearch, COPRO, MIPRO). This pattern enforces clarity that a training set must be provided and avoids accidental swapping of positional arguments. An inconsistency here is <strong>BootstrapFinetune</strong>, whose <code>compile</code> signature does <strong>not</strong> enforce keyword-only for <code>trainset</code> (it takes <code>student, trainset</code> positionally). This makes BootstrapFinetune stand out as allowing <code>compile(prog, data)</code> without naming <code>trainset</code>, whereas others would require <code>compile(prog, trainset=data)</code>. It’s likely an oversight in that implementation because the conceptual pattern is that <code>trainset</code> should be keyword-only for all.</p></li>
<li><p><strong>Positional vs Keyword-only in Constructors:</strong> The base classes (Teleprompter, FinetuneTeleprompter) and some simple ones have very few parameters and thus no need for keyword-only in <code>__init__</code>. E.g., Teleprompter and FinetuneTeleprompter have none or one parameter and don’t use <code>*</code>. But <strong>Ensemble</strong> explicitly uses <code>*</code> to force its three parameters (<code>reduce_fn, size, deterministic</code>) to be keyword-only in the constructor. This is a design choice to improve readability: calling <code>Ensemble(size=3, reduce_fn=majority)</code> is self-documenting, versus relying on positional order. Other optimizers like <strong>BootstrapFewShot, BootstrapFewShotWithRandomSearch, BootstrapFinetune, COPRO, MIPROv2</strong> did not enforce <code>*</code> in their <code>__init__</code>, despite having many parameters. This means in theory one could call <code>BootstrapFewShot(None, {}, 4, 16, 1)</code> positionally, but that would be very unclear. In practice, users likely call <code>BootstrapFewShot(metric=my_metric, max_rounds=2, ...)</code>. The lack of uniform use of keyword-only in constructors is an inconsistency. A pattern is that newer or more user-facing classes (Ensemble, perhaps MIPRO if it was considered user-facing) lean towards keyword-only for clarity, whereas older classes did not enforce it.</p></li>
<li><p><strong>Parameter Naming Conventions:</strong></p>
<ul>
<li><p>Most classes use <code>trainset</code> and (optionally) <code>valset</code> consistently to refer to data. This is uniform across optimizers.</p></li>
<li><p>The use of <code>teacher</code> vs <code>teacher_settings</code> is a bit confusing across classes:</p>
<ul>
<li><p><strong>BootstrapFewShot</strong> and RandomSearch have a <code>teacher_settings</code> in the constructor (for LM config) and a <code>teacher</code> argument in compile (for an actual program instance).</p></li>
<li><p><strong>BootstrapFinetune</strong> similarly takes an <code>adapter</code> (similar concept to teacher settings, but specific to fine-tuning) in constructor and a <code>teacher</code> in compile.</p></li>
<li><p><strong>MIPROv2</strong> uses <code>teacher_settings</code> in constructor (to adjust the teacher LM) and <code>teacher</code> in compile.</p></li>
<li><p><strong>LabeledFewShot</strong> and <strong>Ensemble</strong> do not involve a teacher at all.</p></li>
<li><p><strong>COPRO</strong> does not have a <code>teacher</code> parameter either; instead it has <code>prompt_model</code> and uses the student’s own execution for evaluation. Inconsistency arises in naming: e.g., <strong>BootstrapFewShotWithRandomSearch</strong> reuses <code>teacher_settings</code> from its parent and has <code>teacher</code> in compile, whereas <strong>FinetuneTeleprompter/BootstrapFinetune</strong> introduced a separate concept of <code>adapter</code> and <code>train_kwargs</code> for fine-tuning. These serve a similar role (configuring how the “teaching” or training is done) but under different names. Also, in <strong>MIPROv2</strong>, there is both <code>teacher_settings</code> and a <code>teacher</code> argument, plus separate <code>prompt_model</code> and <code>task_model</code>. This can be conceptually hard to follow:</p>
<ul>
<li><code>teacher</code> generally means an alternate DSPy program or LM used to <em>generate outputs for bootstrapping</em>.</li>
<li><code>teacher_settings</code> means a dictionary of parameters to apply to whichever model is acting as teacher (like setting its temperature or max tokens).</li>
<li><code>prompt_model</code> is an LM used for generating new prompt text (distinct from the task).</li>
<li><code>adapter</code> in finetuning is an object encapsulating how to fine-tune (distinct from anything in non-finetune classes). Ideally, the interface could be cleaner if, for example, every Teleprompter had a <code>teacher</code> argument in compile (for a program or LM) and possibly a unified way to specify how that teacher should behave (maybe always via <code>teacher_settings</code>). Currently it’s partly unified (teacher + teacher_settings) in bootstrap classes, but fine-tune adds adapter, and COPRO/MIPRO add prompt_model separately. This is an area of inconsistency in naming and usage.</li>
</ul></li>
</ul></li>
<li><p><strong>Metric and Threshold:</strong> Every optimizer that evaluates outputs uses a <code>metric</code> parameter name for the evaluation function. This is consistent. Some optimizers (BootstrapFewShot, RandomSearch, MIPRO) also use <code>metric_threshold</code> as an optional cutoff for success. The concept of <code>metric_threshold</code> is not present in others like Finetune or COPRO (COPRO could theoretically use it but doesn’t expose it; Finetune focuses on loss). The inconsistent part is documentation vs implementation: e.g., the official docs for BootstrapFewShot did not list <code>metric_threshold</code> or <code>max_errors</code>, yet the code and random search clearly use them. This indicates either a new feature that wasn’t documented or a parameter considered more internal. As a pattern, many classes allow a <code>None</code> metric to mean “no filtering, just optimize blindly” and some threshold to refine what “success” means.</p></li>
<li><p><strong>Demo-related parameters:</strong> We see repeated parameters controlling number of examples:</p>
<ul>
<li><code>k</code> in LabeledFewShot.</li>
<li><code>max_bootstrapped_demos</code> and <code>max_labeled_demos</code> in BootstrapFewShot, RandomSearch, MIPRO. These generally default to some small numbers (4 and 16, or 4 and 4 in MIPRO). The choice of 4/16 vs 4/4 is inconsistent. Possibly, earlier versions assumed up to 16 labeled demos is fine (for simpler tasks or lots of data), whereas MIPRO’s authors might have found using 16 made the search space too large or wasn’t needed, and so they reduced both defaults to 4. It’s an inconsistency in default tuning: two classes aimed at similar goals have different default for max labeled demos (16 vs 4). Similarly, <strong>LabeledFewShot</strong> and <strong>BootstrapFewShot</strong> share the 16 default for labeled demos (and LabeledFewShot’s sole param k=16 aligns with BootstrapFewShot’s 16), whereas MIPRO diverges.</li>
</ul></li>
<li><p><strong>Parallelism parameters:</strong> <code>num_threads</code> appears in BootstrapFewShotWithRandomSearch, BootstrapFinetune, MIPRO, but not in plain BootstrapFewShot or LabeledFewShot. The base Evaluate class in DSPy likely uses a global thread count if not specified. The newer/complex optimizers expose <code>num_threads</code> to give the user control over parallel evaluations. This is a pattern of evolving design: earlier optimizers didn’t surface this (assuming either single-thread or using global config), later ones made it explicit. So there’s inconsistency across classes – e.g., one can’t directly set threads in BootstrapFewShot without going through dspy.settings, but one can in RandomSearch via the teleprompter’s param.</p></li>
<li><p><strong>Boolean flags for features:</strong> Some advanced optimizers (MIPRO) have many boolean flags to toggle sub-behaviors (program_aware_proposer, etc.), whereas simpler ones bake in one strategy. This reflects differing complexity: simpler optimizers don’t have these flags at all. It’s expected, but it means the interface isn’t uniform – MIPRO stands out with a very large signature and lots of optional toggles, compared to something like BootstrapFewShot which has a concise interface. From a consistency standpoint, MIPRO’s interface might be overwhelming relative to others.</p></li>
</ul></li>
<li><p><strong>Use of <code>*</code> in method signatures:</strong> As noted, <strong>almost all compile methods</strong> use <code>*</code> to separate <code>student</code> (positional) from the rest (keyword-only). This is a clear pattern for compile. The only exceptions:</p>
<ul>
<li>BootstrapFinetune’s compile, which did not put a <code>*</code> before <code>teacher</code> and <code>trainset</code> in the older implementation. (Documentation suggests there might be a version that does, but the code we saw treats <code>teacher</code> as positional after student, which is unusual).</li>
<li>Ensemble.compile doesn’t use <code>*</code> simply because it has a single argument. This pattern – having the dataset and other settings be keyword-only – is generally followed and is good for clarity. The inconsistency in BootstrapFinetune is likely something to correct for uniformity.</li>
</ul></li>
<li><p><strong>Public Method Names (step vs compile):</strong> All these optimizers use a method named <code>compile</code> as the entry point to perform optimization, rather than something like <code>step()</code> or <code>optimize()</code>. The user question mentioned “methods such as step or optimize,” but in DSPy’s design it appears <code>compile</code> is the standard name (compiling a program with a teleprompter means optimizing it). None of the classes have a public method literally named <code>step</code> or <code>optimize</code> – they all stick to <code>compile()</code>. Internally, some have helper methods (<code>_bootstrap_one_example</code>, <code>_train</code>, etc.) but those are private. So there is consistency in using <code>compile</code> as the interface method, inherited from Teleprompter. The only slight oddity is Ensemble using compile in a non-learning sense, but still logically “compiling an ensemble program.”</p></li>
<li><p><strong>Outlier Classes:</strong></p>
<ul>
<li><p><strong>Ensemble</strong> is quite different in purpose (no metric, no trainset). It still fits the Teleprompter interface (taking programs and returning a program), but its parameter set (reduce_fn, deterministic, etc.) doesn’t overlap with others. It’s an idiosyncratic case included in the same module for convenience.</p></li>
<li><p><strong>FinetuneTeleprompter</strong> as a base class is a bit of an abstraction layer not exposed to end-users typically. It doesn’t quite act on its own. This is an internal consistency: Teleprompter vs FinetuneTeleprompter both serving as abstract bases for two families (prompt-based vs fine-tune-based optimizers). They share the interface but introduce different init params (none vs train_kwargs). A slight inconsistency is that Teleprompter base has no init params, FinetuneTeleprompter does – but that’s due to the nature of fine-tuning needing configuration up front.</p></li>
<li><p><strong>COPRO and MIPRO</strong> introduce parameter names not seen elsewhere (e.g., <code>breadth</code>, <code>depth</code>, <code>auto</code>, all the proposer flags). They were likely developed later to tackle prompt optimization more holistically. They still follow patterns like requiring trainset and using metric, but add their own twist. COPRO, for instance, doesn’t accept <code>teacher</code> or use <code>max_rounds</code> – instead it has <code>depth</code> for iterations of prompt proposals, essentially analogous but specific to its domain. MIPRO aggregates parameters from many others, making it quite an outlier in complexity.</p></li>
</ul></li>
<li><p><strong>Defaults and Range of Values:</strong> Many numeric defaults seem somewhat ad-hoc but within a small range:</p>
<ul>
<li>4 and 16 appear frequently (suggesting maybe at most 4 bootstrapped examples or 16 labeled examples as a reasonable default).</li>
<li>Max rounds default to 1 in bootstrap (a single iteration is often enough to get some improvement).</li>
<li>RandomSearch defaults to 16 candidate programs (which aligns with maybe trying seeds -3, -2, -1 and 0..12 – indeed in code they loop <code>range(-3, num_candidate_sets)</code> which for 16 gives seeds -3..15 inclusive, that’s 19, but likely they intended a fixed count; perhaps the special negatives are not counted in that num).</li>
<li>Finetuning hyperparams default to typical values like 1 epoch, batch 12, lr 5e-5 – those mirror common practice in ML.</li>
<li>The <code>auto="light"</code> default in MIPRO suggests they wanted the safer, quicker configuration by default.</li>
</ul>
<p>The inconsistencies here are minor – just that some defaults might not align (e.g., if one expected MIPRO to default to the same 16 labeled demos as simpler teleprompters, they’d be surprised it’s 4). Another example: LabeledFewShot vs BootstrapFewShot default k=16 vs max_labeled_demos=16 (consistent), but Bootstrapped demos default 4 vs Labeled default 16 in simple version, whereas MIPRO uses 4 for both – possibly to balance that it will do iterative improvements.</p></li>
<li><p><strong>Error handling and user interaction parameters:</strong> Some newer classes have parameters related to robustness:</p>
<ul>
<li><code>max_errors</code> is present in BootstrapFewShot and RandomSearch (to avoid infinite loops or crashes if too many errors occur). Others like Finetune don’t expose <code>max_errors</code> (though Evaluate inside might use a global max error).</li>
<li>MIPRO uses <code>requires_permission_to_run</code> to ensure the user is aware of resource cost; no other class does something like that (likely because MIPRO can be very expensive). This is a unique design consideration for an outlier.</li>
<li><code>provide_traceback</code> is similarly only in MIPRO, aimed at debugging – indicating MIPRO expects potentially long runs where silent failures would be frustrating.</li>
<li>Ensemble asserts if <code>deterministic=True</code> because it’s not implemented, which is a bit user-unfriendly (they could have just not offered the parameter or documented that it’s a future feature). This is an idiosyncrasy in Ensemble’s interface (exposing a param that only throws an error if set True).</li>
</ul></li>
</ul>
<p>In summary, <strong>patterns</strong> include the consistent use of a <code>compile</code> method with <code>student</code> + keyword-only datasets/metrics, the presence of metric functions in most, and repeated use of parameters controlling how many examples to use or generate. <strong>Idiosyncrasies</strong> and inconsistencies include differences in keyword-only enforcement, slight naming mismatches (<code>teacher_settings</code> vs <code>adapter</code> vs separate model params), differences in default values for similar concepts, and the sheer divergence in complexity between simpler teleprompters (LabeledFewShot, BootstrapFewShot) and the complex ones (MIPRO, COPRO).</p>
<p>Each optimizer class was likely developed to extend functionality, which led to some divergence in interface. For example, COPRO and MIPRO added new kinds of parameters (depth, breadth, auto, etc.) that don’t appear in earlier classes, making the overall module less uniform.</p>
</section>
<section id="recommendations-for-unifying-the-interface" class="level2">
<h2 class="anchored" data-anchor-id="recommendations-for-unifying-the-interface">Recommendations for Unifying the Interface</h2>
<p>To improve consistency and usability across these teleprompter optimizers, we suggest the following changes:</p>
<ol type="1">
<li><p><strong>Enforce Keyword-Only for Key Parameters:</strong> Ensure that in all optimizers, important parameters like <code>trainset</code>, <code>teacher</code>, and other configuration options are keyword-only. This means adding <code>*,</code> where missing (e.g., in <code>BootstrapFinetune.compile</code> to require naming <code>trainset</code> and <code>teacher</code>, and in any constructor where positional use could be confusing). A uniform rule could be: <em>any optimizer method that takes a dataset or multiple optional settings should use keyword-only args beyond the program argument</em>. This will prevent mistakes and make code more self-documenting.</p></li>
<li><p><strong>Standardize Teacher Configuration:</strong> Unify the approach to teacher models across classes:</p>
<ul>
<li>Always use a <code>teacher</code> argument in <code>compile</code> for providing an alternate program or LM for generating outputs (as is done in BootstrapFewShot, etc.), and consistently use a <code>teacher_settings</code> (or similarly named) parameter in the constructor to configure that teacher’s behavior. For fine-tuning, instead of introducing a separate <code>adapter</code> parameter, consider treating it analogously (e.g., a <code>teacher_settings</code> could include an adapter or fine-tune specific config). If that’s too abstract, at least rename <code>adapter</code> to something like <code>finetune_adapter</code> and document it as the analog of teacher settings but for fine-tune.</li>
<li>If <code>prompt_model</code> and <code>task_model</code> (as in MIPRO) are essentially playing roles of teacher vs student, clarify that or even rename them to <code>teacher_model</code> and <code>student_model</code> for consistency. Alternatively, provide a unified interface where Teleprompter base could accept something like <code>teacher=...</code> in init or compile that could be a model or program. Having multiple parameters (<code>prompt_model</code>, <code>task_model</code>, <code>teacher</code>) is confusing; consolidating where possible would help (e.g., maybe define that <code>teacher</code> can be either a full DSPy Program or a raw LM; if the latter, treat it as the model to generate prompts).</li>
<li>Essentially, reduce the terminology: decide on either “teacher” or specific terms, and use them consistently. If the role is to generate new prompts, maybe call it <code>generator_model</code> everywhere instead of <code>prompt_model</code> in one place and implicitly using teacher in another. Consistency in naming would reduce user confusion.</li>
</ul></li>
<li><p><strong>Unify Metric Handling:</strong> Make sure the role of <code>metric</code> and <code>metric_threshold</code> is consistently implemented and documented:</p>
<ul>
<li>If <code>metric_threshold</code> is supported in some optimizers (BootstrapFewShot, RandomSearch, MIPRO), consider supporting it in others that might benefit (or explicitly excluding it). At least document it uniformly. It might be useful in COPRO too (maybe to decide if a prompt is “good enough”). If it’s an advanced feature, ensure all classes that use metrics either accept <code>metric_threshold</code> or none of them do. As it stands, a user might not realize BootstrapFewShot accepts a <code>metric_threshold</code> because it wasn’t in official docs, which is a documentation inconsistency.</li>
<li>Similarly, if <code>max_errors</code> is a common safeguard, consider exposing it in all relevant optimizers (for example, COPRO and MIPRO do handle errors but not via a parameter; they rely on global settings or internal logic). It might be good to allow the user to set <code>max_errors</code> in MIPRO too for consistency, or state clearly that it uses the global <code>dspy.settings.max_errors</code>. Unifying this across classes (all teleprompters either take a <code>max_errors</code> or none do and it’s purely global) would avoid confusion.</li>
</ul></li>
<li><p><strong>Align Default Values and Ranges:</strong> Review the default values for parameters that serve similar purposes and align them unless there’s a strong reason not to:</p>
<ul>
<li>For example, the default <code>max_labeled_demos</code> in MIPROv2 is 4 whereas in BootstrapFewShot it’s 16. If 16 was found to be too high in practice, perhaps all classes should default to 4 for consistency (or vice versa if 16 is preferred for thoroughness). Choose one philosophy (fewer demos vs more) and apply it uniformly so users have a consistent expectation.</li>
<li>Likewise, ensure that if an optimization class is essentially a generalization of another, its defaults should not dramatically conflict. MIPROv2 is like a superset of BootstrapFewShot + COPRO; one would expect that if you use MIPROv2 in a “minimal” way, it might by default behave somewhat like a BootstrapFewShot (just with added capabilities). That could mean defaulting <code>max_labeled_demos=16</code> as in BootstrapFewShot for a fair comparison, or at least documenting why it’s different.</li>
<li>Another default to align: LabeledFewShot’s <code>k=16</code> vs BootstrapFewShot’s <code>max_labeled_demos=16</code> (those match), but if any divergence occurs in future, keep them in sync.</li>
<li>If possible, use the same default <code>num_threads</code> behavior – e.g., default None meaning use <code>dspy.settings.num_threads</code>. Document that consistently so users know None implies some global or single-thread. Right now, it’s implied but not always explicitly stated in each class docs.</li>
</ul></li>
<li><p><strong>Refine and Simplify Interfaces of Complex Classes:</strong> For very complex optimizers like MIPROv2 (and to a lesser extent COPRO), consider grouping some of the less commonly changed hyperparameters into a config object or using **kwargs to pass through to internal methods. As it stands, the <code>compile</code> signature of MIPROv2 is extremely long, which can be intimidating. Some ideas:</p>
<ul>
<li>Group the proposer-related booleans into one structure or prefix them clearly. For example, instead of five separate flags, one could have a single <code>proposers=dict(program_aware=True, data_aware=True, tip_aware=True, fewshot_aware=True)</code> or similar. This way the signature is shorter and it’s clear they belong together. Or provide a simpler toggle that sets a combination of them (e.g., a mode for proposers).</li>
<li>The <code>minibatch</code>, <code>minibatch_size</code>, <code>minibatch_full_eval_steps</code> could perhaps be combined or managed by the <code>auto</code> mode. If <code>auto</code> is heavy, maybe always use full eval (minibatch=False). Document or enforce such relationships to reduce what the user must consider. If not grouping, at least document in one place how they interact (some of which the code does via errors).</li>
<li>Another approach: provide preset configurations for MIPRO (like how <code>auto</code> does) but maybe even expose them at a higher level rather than lots of individual args. For instance, an <code>auto="heavy"</code> sets many underlying defaults. Perhaps include in docs or interface something like <code>MIPROv2.heavy()</code> as an alternate constructor classmethod to preconfigure, etc. This doesn’t change parameters per se, but helps users not have to tweak each one. This is more of a usability suggestion beyond just parameter format.</li>
</ul>
<p>While these suggestions don’t unify across all classes (since simpler ones don’t need it), they do make the outlier interfaces easier to handle, which indirectly unifies the <em>experience</em>. A user switching from BootstrapFewShot to MIPROv2 wouldn’t want to worry about 10 new parameters if not needed; having reasonable defaults and grouping helps.</p></li>
<li><p><strong>Consistent Documentation and Naming</strong>: Ensure that the documentation (docstrings or user guides) for each optimizer class follows a consistent template:</p>
<ul>
<li><p>List out positional and keyword-only arguments explicitly, and use the same terminology for similar things (e.g., always call them “bootstrapped demos” vs sometimes “augmented demos” etc., to avoid confusion).</p></li>
<li><p>If a parameter is effectively doing the same thing across classes, use the same name. For example, <strong>if</strong> we decide <code>teacher_settings</code> is the term, then perhaps <code>adapter</code> in BootstrapFinetune could be encompassed by <code>teacher_settings</code> as well (it could have keys for adapter vs others) or be renamed to something like <code>finetune_settings</code>. Right now the names <code>teacher_settings</code>, <code>train_kwargs</code>, and <code>adapter</code> all refer to configuration of the “optimization process or model” beyond just metric and data. A unified naming (maybe a generic <code>config</code> dict or breaking them into clearer categories) would help. For instance:</p>
<ul>
<li><code>teacher_settings</code> could be expanded to handle fine-tuning specifics (not ideal semantic fit), or</li>
<li>use <code>train_kwargs</code> for all cases of LM training/hyperparameters (so BootstrapFewShot might not need it, but FinetuneTeleprompter does, and maybe MIPRO could reuse <code>train_kwargs</code> for consistency instead of burying fine-tune params in compile).</li>
</ul></li>
<li><p>The goal is that a user reading the docs doesn’t have to guess that “adapter” in one class serves a role analogous to “teacher_settings” in another. If they truly are different in nature, clarify that in docs or choose distinct naming that reflects purpose (e.g., <code>lm_adapter</code> vs <code>teacher_lm_settings</code> might clarify one is for fine-tuning method, one for prompting method).</p></li>
</ul></li>
<li><p><strong>Unify Process Flow Where Possible:</strong> While not directly about parameters, making sure each Teleprompter clearly states its two main phases (if any) in a similar way could help unify understanding. For instance, all compile methods could follow a pattern in documentation: “Preprocess (e.g., prepare student/teacher), Optimize (via bootstrapping or search), Post-process (attach demos or fine-tune weights)”. If the interface and documentation emphasize these stages similarly, users can map parameters to each stage (e.g., max_rounds -&gt; relates to optimization loop, exclude_demos -&gt; relates to post-process). Right now, each class’s documentation is isolated; a unified narrative would make the parameter sets feel more coherent.</p></li>
</ol>
<p>By implementing these recommendations, the teleprompter optimizers would have a <strong>more consistent interface</strong>. For example, a user could expect that <strong>every</strong> optimizer’s <code>compile</code> is called with <code>student=... , trainset=... , teacher=... , valset=...</code> (where relevant) without worrying about positional quirks, and that if they see a parameter like <code>max_x_demos</code> or <code>num_threads</code>, it means the same general concept across the board. It would reduce the learning curve when moving from one optimizer to another and lower the chance of misuse due to inconsistent conventions.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/maximerivest\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions"><ul><li><a href="https://github.com/MaximeRivest/maximerivest-blog/edit/main/posts/optimizer.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div></div></footer></body></html>