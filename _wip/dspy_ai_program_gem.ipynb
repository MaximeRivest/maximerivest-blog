{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5202e335-607a-4668-a68d-f3f21cb1686e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"How to build and Automatically branching Chat with DSPy\"\n",
    "date: 2025-07-07\n",
    "author: Maxime Rivest\n",
    "description: \"DSPy is particularly useful for making use of AI generation straight back in you code. As a way to demonstrate, we will build a chat application that has no single conversation. Where your message go will be decided automatically by AI.\"\n",
    "draft: false\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-location: right\n",
    "    code-tools: true\n",
    "    reference-location: margin\n",
    "title-block-banner: false\n",
    "title-block-style: none\n",
    "execute:\n",
    "  echo: true  \n",
    "  #cache: true\n",
    "  #freeze: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2767df12-9b9f-4547-b66f-3e935e0974b3",
   "metadata": {},
   "source": [
    "![](images/conversation_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d5b791-fd79-4ff9-a4b8-87ce95c43e33",
   "metadata": {},
   "source": [
    "::: {.callout-tip collapse=\"true\"}\n",
    "## Setting up\n",
    "\n",
    "For this tutorial, you will need to install a few libraries and an llm. The llm will be the one that organises the chat and the one you talk to. If you use a locally hosted model (you can!) you can skip the setting up of the api key [Click here to get a key visit](https://console.groq.com/keys).\n",
    "\n",
    "For this tutorial, I have choosen Kimi-K2 hosted by Groq. This is pretty cheap, very fast, and pretty smart! \n",
    "\n",
    "::: {.callout-note icon=false appearance=\"simple\" collapse=\"true\"}\n",
    "## python library requirements\n",
    "I like to use uv to install my libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "856ce6fd-193b-407f-8826-0511cb3c50ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.10 environment at: /home/maxime/Projects/maximerivest-blog/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m88 packages\u001b[0m \u001b[2min 293ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 24ms\u001b[0m\u001b[0m                                               \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manytree\u001b[0m\u001b[2m==2.13.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "#| code-fold: false\n",
    "#| code-summary: \"\"\n",
    "!uv pip install dspy networkx pyvis anytree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8146b6-dfe5-4cb6-86cd-9b40e04708a5",
   "metadata": {},
   "source": [
    ":::\n",
    "\n",
    "::: {.callout-note icon=false appearance=\"simple\" collapse=\"true\"}\n",
    "## api key setup\n",
    "I generally setup my key permanently but you can also do this to set it up just for here and now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b469c565-09b5-46e2-a1a2-9aef14511066",
   "metadata": {},
   "source": "::: {.callout-note icon=false appearance=\"simple\" collapse=\"true\"}\n## Make GROQ_API_KEY permanent\n\n###### Linux / macOS\nAppend to your shell start-up file (pick the one you actually use):\n\n```bash\necho \"export GROQ_API_KEY='gsk_[REDACTED]'\" >> ~/.bashrc\n# or ~/.zshrc, ~/.profile, etc.\nsource ~/.bashrc   # reload once\n```\n\n###### Windows â€“ CMD\n```cmd\nsetx GROQ_API_KEY \"gsk_[REDACTED]\"\n```\nClose and reopen the terminal.\n\n###### Windows â€“ PowerShell\n```powershell\n[Environment]::SetEnvironmentVariable(\"GROQ_API_KEY\", \"gsk_[REDACTED]\", \"User\")\n```\nRefresh with `refreshenv` or open a new window.\n:::"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0d38b5-f0a4-48d5-a777-c6ff342a9cae",
   "metadata": {},
   "outputs": [],
   "source": "#| output: false\n#| eval: false\n# import os\n# os.environ[\"GROQ_API_KEY\"] = \"gsk_[REDACTED]\""
  },
  {
   "cell_type": "markdown",
   "id": "a96ca200-2c5b-4060-976c-f2d4f51d697a",
   "metadata": {},
   "source": [
    ":::\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09964c-ee27-4f83-a3c0-4a305a82aa12",
   "metadata": {},
   "source": [
    "To build an automaticaly branching chat we will need many pieces.\n",
    "\n",
    "1. We need a data structure that will old the chat-tree.\n",
    "2. We need a conversation router that will decide where the users prompt get connected in the tree\n",
    "3. We need an interface for the user to chat\n",
    "4. We need a way to build back the relevant conversation trace into an llm ready conversation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e0bc5-d645-4856-8135-e74ead2855d8",
   "metadata": {},
   "source": [
    "::: {.callout-note appearance=\"simple\"}\n",
    "\n",
    "In this, tutorial we focus on the AI programming part. As such, our implementation will be far from optimal computationally.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d335b7d-bbf6-4041-bc01-a4486ce2f297",
   "metadata": {},
   "source": [
    "Let's first think of the data structure. For this tutorial, we will keep it simple. It will\n",
    "be a list of 'turns', each turns will point to their parent turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b983806d-dac8-491e-bebf-f39cf03371e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "class Turn(pydantic.BaseModel):\n",
    "    turn_id: int\n",
    "    parent_turn_id: Optional[int]\n",
    "    user: str\n",
    "    assistant: str\n",
    "    children_ids: List[int] = pydantic.Field(default_factory=list)\n",
    "\n",
    "turn_i = Turn(\n",
    "    turn_id = 0, \n",
    "    parent_turn_id = None, \n",
    "    user = \"Help me understand gravity.\",\n",
    "    assistant = \"Gravity is the force that pulls any two pieces of matter toward each other. On Earth, it gives objects weight and keeps us on the ground. In space, it keeps the Moon orbiting Earth and the planets orbiting the Sun. According to Einstein, massive objects actually bend the fabric of space-time, and what we feel as gravity is simply objects following the curved paths created by that bending.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a8d3d35-82c2-47a2-ac9a-6ae2310121d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"turn_id\": 0,\n",
      "  \"parent_turn_id\": null,\n",
      "  \"user\": \"Help me understand gravity.\",\n",
      "  \"assistant\": \"Gravity is the force that pulls any two pieces of matter toward each other. On Earth, it gives objects weight and keeps us on the ground. In space, it keeps the Moon orbiting Earth and the planets orbiting the Sun. According to Einstein, massive objects actually bend the fabric of space-time, and what we feel as gravity is simply objects following the curved paths created by that bending.\",\n",
      "  \"children_ids\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(turn_i.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9111c475-e19f-4ddb-8937-0e2516d59e6d",
   "metadata": {},
   "source": [
    "We will be using the tree in the image above to develop our programs.\n",
    "\n",
    "Here we are creating a conversation tree object to help us find tips, roots, and collect turns from a tip until a certain depth. If you follow along, you will need to copy paste and run them but you do not need to understand them to understand the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d34eccb3-1f5f-434d-ba8d-4698a1257cd4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: 'defining the ConversationTree object'\n",
    "import pydantic\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "class Turn(pydantic.BaseModel):\n",
    "    turn_id: int\n",
    "    parent_turn_id: Optional[int]\n",
    "    user: str\n",
    "    assistant: str\n",
    "    children_ids: List[int] = pydantic.Field(default_factory=list)\n",
    "\n",
    "class ConversationTree:\n",
    "    def __init__(self):\n",
    "        self.turns: Dict[int, Turn] = {}\n",
    "\n",
    "    def add_turn(self, turn: Turn):\n",
    "        self.turns[turn.turn_id] = turn\n",
    "        if turn.parent_turn_id is not None:\n",
    "            parent_turn = self.turns[turn.parent_turn_id]\n",
    "            parent_turn.children_ids.append(turn.turn_id)\n",
    "\n",
    "    def get_turn(self, turn_id: int) -> Turn:\n",
    "        return self.turns[turn_id]\n",
    "\n",
    "    def get_root_turns(self) -> List[Turn]:\n",
    "        return [turn for turn in self.turns.values() if turn.parent_turn_id is None]\n",
    "\n",
    "    def get_leaf_turns(self) -> List[Turn]:\n",
    "        return [turn for turn in self.turns.values() if len(turn.children_ids) == 0]\n",
    "\n",
    "    def trace_upward(self, turn_id: int, depth: int = 4) -> List[Turn]:\n",
    "        trace = []\n",
    "        current = self.get_turn(turn_id)\n",
    "        while current and len(trace) < depth:\n",
    "            trace.append(current)\n",
    "            if current.parent_turn_id is not None:\n",
    "                current = self.get_turn(current.parent_turn_id)\n",
    "            else:\n",
    "                break\n",
    "        return trace[::-1]  # reverse to get root to leaf order\n",
    "\n",
    "    def trace_downward(self, turn_id: int, depth: int = 4) -> List[List[Turn]]:\n",
    "        traces = []\n",
    "\n",
    "        def dfs(current_id, current_trace):\n",
    "            if len(current_trace) == depth:\n",
    "                traces.append(current_trace[:])\n",
    "                return\n",
    "            current_turn = self.get_turn(current_id)\n",
    "            if not current_turn.children_ids:\n",
    "                traces.append(current_trace[:])\n",
    "                return\n",
    "            for child_id in current_turn.children_ids:\n",
    "                dfs(child_id, current_trace + [self.get_turn(child_id)])\n",
    "\n",
    "        dfs(turn_id, [self.get_turn(turn_id)])\n",
    "        return traces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c89ef05b-5f7b-4a3e-b4b2-2471913a8181",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: 'fulling up the ConversationTree object'\n",
    "\n",
    "conversation_tree = ConversationTree()\n",
    "\n",
    "conversations = [\n",
    "    Turn(turn_id=0, parent_turn_id=None, user=\"Help me understand gravity.\", assistant=\"Gravity is the force...\"),\n",
    "    Turn(turn_id=1, parent_turn_id=0, user=\"What's the difference between Newton's and Einstein's theories of gravity?\", assistant=\"Newton pictured gravity...\"),\n",
    "    Turn(turn_id=2, parent_turn_id=1, user=\"Is gravity a force or something else?\", assistant=\"It depends on the theory...\"),\n",
    "    Turn(turn_id=3, parent_turn_id=0, user=\"you said Gravity is the force that pulls any two pieces of matter, can you show me the formula\", assistant=\"Newtonâ€™s universal law...\"),\n",
    "    Turn(turn_id=4, parent_turn_id=None, user=\"Give me a good recipe for a vegan pasta sauce.\", assistant=\"Creamy Tomato-Basil Vegan Pasta Sauce...\"),\n",
    "    Turn(turn_id=5, parent_turn_id=4, user=\"For the recipe, I don't like onion can you improve\", assistant=\"Creamy Tomato-Basil Vegan Pasta Sauce (No-Onion Version)...\"),\n",
    "    Turn(turn_id=6, parent_turn_id=None, user=\"Who coined the word gravity?\", assistant=\"Isaac Newton first used...\"),\n",
    "    Turn(turn_id=7, parent_turn_id=6, user=\"How old was he?\", assistant=\"Isaac Newton was 44â€“45 years old...\"),\n",
    "    Turn(turn_id=8, parent_turn_id=7, user=\"Where did he live?\", assistant=\"He lived in England...\"),\n",
    "]\n",
    "\n",
    "for conv in conversations:\n",
    "    conversation_tree.add_turn(conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34464bed-f7e9-4762-9915-b7bd514b6c40",
   "metadata": {},
   "source": [
    "Now that we have a data structure (the turns and tree) we can focus on the interesting part, the conversation router!\n",
    "\n",
    "The goal of this part is to find the best place in the tree to attach the current user prompt.\n",
    "\n",
    "We will do that by doing a sort of tournament for each roots (10 at a time) and also the the tips.\n",
    "The reason for the tournament is that we could have 100s of roots and even more tips and an llm has a context window limit. \n",
    "\n",
    "We are doing 10 at a time because llms tend to be better an ranking then scoring according to a rubric [RULER is inspiring me](https://art.openpipe.ai/fundamentals/ruler).\n",
    "\n",
    "We will do it as follow:\n",
    "\n",
    "1. Check me relevance of root using first 4 turns.\n",
    "2. For the 10 most relevant root, check for relevance of tips (roots may have several tips)\n",
    "3. For the 10 most relevant tips, collect all the conversation traces.\n",
    "4. Check for the most relevant conversation turn to attach too out of all turns in the short listed conversation traces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0140c445-b372-4d7d-9898-2afd46d04787",
   "metadata": {},
   "source": [
    "let's collect all roots and get 4 turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f0d3b0bc-5da0-456b-8c88-6a93234eeb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "roots = conversation_tree.get_root_turns()\n",
    "traces = []\n",
    "for i_root in roots:\n",
    "    traces.extend(conversation_tree.trace_downward(turn_id=i_root.turn_id, depth=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd953987-87b8-457c-a199-34da23b16ed8",
   "metadata": {},
   "source": [
    "Let's see how many traces we collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e13d5f72-6b78-4730-a09e-bf2e7c2fafb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8e3e063c-a9fc-405d-b7d8-86d6c3d75f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Turn(turn_id=0, parent_turn_id=None, user='Help me understand gravity.', assistant='Gravity is the force...', children_ids=[1, 3]),\n",
       " Turn(turn_id=1, parent_turn_id=0, user=\"What's the difference between Newton's and Einstein's theories of gravity?\", assistant='Newton pictured gravity...', children_ids=[2]),\n",
       " Turn(turn_id=2, parent_turn_id=1, user='Is gravity a force or something else?', assistant='It depends on the theory...', children_ids=[])]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5930d-10c6-48d3-adc8-2dd537fa48c8",
   "metadata": {},
   "source": [
    "We could probably show these to the llm but I think we can render that into something a little more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "79106fd4-774b-4c2c-ab12-7538a48bbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_trace(trace: List[Turn]) -> str:\n",
    "    trace_string = \"\"\n",
    "    for turn in trace:\n",
    "        trace_string += \"\\n\\n## User: \\n\\t\" + turn.user + \\\n",
    "                        \"\\n\\n## Assistant: \\n\\t\" + turn.assistant\n",
    "    return trace_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "50219d80-7818-436a-9ab4-c90bcc956e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "## User: \n",
      "\tGive me a good recipe for a vegan pasta sauce.\n",
      "\n",
      "## Assistant: \n",
      "\tCreamy Tomato-Basil Vegan Pasta Sauce...\n",
      "\n",
      "## User: \n",
      "\tFor the recipe, I don't like onion can you improve\n",
      "\n",
      "## Assistant: \n",
      "\tCreamy Tomato-Basil Vegan Pasta Sauce (No-Onion Version)...\n"
     ]
    }
   ],
   "source": [
    "print(format_trace(traces[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584b5850-848d-4167-b9f0-2b1f00c0d462",
   "metadata": {},
   "source": [
    "Now that we have all conversation segments from root to 4 we are ready to rank them by relevance.\n",
    "\n",
    "So for this, we need to provide a prompt and the candidate segment and we would like to receive a ranking where one is the best, and a relevance score from 0 to 1, and maybe some sort of temporary trace id so that we know what relevance goes for which trace. let's make a list:\n",
    "\n",
    "Inputs:\n",
    "\n",
    "* current user prompt (string)\n",
    "* first 4 turns of many root chats  (string with chat trace temporary id)\n",
    "\n",
    "Outputs:\n",
    "\n",
    "* a sorted list of evaluations which if contains: a rank (int), a trace id (int), a relevance score (float from 0 to 1)\n",
    "\n",
    "let's turn that into a dspy program.\n",
    "\n",
    "The first step for our dspy program is to define a class that will ensure we get from the llm exactly what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1f03f3c0-fcef-491a-aa67-535c4513caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentEvaluation(pydantic.BaseModel):\n",
    "    trace_id: int\n",
    "    relevance: float\n",
    "    segment_rank: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520c0f18-95e5-411d-860e-515eb3706f29",
   "metadata": {},
   "source": [
    "Now we will write our program's instructions, inputs and outputs as a DSPy signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "babb2541-94d6-4e44-b6c5-88f5c6b07689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateRootSegments(dspy.Signature):\n",
    "    \"\"\"Evaluate root conversation segments for relevance to a new prompt.\n",
    "\n",
    "    For each segment, identify if has topical connection to the user prompt. Consider if the prompt is:\n",
    "    - A direct follow-up question.\n",
    "    - A request for clarification.\n",
    "    - An exploration of a related sub-topic.\n",
    "    - A completely different subject.\n",
    "    \n",
    "    Assign a relevance score from 0.0 (completely irrelevant) to 1.0 (a direct continuation of the topic).\n",
    "    You will also rank the segments where 1 is the most relevant of the group\n",
    "    \"\"\"\n",
    "    #Inputs\n",
    "    user_prompt: str = dspy.InputField(desc=\"The new user prompt to be integrated.\")\n",
    "    segments_to_evaluate: str = dspy.InputField(desc=\"A stringified list of conversation segments, each with its trace_id and content.\")\n",
    "    \n",
    "    #Outputs\n",
    "    evaluations: List[SegmentEvaluation] = dspy.OutputField(desc=\"A list of evaluations, one for each segment, including detailed reasoning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b70ebc0-cfe7-4967-8961-26b7b0ab17a9",
   "metadata": {},
   "source": [
    "Now to make that signature callable we have to make it into a module. The simplest one is `dspy.Predict`, let's use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "576b34e9-cf2a-4aea-9349-51945a4294ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_relevance_evaluator = dspy.Predict(EvaluateRootSegments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6041d-0e74-4492-a4bc-4d2c20d5f594",
   "metadata": {},
   "source": [
    "We are almost ready to call an AI and get those traces ranked, but we first need to format our traces all in one nice piece of context for the llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f1ae5177-cfad-4b5d-9deb-d7e108a98738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<trace_id = 1>\n",
      "\n",
      "\n",
      "## User: \n",
      "\tHelp me understand gravity.\n",
      "\n",
      "## Assistant: \n",
      "\tGravity is the force...\n",
      "\n",
      "## User: \n",
      "\tWhat's the difference between Newton's and Einstein's theories of gravity?\n",
      "\n",
      "## Assistant: \n",
      "\tNewton pictured gravity...\n",
      "\n",
      "## User: \n",
      "\tIs gravity a force or something else?\n",
      "\n",
      "## Assistant: \n",
      "\tIt depends on the theory...\n",
      "</trace_id = 1>\n",
      "<trace_id = 2>\n",
      "\n",
      "\n",
      "## User: \n",
      "\tHelp me understand gravity.\n",
      "\n",
      "## Assistant: \n",
      "\tGravity is the force...\n",
      "\n",
      "## User: \n",
      "\tyou said Gravity is the force that pulls any two pieces of matter, can you show me the formula\n",
      "\n",
      "## Assistant: \n",
      "\tNewtonâ€™s universal law...\n",
      "</trace_id = 2>\n",
      "<trace_id = 3>\n",
      "\n",
      "\n",
      "## User: \n",
      "\tGive me a good recipe for a vegan pasta sauce.\n",
      "\n",
      "## Assistant: \n",
      "\tCreamy Tomato-Basil Vegan Pasta Sauce...\n",
      "\n",
      "## User: \n",
      "\tFor the recipe, I don't like onion can you improve\n",
      "\n",
      "## Assistant: \n",
      "\tCreamy Tomato-Basil Vegan Pasta Sauce (No-Onion Version)...\n",
      "</trace_id = 3>\n",
      "<trace_id = 4>\n",
      "\n",
      "\n",
      "## User: \n",
      "\tWho coined the word gravity?\n",
      "\n",
      "## Assistant: \n",
      "\tIsaac Newton first used...\n",
      "\n",
      "## User: \n",
      "\tHow old was he?\n",
      "\n",
      "## Assistant: \n",
      "\tIsaac Newton was 44â€“45 years old...\n",
      "\n",
      "## User: \n",
      "\tWhere did he live?\n",
      "\n",
      "## Assistant: \n",
      "\tHe lived in England...\n",
      "</trace_id = 4>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_traces_with_id(traces):\n",
    "    count = 0\n",
    "    all_traces_string = \"\"\n",
    "    for trace in traces:\n",
    "        count += 1\n",
    "        all_traces_string += f\"<trace_id = {count}>\\n\" + \\\n",
    "                                    format_trace(trace)+ \\\n",
    "                             f\"\\n</trace_id = {count}>\\n\"\n",
    "    return all_traces_string \n",
    "print(format_traces_with_id(traces))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8b4b2-4ebe-4885-a039-978ed02f0719",
   "metadata": {},
   "source": [
    "We will be calling an AI now to execute `root_relevance_evaluator`. So we set is up like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d8309b9f-46ff-4e09-85dc-c91e133533d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\"groq/moonshotai/kimi-k2-instruct\")\n",
    "dspy.configure(lm = lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d37f44-b6fb-4d8b-87a0-1bcffceab13e",
   "metadata": {},
   "source": [
    "Connecting to different models and providers in DSPy is very easy. You just have to change `groq/moonshotai/kimi-k2-instruct` for the path to the provider and model you want. Behind the scene, dspy uses litellm so this path is one that would work with litellm^[for instance you could do `gpt-4.1`, or `ollama/<ollama_model>`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6dd15bf8-2270-4a30-9557-9c637b803505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SegmentEvaluation(trace_id=1, relevance=0.0, segment_rank=4),\n",
       " SegmentEvaluation(trace_id=2, relevance=0.0, segment_rank=3),\n",
       " SegmentEvaluation(trace_id=3, relevance=0.15, segment_rank=1),\n",
       " SegmentEvaluation(trace_id=4, relevance=0.0, segment_rank=2)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = root_relevance_evaluator(\n",
    "    user_prompt = \"should I add cake for desert?\",\n",
    "    segments_to_evaluate = format_traces_with_id(traces)\n",
    ")\n",
    "root_evals = evaluation.evaluations\n",
    "root_evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57ed3e-b7f3-4869-8791-129cf7bfa751",
   "metadata": {},
   "source": [
    "let's do the same for the tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "daec1685-762c-417d-8f60-b48d1f651ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<trace_id = 1>\n",
      "\n",
      "\n",
      "## User: \n",
      "\tHelp me understand gravity.\n",
      "\n",
      "## Assistant: \n",
      "\tGravity is the force...\n",
      "\n",
      "## User: \n",
      "\tWhat's the difference between Newton's and Einstein's theories of gravity?\n",
      "\n",
      "## Assistant: \n",
      "\tNewton pictured gravity...\n",
      "\n",
      "## User: \n",
      "\tIs gravity a force or something else?\n",
      "\n",
      "## Assistant: \n",
      "\tIt depends on the theory...\n",
      "</trace_id = 1>\n",
      "<trace_id = 2>\n",
      "\n",
      "\n",
      "## User: \n",
      "\tHelp me understand gravity.\n",
      "\n",
      "## Assistant: \n",
      "\tGravity is the force...\n",
      "\n",
      "## User: \n",
      "\tyou said Gravity is the force that pulls any two pieces of matter, can you show me the formula\n",
      "\n",
      "## Assistant: \n",
      "\tNewtonâ€™s universal law...\n",
      "</trace_id = 2>\n",
      "<trace_id = 3>\n",
      "\n",
      "\n",
      "## User: \n",
      "\tGive me a good recipe for a vegan pasta sauce.\n",
      "\n",
      "## Assistant: \n",
      "\tCreamy Tomato-Basil Vegan Pasta Sauce...\n",
      "\n",
      "## User: \n",
      "\tFor the recipe, I don't like onion can you improve\n",
      "\n",
      "## Assistant: \n",
      "\tCreamy Tomato-Basil Vegan Pasta Sauce (No-Onion Version)...\n",
      "</trace_id = 3>\n",
      "<trace_id = 4>\n",
      "\n",
      "\n",
      "## User: \n",
      "\tWho coined the word gravity?\n",
      "\n",
      "## Assistant: \n",
      "\tIsaac Newton first used...\n",
      "\n",
      "## User: \n",
      "\tHow old was he?\n",
      "\n",
      "## Assistant: \n",
      "\tIsaac Newton was 44â€“45 years old...\n",
      "\n",
      "## User: \n",
      "\tWhere did he live?\n",
      "\n",
      "## Assistant: \n",
      "\tHe lived in England...\n",
      "</trace_id = 4>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips = conversation_tree.get_leaf_turns()\n",
    "traces = []\n",
    "for i_tip in tips:\n",
    "    traces.append(conversation_tree.trace_upward(turn_id=i_tip.turn_id, depth=4))\n",
    "\n",
    "print(format_traces_with_id(traces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "09b8acb8-ff09-4652-8408-99e56202fe2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SegmentEvaluation(trace_id=1, relevance=0.0, segment_rank=4),\n",
       " SegmentEvaluation(trace_id=2, relevance=0.0, segment_rank=3),\n",
       " SegmentEvaluation(trace_id=3, relevance=0.15, segment_rank=1),\n",
       " SegmentEvaluation(trace_id=4, relevance=0.0, segment_rank=2)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = root_relevance_evaluator(\n",
    "    user_prompt = \"should I add cake for desert?\",\n",
    "    segments_to_evaluate = format_traces_with_id(traces)\n",
    ")\n",
    "tip_evals = evaluation.evaluations\n",
    "tip_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "26eee46b-76ca-4dc3-a277-9d611c521769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trace_id=3 relevance=0.15 segment_rank=1\n",
      "trace_id=3 relevance=0.15 segment_rank=1\n"
     ]
    }
   ],
   "source": [
    "best_root_eval = max(root_evals, key=lambda x: x.relevance)\n",
    "best_tip_eval = max(tip_evals, key=lambda x: x.relevance)\n",
    "\n",
    "print(best_root_eval)\n",
    "print(best_tip_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecdc73db-59fc-44d3-bd2b-091e4e56cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectionDecision(pydantic.BaseModel):\n",
    "    turn_id: int\n",
    "    attach_at_turn_index: int\n",
    "    reasoning: str\n",
    "\n",
    "\n",
    "class ResolveConflict(dspy.Signature):\n",
    "    user_prompt: str = dspy.InputField()\n",
    "    candidate_A_info: str = dspy.InputField()\n",
    "    candidate_B_info: str = dspy.InputField()\n",
    "    decision: ConnectionDecision = dspy.OutputField()\n",
    "\n",
    "\n",
    "class FollowUpDecision(pydantic.BaseModel):\n",
    "    is_follow_up: bool = pydantic.Field(..., description=\"True if the new prompt is a direct contextual follow-up, otherwise False.\")\n",
    "    reasoning: str = pydantic.Field(..., description=\"A brief explanation for the decision.\")\n",
    "\n",
    "class CheckForFollowUp(dspy.Signature):\n",
    "    \"\"\"Determine if a new prompt is a direct follow-up to the immediate previous conversation turn.\n",
    "\n",
    "    Focus on pronouns (he, she, it, they), demonstratives (that, this), or questions that only make sense with the preceding context.\n",
    "    \"\"\"\n",
    "    last_turn: str = dspy.InputField(desc=\"The most recent user-assistant exchange.\")\n",
    "    new_prompt: str = dspy.InputField(desc=\"The new user prompt.\")\n",
    "    decision: FollowUpDecision = dspy.OutputField(desc=\"A boolean decision and reasoning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac081a96-ff0c-498d-b88b-869ba8e94bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationRouter(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.evaluate_segments = dspy.ChainOfThought(EvaluateSegments)\n",
    "        self.resolve_conflict = dspy.Predict(ResolveConflict)\n",
    "        # NEW: Add the follow-up checker\n",
    "        self.check_follow_up = dspy.Predict(CheckForFollowUp)\n",
    "\n",
    "    def _format_turns(self, turns: List[dict]) -> str:\n",
    "        return \"\\n\".join([f\"{turn['role'].capitalize()}: {turn['content']}\" for turn in turns])\n",
    "\n",
    "    def forward(self, user_prompt: str, conversations: List[dict], last_active_turn: dict = None):\n",
    "        if not conversations:\n",
    "            decision = ConnectionDecision(turn_id=0, attach_at_turn_index=-1, reasoning=\"No existing conversations.\")\n",
    "            return {'decision': decision, 'score': 1.0}\n",
    "\n",
    "        # --- NEW: Recency / Follow-up Check ---\n",
    "        if last_active_turn:\n",
    "            # Context is the last exchange in the most recent turn\n",
    "            # We take the full message list of the last active turn segment\n",
    "            last_turn_context = self._format_turns(last_active_turn['messages'])\n",
    "            follow_up_result = self.check_follow_up(last_turn=last_turn_context, new_prompt=user_prompt)\n",
    "\n",
    "            if follow_up_result.decision.is_follow_up:\n",
    "                print(f\"ðŸ§  Follow-up Detected: {follow_up_result.decision.reasoning}\")\n",
    "                \n",
    "                # The parent is the last turn itself, and we attach at its very last message\n",
    "                attach_index = len(last_active_turn['messages']) - 1\n",
    "                \n",
    "                decision = ConnectionDecision(\n",
    "                    turn_id=last_active_turn['turnID'],\n",
    "                    attach_at_turn_index=attach_index,\n",
    "                    reasoning=f\"Direct follow-up to the most recent turn. {follow_up_result.decision.reasoning}\"\n",
    "                )\n",
    "                # Return with a perfect score to ensure it's selected\n",
    "                return {'decision': decision, 'score': 1.0}\n",
    "        \n",
    "        # --- ORIGINAL LOGIC (if no follow-up is detected) ---\n",
    "        print(\"ðŸ¤” No direct follow-up. Evaluating all turnes for topical relevance...\")\n",
    "        root_segments_data = [{\"turn_id\": c['turnID'], \"content\": self._format_turns(c['messages'][:4])} for c in conversations]\n",
    "        tip_segments_data = [{\"turn_id\": c['turnID'], \"content\": self._format_turns(c['messages'][-4:])} for c in conversations]\n",
    "        \n",
    "        root_evals = self.evaluate_segments(user_prompt=user_prompt, segments_to_evaluate=str(root_segments_data)).evaluations\n",
    "        tip_evals = self.evaluate_segments(user_prompt=user_prompt, segments_to_evaluate=str(tip_segments_data)).evaluations\n",
    "\n",
    "        best_root_eval = max(root_evals, key=lambda x: x.relevance)\n",
    "        best_tip_eval = max(tip_evals, key=lambda x: x.relevance)\n",
    "\n",
    "        if best_root_eval.turn_id == best_tip_eval.turn_id:\n",
    "            chosen_turn_id = best_root_eval.turn_id\n",
    "            chosen_turn = next((c for c in conversations if c['turnID'] == chosen_turn_id), None)\n",
    "            decision = ConnectionDecision(\n",
    "                turn_id=chosen_turn_id,\n",
    "                attach_at_turn_index=len(chosen_turn['messages']) - 1,\n",
    "                reasoning=f\"Both root and tip analysis converged on turn {chosen_turn_id}.\"\n",
    "            )\n",
    "            return {'decision': decision, 'score': best_tip_eval.relevance}\n",
    "        else:\n",
    "            # This conflict resolution could be improved, but we'll leave it for now\n",
    "            conflict_decision = self.resolve_conflict(\n",
    "                user_prompt=user_prompt,\n",
    "                candidate_A_info=f\"turn ID: {best_root_eval.turn_id}, Reasoning: {best_root_eval.reasoning}\",\n",
    "                candidate_B_info=f\"turn ID: {best_tip_eval.turn_id}, Reasoning: {best_tip_eval.reasoning}\"\n",
    "            ).decision\n",
    "            best_score = max(best_root_eval.relevance, best_tip_eval.relevance)\n",
    "            return {'decision': conflict_decision, 'score': best_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e3884390-4499-4c0f-9fa2-9103ba33baf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def debug_print_tree(conversations: list):\n",
    "    \"\"\"\n",
    "    An accurate debugger that understands the \"taxonomy\" data structure,\n",
    "    where forked turnes only contain new messages.\n",
    "    \"\"\"\n",
    "    if not conversations:\n",
    "        print(\"[DEBUG] Tree is empty.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\"*20 + \" [DEBUG] Tree Structure \" + \"=\"*20)\n",
    "    \n",
    "    # Create a lookup map for turnes and their children\n",
    "    turn_map = {b['turnID']: b for b in conversations}\n",
    "    children_map = {}\n",
    "    for turn_id, turn in turn_map.items():\n",
    "        parent_id = turn.get('parent_turn_id')\n",
    "        if parent_id is not None:\n",
    "            if parent_id not in children_map:\n",
    "                children_map[parent_id] = []\n",
    "            children_map[parent_id].append(turn_id)\n",
    "\n",
    "    # Find root nodes (those without a parent)\n",
    "    root_ids = [b['turnID'] for b in conversations if b.get('parent_turn_id') is None]\n",
    "\n",
    "    def print_turn_recursively(turn_id, indent=\"\"):\n",
    "        turn = turn_map.get(turn_id)\n",
    "        if not turn: return\n",
    "        \n",
    "        # Print only the messages that physically exist in this turn\n",
    "        for i, msg in enumerate(turn['messages']):\n",
    "            # Determine the prefix for the line\n",
    "            if i == 0:\n",
    "                parent_id = turn.get('parent_turn_id')\n",
    "                parent_turn = turn.get('parent_turn_index')\n",
    "                if parent_id is not None:\n",
    "                    # This is the first message of a forked turn\n",
    "                    prefix = f\"{indent}ðŸŒ¿ Fork from turn {parent_id}[{parent_turn}] âžœ Turn {i}\"\n",
    "                else:\n",
    "                    # This is the first message of a root turn\n",
    "                    prefix = f\"{indent}ðŸŒ¿ Root turn {turn_id} âžœ Turn {i}\"\n",
    "            else:\n",
    "                # Subsequent messages in the same turn\n",
    "                prefix = f\"{indent}{' ' * (len(str(turn_id)) + 14)}âžœ Turn {i}\"\n",
    "\n",
    "            content_preview = (msg['content'][:60] + '...').replace('\\n', ' ')\n",
    "            print(f\"{prefix}: {content_preview}\")\n",
    "\n",
    "        # Recurse for children\n",
    "        if turn_id in children_map:\n",
    "            for child_id in children_map[turn_id]:\n",
    "                print_turn_recursively(child_id, indent + \"  \")\n",
    "\n",
    "    # Start printing from each root to show all separate trees\n",
    "    for root_id in root_ids:\n",
    "        print_turn_recursively(root_id)\n",
    "        print(\"-\" * 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5350dcc-8b22-4c9e-ad85-fc0edb135521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Signature for Generating a Response ---\n",
    "class GenerateResponse(dspy.Signature):\n",
    "    \"\"\"Given a conversation history, continue the conversation by responding to the last user question.\"\"\"\n",
    "    history: str = dspy.InputField(desc=\"The history of the conversation so far.\")\n",
    "    question: str = dspy.InputField(desc=\"The user's latest question.\")\n",
    "    answer: str = dspy.OutputField(desc=\"A concise and helpful answer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a5ff778-0671-4773-a2df-33edab5db02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulChat:\n",
    "    RELEVANCE_THRESHOLD = 0.6\n",
    "\n",
    "    def __init__(self, filepath=\"conversation_tree.json\"):\n",
    "        self.filepath = filepath\n",
    "        self.router = ConversationRouter()\n",
    "        self.responder = dspy.Predict(GenerateResponse)\n",
    "        self.conversations = self._load()\n",
    "        self.turn_map = {b['turnID']: b for b in self.conversations}\n",
    "        # NEW: Track the last turn that was added or modified\n",
    "        self.last_active_turn = self.turn_map.get(max(self.turn_map.keys())) if self.turn_map else None\n",
    "        print(f\"Chat manager initialized. Loaded {len(self.conversations)} turnes.\")\n",
    "\n",
    "    def _load(self):\n",
    "        if not os.path.exists(self.filepath): return []\n",
    "        with open(self.filepath, 'r', encoding='utf-8') as f: return json.load(f)\n",
    "\n",
    "    def _save(self):\n",
    "        with open(self.filepath, 'w', encoding='utf-8') as f: json.dump(self.conversations, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def _get_full_context(self, turn_id, turn_index) -> list:\n",
    "        full_history = []\n",
    "        curr_turn_id, curr_turn_index = turn_id, turn_index\n",
    "        while curr_turn_id is not None:\n",
    "            turn = self.turn_map.get(curr_turn_id)\n",
    "            if not turn: break\n",
    "            messages_in_segment = turn['messages'][:curr_turn_index + 1]\n",
    "            full_history = messages_in_segment + full_history\n",
    "            curr_turn_index = turn.get('parent_turn_index')\n",
    "            curr_turn_id = turn.get('parent_turn_id')\n",
    "        return full_history\n",
    "\n",
    "    def _add_to_tree(self, decision, user_prompt, assistant_response):\n",
    "        max_id = max([c['turnID'] for c in self.conversations] + [0])\n",
    "        new_turn_id = max_id + 1\n",
    "        \n",
    "        new_turns = [{\"role\": \"user\", \"content\": user_prompt}, {\"role\": \"assistant\", \"content\": assistant_response}]\n",
    "        \n",
    "        if decision.attach_at_turn_index == -1: # New Root\n",
    "            new_turn = { \"turnID\": new_turn_id, \"parent_turn_id\": None, \"parent_turn_index\": None, \"messages\": new_turns }\n",
    "        else: # Fork\n",
    "            new_turn = { \"turnID\": new_turn_id, \"parent_turn_id\": decision.turn_id, \"parent_turn_index\": decision.attach_at_turn_index, \"messages\": new_turns }\n",
    "        \n",
    "        self.conversations.append(new_turn)\n",
    "        self.turn_map[new_turn_id] = new_turn\n",
    "        # NEW: Update the last active turn reference\n",
    "        self.last_active_turn = new_turn\n",
    "\n",
    "    def chat(self, user_prompt: str):\n",
    "        print(f\"\\n>> User: {user_prompt}\")\n",
    "        \n",
    "        # MODIFIED: Pass the last_active_turn object to the router\n",
    "        routing_result = self.router(\n",
    "            user_prompt=user_prompt, \n",
    "            conversations=self.conversations,\n",
    "            last_active_turn=self.last_active_turn \n",
    "        )\n",
    "        decision, score = routing_result['decision'], routing_result['score']\n",
    "        print(f\"ðŸ§  Router Score: {score:.2f}\")\n",
    "\n",
    "        # The threshold logic now correctly handles both topical relevance and forced follow-ups\n",
    "        if score < self.RELEVANCE_THRESHOLD:\n",
    "            print(f\"âš ï¸ Score is below threshold. Creating a new root turn.\")\n",
    "            # Override decision to ensure a new root is created\n",
    "            decision = ConnectionDecision(turn_id=0, attach_at_turn_index=-1, reasoning=\"Score below threshold, creating new topic.\")\n",
    "        else:\n",
    "            print(f\"âœ… Score is above threshold. Following decision: {decision.reasoning}\")\n",
    "        \n",
    "        context_messages = []\n",
    "        if decision.attach_at_turn_index != -1:\n",
    "            context_messages = self._get_full_context(decision.turn_id, decision.attach_at_turn_index)\n",
    "        \n",
    "        history_str = self.router._format_turns(context_messages)\n",
    "        response = self.responder(history=history_str, question=user_prompt)\n",
    "        assistant_response = response.answer\n",
    "        print(f\"<< Assistant: {assistant_response}\")\n",
    "\n",
    "        self._add_to_tree(decision, user_prompt, assistant_response)\n",
    "        self._save()\n",
    "        #print(f\"ðŸ’¾ Tree updated and saved. Total turnes: {len(self.conversations)}\")\n",
    "        #debug_print_tree(self.conversations)\n",
    "        return assistant_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00cff3ed-7f40-4d35-8f92-c6373f249d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat manager initialized. Loaded 0 branches.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"my_chat_tree.json\"):\n",
    "    os.remove(\"my_chat_tree.json\")\n",
    "    \n",
    "chat_app = StatefulChat(filepath=\"my_chat_tree.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58a0aac1-f1fb-448d-9ca5-9873d3a7a8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> User: Help me understand gravity.\n",
      "ðŸ§  Router Score: 1.00\n",
      "âœ… Score is above threshold. Following decision: No existing conversations.\n",
      "<< Assistant: Gravity is the force that pulls any two pieces of matter toward each other. On Earth, it gives objects weight and keeps us on the ground. In space, it keeps planets orbiting the Sun and moons orbiting planets. The more mass something has, the stronger its gravity; the closer you are to it, the stronger the pull. Einstein later showed that gravity isnâ€™t just a force but a curvature of space-time caused by mass.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Gravity is the force that pulls any two pieces of matter toward each other. On Earth, it gives objects weight and keeps us on the ground. In space, it keeps planets orbiting the Sun and moons orbiting planets. The more mass something has, the stronger its gravity; the closer you are to it, the stronger the pull. Einstein later showed that gravity isnâ€™t just a force but a curvature of space-time caused by mass.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_app.chat(\"Help me understand gravity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17496b6e-aafe-4c18-8f95-f5c48bfcbd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> User: What's the difference between Newton's and Einstein's theories of gravity?\n",
      "ðŸ§  Follow-up Detected: The new prompt directly references \"Newton's and Einstein's theories of gravity,\" which were just discussed in the last turn. The question only makes sense in the context of the previous explanation about gravity and Einstein's refinement of the concept.\n",
      "ðŸ§  Router Score: 1.00\n",
      "âœ… Score is above threshold. Following decision: Direct follow-up to the most recent turn. The new prompt directly references \"Newton's and Einstein's theories of gravity,\" which were just discussed in the last turn. The question only makes sense in the context of the previous explanation about gravity and Einstein's refinement of the concept.\n",
      "<< Assistant: Newton pictured gravity as an invisible force acting instantly between masses, with strength depending only on mass and distance. Einstein replaced that force with geometry: mass and energy curve the fabric of space-time, and objects follow the straightest possible paths (geodesics) through that curved geometry. Newtonâ€™s theory works well for everyday speeds and weak fields, but Einsteinâ€™s general relativity predicts and explains phenomena Newtonâ€™s cannotâ€”such as Mercuryâ€™s orbit, gravitational time dilation, and the bending of light by massive objects.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Newton pictured gravity as an invisible force acting instantly between masses, with strength depending only on mass and distance. Einstein replaced that force with geometry: mass and energy curve the fabric of space-time, and objects follow the straightest possible paths (geodesics) through that curved geometry. Newtonâ€™s theory works well for everyday speeds and weak fields, but Einsteinâ€™s general relativity predicts and explains phenomena Newtonâ€™s cannotâ€”such as Mercuryâ€™s orbit, gravitational time dilation, and the bending of light by massive objects.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_app.chat(\"What's the difference between Newton's and Einstein's theories of gravity?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b525989-d70e-4aff-846c-e457c96bbabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> User: Is gravity a force or something else?\n",
      "ðŸ§  Follow-up Detected: The new prompt directly continues the topic of gravity and uses the pronoun 'it' implicitly referring to the concept just discussed. The question 'Is gravity a force or something else?' only makes sense in the context of the previous explanation contrasting Newton's force-based view with Einstein's geometric interpretation.\n",
      "ðŸ§  Router Score: 1.00\n",
      "âœ… Score is above threshold. Following decision: Direct follow-up to the most recent turn. The new prompt directly continues the topic of gravity and uses the pronoun 'it' implicitly referring to the concept just discussed. The question 'Is gravity a force or something else?' only makes sense in the context of the previous explanation contrasting Newton's force-based view with Einstein's geometric interpretation.\n",
      "<< Assistant: It depends on which theory you use. In Newtonâ€™s view, gravity is a force that acts between masses. In Einsteinâ€™s general relativity, gravity is not a force at allâ€”itâ€™s the curvature of space-time caused by mass and energy, and objects simply follow that curvature. So modern physics treats gravity as geometry, not a force.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It depends on which theory you use. In Newtonâ€™s view, gravity is a force that acts between masses. In Einsteinâ€™s general relativity, gravity is not a force at allâ€”itâ€™s the curvature of space-time caused by mass and energy, and objects simply follow that curvature. So modern physics treats gravity as geometry, not a force.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_app.chat(\"Is gravity a force or something else?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8432be4-9b7a-4db3-9a6a-d3a26b54386b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> User: Give me a good recipe for a vegan pasta sauce.\n",
      "ðŸ¤” No direct follow-up. Evaluating all branches for topical relevance...\n",
      "ðŸ§  Router Score: 0.00\n",
      "âš ï¸ Score is below threshold. Creating a new root branch.\n",
      "<< Assistant: Creamy Tomato-Basil Vegan Pasta Sauce  \n",
      "Ingredients (serves 4):  \n",
      "- 2 Tbsp olive oil  \n",
      "- 4 cloves garlic, minced  \n",
      "- 1 small onion, diced  \n",
      "- 1 can (14 oz) crushed tomatoes  \n",
      "- 1 cup raw cashews, soaked 30 min & drained  \n",
      "- Â½ cup unsweetened plant milk (soy/almond)  \n",
      "- 2 Tbsp nutritional yeast  \n",
      "- 1 tsp dried oregano  \n",
      "- Â½ tsp red-pepper flakes (optional)  \n",
      "- Salt & pepper to taste  \n",
      "- 1 packed cup fresh basil leaves, torn  \n",
      "\n",
      "Steps:  \n",
      "1. SautÃ© onion in olive oil over medium heat 4 min; add garlic 1 min more.  \n",
      "2. Stir in tomatoes, oregano, pepper flakes; simmer 10 min.  \n",
      "3. Blend cashews with plant milk until silky smooth.  \n",
      "4. Pour cashew cream into sauce; simmer 5 min. Season.  \n",
      "5. Off heat, fold in basil. Serve hot over pasta, garnished with extra basil.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Creamy Tomato-Basil Vegan Pasta Sauce  \\nIngredients (serves 4):  \\n- 2 Tbsp olive oil  \\n- 4 cloves garlic, minced  \\n- 1 small onion, diced  \\n- 1 can (14 oz) crushed tomatoes  \\n- 1 cup raw cashews, soaked 30 min & drained  \\n- Â½ cup unsweetened plant milk (soy/almond)  \\n- 2 Tbsp nutritional yeast  \\n- 1 tsp dried oregano  \\n- Â½ tsp red-pepper flakes (optional)  \\n- Salt & pepper to taste  \\n- 1 packed cup fresh basil leaves, torn  \\n\\nSteps:  \\n1. SautÃ© onion in olive oil over medium heat 4 min; add garlic 1 min more.  \\n2. Stir in tomatoes, oregano, pepper flakes; simmer 10 min.  \\n3. Blend cashews with plant milk until silky smooth.  \\n4. Pour cashew cream into sauce; simmer 5 min. Season.  \\n5. Off heat, fold in basil. Serve hot over pasta, garnished with extra basil.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_app.chat(\"Give me a good recipe for a vegan pasta sauce.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "916c3548-38c3-45ff-a689-da0785b16b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> User: Who coined the word gravity?\n",
      "ðŸ¤” No direct follow-up. Evaluating all branches for topical relevance...\n",
      "ðŸ§  Router Score: 0.00\n",
      "âš ï¸ Score is below threshold. Creating a new root branch.\n",
      "<< Assistant: Isaac Newton first used the word â€œgravityâ€ in its modern scientific sense in his 1687 work *PhilosophiÃ¦ Naturalis Principia Mathematica*.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Isaac Newton first used the word â€œgravityâ€ in its modern scientific sense in his 1687 work *PhilosophiÃ¦ Naturalis Principia Mathematica*.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_app.chat(\"Who coined the word gravity?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f8355f5-0c5e-4234-831c-5c5ed296e040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> User: How old was he?\n",
      "ðŸ§  Follow-up Detected: The pronoun 'he' clearly refers to Isaac Newton, the subject of the previous turn, making this a direct contextual follow-up.\n",
      "ðŸ§  Router Score: 1.00\n",
      "âœ… Score is above threshold. Following decision: Direct follow-up to the most recent turn. The pronoun 'he' clearly refers to Isaac Newton, the subject of the previous turn, making this a direct contextual follow-up.\n",
      "<< Assistant: Isaac Newton was 44 or 45 years old when *Principia* was published in 1687 (born 25 December 1642, old style calendar).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Isaac Newton was 44 or 45 years old when *Principia* was published in 1687 (born 25 December 1642, old style calendar).'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_app.chat(\"How old was he?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a888c4c2-2213-4d0a-ac6f-1b8bc1b64c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> User: Where did he live?\n",
      "ðŸ§  Follow-up Detected: The pronoun \"he\" clearly refers to Isaac Newton, the subject of the previous turn, making this a direct contextual follow-up.\n",
      "ðŸ§  Router Score: 1.00\n",
      "âœ… Score is above threshold. Following decision: Direct follow-up to the most recent turn. The pronoun \"he\" clearly refers to Isaac Newton, the subject of the previous turn, making this a direct contextual follow-up.\n",
      "<< Assistant: He lived at Woolsthorpe Manor in Lincolnshire, England, and later in Cambridge while working at Trinity College.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'He lived at Woolsthorpe Manor in Lincolnshire, England, and later in Cambridge while working at Trinity College.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_app.chat(\"Where did he live?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b3422ea-8aa9-433b-bf86-e55e8319fbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> User: For the recipe, I don't like onion can you improve\n",
      "ðŸ¤” No direct follow-up. Evaluating all branches for topical relevance...\n",
      "ðŸ§  Router Score: 1.00\n",
      "âœ… Score is above threshold. Following decision: Both root and tip analysis converged on Branch 4.\n",
      "<< Assistant: Simply omit the onion. Start by warming the olive oil over medium heat and add the minced garlic right away; sautÃ© just 30â€“45 seconds until fragrant, then continue with the recipe as written. The sauce will still be rich and flavorful thanks to the tomatoes, cashew cream, nutritional yeast, and fresh basil.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Simply omit the onion. Start by warming the olive oil over medium heat and add the minced garlic right away; sautÃ© just 30â€“45 seconds until fragrant, then continue with the recipe as written. The sauce will still be rich and flavorful thanks to the tomatoes, cashew cream, nutritional yeast, and fresh basil.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_app.chat(\"For the recipe, I don't like onion can you improve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a339d135-206a-4d2b-90e3-c5a6b89a0647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> User: you said Gravity is the force that pulls any two pieces of matter, can you show me the formula \n",
      "ðŸ¤” No direct follow-up. Evaluating all branches for topical relevance...\n",
      "ðŸ§  Router Score: 1.00\n",
      "âœ… Score is above threshold. Following decision: Both root and tip analysis converged on Branch 9.\n",
      "<< Assistant: F = G Â· (mâ‚Â·mâ‚‚) / rÂ²\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'F = G Â· (mâ‚Â·mâ‚‚) / rÂ²'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_app.chat(\"you said Gravity is the force that pulls any two pieces of matter, can you show me the formula \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6daece4c-6580-453a-bbf1-e5b08e1db406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_unified_tree(conversations: list, filename: str = \"conversation_unified_tree.html\"):\n",
    "    \"\"\"\n",
    "    Creates and displays a single, unified tree by connecting all root\n",
    "    turnes to a universal \"super root\" node.\n",
    "    \"\"\"\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # 1. ADD THE UNIVERSAL ROOT NODE\n",
    "    G.add_node('UNIVERSAL_ROOT', label='All Conversations', shape='star', color='#C70039', size=25)\n",
    "    \n",
    "    # 2. Add all nodes from all turnes (same as before)\n",
    "    for turn in conversations:\n",
    "        turn_id = turn['turnID']\n",
    "        for i, turn in enumerate(turn['messages']):\n",
    "            node_id = f\"{turn_id}_{i}\"\n",
    "            content = turn.get('content', '')\n",
    "            label_text = (content[:97] + '...') if len(content) > 100 else content\n",
    "            node_label = '\\n'.join(textwrap.wrap(label_text, width=30))\n",
    "            node_title = content\n",
    "            node_shape = 'box'\n",
    "            color = \"#4E86E8\" if turn['role'] == 'user' else \"#D4A35D\"\n",
    "            G.add_node(node_id, label=node_label, title=node_title, shape=node_shape, color=color)\n",
    "\n",
    "    # 3. Add all edges, including connections to the universal root\n",
    "    for turn in conversations:\n",
    "        turn_id = turn['turnID']\n",
    "        # Connect turns within the same turn\n",
    "        for i in range(len(turn['messages']) - 1):\n",
    "            from_node, to_node = f\"{turn_id}_{i}\", f\"{turn_id}_{i+1}\"\n",
    "            G.add_edge(from_node, to_node)\n",
    "            \n",
    "        parent_id = turn.get('parent_turn_id')\n",
    "        if parent_id is not None:\n",
    "            # This is a FORKED turn, connect it to its direct parent\n",
    "            parent_turn = turn.get('parent_turn_index')\n",
    "            fork_start_node = f\"{turn_id}_0\"\n",
    "            parent_node = f\"{parent_id}_{parent_turn}\"\n",
    "            if G.has_node(parent_node) and G.has_node(fork_start_node):\n",
    "                 G.add_edge(parent_node, fork_start_node, color=\"#C5C5C5\", dashes=True)\n",
    "        else:\n",
    "            # This is a ROOT turn, connect it to the UNIVERSAL_ROOT\n",
    "            turn_start_node = f\"{turn_id}_0\"\n",
    "            if G.has_node(turn_start_node):\n",
    "                 G.add_edge('UNIVERSAL_ROOT', turn_start_node, color=\"#A9A9A9\", dashes=True)\n",
    "\n",
    "    # --- Hierarchical Layout and Display ---\n",
    "    net = Network(height=\"800px\", width=\"100%\", notebook=True, directed=True, cdn_resources='in_line')\n",
    "    net.from_nx(G)\n",
    "    options = \"\"\"\n",
    "    {\n",
    "      \"layout\": { \"hierarchical\": { \"enabled\": true, \"direction\": \"UD\", \"sortMethod\": \"directed\", \"levelSeparation\": 150, \"nodeSpacing\": 200 }},\n",
    "      \"physics\": { \"enabled\": false }\n",
    "    }\n",
    "    \"\"\"\n",
    "    net.set_options(options)\n",
    "    net.show(filename)\n",
    "    print(f\"ðŸŒ³ Unified Hierarchical Tree graph saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2a60180-476f-4724-b418-dce2fc7e4096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversation_unified_tree.html\n",
      "ðŸŒ³ Unified Hierarchical Tree graph saved to conversation_unified_tree.html\n"
     ]
    }
   ],
   "source": [
    "visualize_unified_tree(chat_app.conversations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}