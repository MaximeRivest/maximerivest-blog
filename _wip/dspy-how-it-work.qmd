---
title: "How DSPy really works [wip]"
date: 2025-07-08
execute:
  cache: false
---

Lots of people find DSPy magical (I am one of them). To some this is unsettling as they feel they lose control over the LLM and the prompt, by the end of this tutorial you will feel that you have both control and magic. We will build our own (simplified) implementation of all major dspy component in a way that we will control and trace the whole flow of the prompt and its optimization. I want to show you this, not because you need it to use DSPy, but because you need it to *grok* and the paradigm (I call it Intent-Oriented Programming) behind it.

> **Intent-Oriented Programming** is a programming paradigm where you explicitly declare the intent of your task—the inputs, desired outputs, success criteria (through metric and training set), and necessary context—without specifying exact implementations. This structured intent can then be automatically specialized, optimized, and adapted to various execution engines (AI models, human agents, APIs, or future intelligent systems). This clearly separates what you want from how it’s achieved, enabling maximum flexibility and continuous improvement.

At its core, in DSPy, information flows like this:

```{ojs}
//| echo: false
d3 = require("d3@7")

{
  const width = 1000;
  const height = 800;
  const svg = d3.create("svg")
    .attr("width", width)
    .attr("height", height)
    .attr("viewBox", [0, 0, width, height])
    .attr("style", "max-width: 100%; height: auto;");

  // Define arrow marker
  svg.append("defs").append("marker")
    .attr("id", "arrow")
    .attr("viewBox", "0 -5 10 10")
    .attr("refX", 5)
    .attr("refY", 0)
    .attr("markerWidth", 6)
    .attr("markerHeight", 6)
    .attr("orient", "auto")
    .append("path")
      .attr("d", "M0,-5L10,0L0,5")
      .attr("fill", "black");

  // Signature box
  const sig_x = 400;
  const sig_y = 50;
  const sig_width = 540;
  const sig_height = 140;
  svg.append("rect")
    .attr("x", sig_x)
    .attr("y", sig_y)
    .attr("width", sig_width)
    .attr("height", sig_height)
    .attr("fill", "white")
    .attr("stroke", "black")
    .attr("rx", 10)
    .attr("ry", 10);
  svg.append("text")
    .attr("x", sig_x + sig_width / 2)
    .attr("y", sig_y + 30)
    .attr("text-anchor", "middle")
    .attr("font-size", 16)
    .text("Signature");

  // Nodes
  const nodes = [
    {id: "fewshot", x: 50, y: 100, width: 160, height: 60, text: ["Few-shot examples", "(trainable)"]},
    {id: "user", x: 240, y: 100, width: 120, height: 40, text: "User Input"},
    {id: "inputdesc", x: 420, y: 100, width: 160, height: 60, text: ["Input name, type &", "description"]},
    {id: "outputdesc", x: 590, y: 100, width: 160, height: 60, text: ["Output name, type &", "description"]},
    {id: "system", x: 760, y: 100, width: 160, height: 60, text: ["System Prompt", "(trainable)"]},
    {id: "adapter1", x: 440, y: 250, width: 120, height: 40, text: "Adapter"},
    {id: "formatted", x: 420, y: 350, width: 160, height: 40, text: "Formatted Prompt"},
    {id: "llm", x: 460, y: 450, width: 80, height: 40, text: "LLM"},
    {id: "raw", x: 420, y: 550, width: 160, height: 40, text: "Raw Response"},
    {id: "adapter2", x: 440, y: 650, width: 120, height: 40, text: "Adapter"},
    {id: "prediction", x: 430, y: 750, width: 140, height: 40, text: "Prediction"},
  ];

  const nodeMap = new Map(nodes.map(d => [d.id, d]));

  // Draw nodes
  const nodeGroup = svg.selectAll("g.node")
    .data(nodes)
    .enter()
    .append("g")
    .attr("class", "node");

  nodeGroup.append("rect")
    .attr("x", d => d.x)
    .attr("y", d => d.y)
    .attr("width", d => d.width)
    .attr("height", d => d.height)
    .attr("fill", "white")
    .attr("stroke", "black")
    .attr("rx", 5)
    .attr("ry", 5);

  nodeGroup.append("text")
    .attr("text-anchor", "middle")
    .attr("font-size", 12)
    .attr("x", d => d.x + d.width / 2)
    .attr("y", d => d.y + d.height / 2)
    .each(function(d) {
      const text = d3.select(this);
      if (typeof d.text === "string") {
        text.append("tspan")
          .attr("dy", "0.35em")
          .text(d.text);
      } else {
        const numLines = d.text.length;
        const lineHeight = 1.1;
        const startDy = - (numLines - 1) * lineHeight / 2;
        d.text.forEach((line, i) => {
          text.append("tspan")
            .attr("x", d.x + d.width / 2)
            .attr("dy", `${i === 0 ? startDy : lineHeight}em`)
            .text(line);
        });
      }
    });

  // Links
  const links = [
    {source: "fewshot", target: "adapter1"},
    {source: "user", target: "adapter1"},
    {source: "inputdesc", target: "adapter1"},
    {source: "outputdesc", target: "adapter1"},
    {source: "system", target: "adapter1"},
    {source: "adapter1", target: "formatted"},
    {source: "formatted", target: "llm"},
    {source: "llm", target: "raw"},
    {source: "raw", target: "adapter2"},
    {source: "adapter2", target: "prediction"}
  ];

  svg.selectAll("path.link")
    .data(links)
    .enter()
    .append("path")
    .attr("class", "link")
    .attr("d", d => {
      const s = [nodeMap.get(d.source).x + nodeMap.get(d.source).width / 2, nodeMap.get(d.source).y + nodeMap.get(d.source).height];
      const t = [nodeMap.get(d.target).x + nodeMap.get(d.target).width / 2, nodeMap.get(d.target).y];
      return `M${s[0]},${s[1]} Q${t[0]},${s[1]} ${t[0]},${t[1]}`;
    })
    .attr("fill", "none")
    .attr("stroke", "black")
    .attr("marker-end", "url(#arrow)");

  return svg.node();
}
```
A signature contains an instruction (this goes into the system prompt and is 'trainable'), input and output fields with their structure (e.g. Int, Float, Json, Literal, dataclass, etc). Input and output are generally not trainable nor is the way that they are presented to the LLM.

To prompt an LLM, one would make a call to a program (dspy.Predict is the simplest one) with a signature and in the call the user (or programmer/you) would fill up the call with the current input you want to provide.

Your current inputs and the signature (instructions, list of inputs and outputs name and datatype) would be given to an adapter. The demos (if any) are also passed to the adapter at this stage. Demos are few-shot examples that show the LLM what you expect. At that stage, the adapter transforms all that into the 'formatted prompt'. It puts (interpolates) the instruction into the system prompt template as well as the names of the inputs and the names of the outputs and their type. It formats the demos (if present) as example input-output pairs in the prompt. It always puts (interpolates) your 'current' inputs into a user message. Then a prompt is ready and sent to LLM providers (leveraging litellm).

This means that your current input + your signature + demos + an (your?) adapter are all (and the only) pieces needed to create the actual prompt. In DSPy, it's not yet well documented (I'm working on that), but you can absolutely make your own adapter. If you do, you build your own prompt by creating these modular pieces: the input, the signature, the demos formatting, the adapter.

When the LLM responds, the parser in the adapter picks it up and parses it into the output python types that were specified by the signature.

The demos part is interesting. They can come from three places: hardcoded in your module (predictor.demos = [...]), passed at call time (predictor(input=x, demos=[...])), or set by an optimizer during compilation. The adapter decides how to format these demos into the prompt (as few-shot examples before your actual input).

All of this is inspectable. You can see the formatted prompt, the demos being used, the parsing logic. The 'magic' is just modular composition of these pieces.
