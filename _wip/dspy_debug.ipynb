{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy Fine-tuning Debug Notebook\n",
    "\n",
    "This notebook includes extensive debugging to understand why training loss is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3090\n",
      "Memory allocated: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Set CUDA device to GPU 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and setup the debugging function\n",
    "from dspy.clients import lm_local\n",
    "import dspy\n",
    "import json\n",
    "\n",
    "# Store the original function\n",
    "original_train_sft_locally = lm_local.train_sft_locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the debugging version\n",
    "def fixed_train_sft_locally(model_name, train_data, train_kwargs):\n",
    "    \"\"\"Fixed version of train_sft_locally with extensive debugging\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîç DEBUG: Starting fixed_train_sft_locally\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Debug: Print training data\n",
    "    print(f\"\\nüìä Training data info:\")\n",
    "    print(f\"  - Number of examples: {len(train_data)}\")\n",
    "    print(f\"  - First example keys: {list(train_data[0].keys()) if train_data else 'No data'}\")\n",
    "    if train_data and len(train_data) > 0:\n",
    "        print(f\"\\n  - First example content:\")\n",
    "        first_example = train_data[0]\n",
    "        print(json.dumps(first_example, indent=2))\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "        from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"For local finetuning, please install torch, transformers, and trl \"\n",
    "            \"by running `pip install -U torch transformers accelerate trl peft`\"\n",
    "        )\n",
    "\n",
    "    device = train_kwargs.get(\"device\", None)\n",
    "    if device is None:\n",
    "        device = \"cuda:0\"  # Use first visible device\n",
    "    print(f\"\\nüñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "    # Load model with explicit device mapping\n",
    "    print(f\"\\nüì• Loading model: {model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_name,\n",
    "        torch_dtype=torch.bfloat16,  # Use bf16\n",
    "        device_map=device,  # Explicit device mapping\n",
    "        use_cache=False,\n",
    "    )\n",
    "    print(f\"‚úÖ Model loaded successfully\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "    # Set up the chat format\n",
    "    try:\n",
    "        model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "        print(\"‚úÖ Chat format setup successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Chat format setup failed (this is often OK): {e}\")\n",
    "\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        print(\"‚ûï Adding pad token to tokenizer\")\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"[!#PAD#!]\"})\n",
    "\n",
    "    print(\"\\nüîÑ Creating dataset\")\n",
    "    if \"max_seq_length\" not in train_kwargs:\n",
    "        train_kwargs[\"max_seq_length\"] = 512  # Reduced from 4096\n",
    "        print(f\"üìè Set max_seq_length to {train_kwargs['max_seq_length']}\")\n",
    "\n",
    "    from datasets import Dataset\n",
    "    hf_dataset = Dataset.from_list(train_data)\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        result = lm_local.encode_sft_example(example, tokenizer, train_kwargs[\"max_seq_length\"])\n",
    "        # Debug first tokenized example\n",
    "        if hasattr(tokenize_function, 'first_call'):\n",
    "            tokenize_function.first_call = False\n",
    "            print(\"\\nüîç DEBUG: First tokenized example:\")\n",
    "            print(f\"  - Input IDs shape: {result['input_ids'].shape}\")\n",
    "            print(f\"  - Labels shape: {result['labels'].shape}\")\n",
    "            print(f\"  - Non-negative labels: {(result['labels'] != -100).sum().item()}\")\n",
    "            # Decode some tokens to see what we're training on\n",
    "            non_neg_indices = (result['labels'] != -100).nonzero(as_tuple=True)[0]\n",
    "            if len(non_neg_indices) > 0:\n",
    "                sample_tokens = result['input_ids'][non_neg_indices[:50]]  # First 50 non-masked tokens\n",
    "                decoded = tokenizer.decode(sample_tokens, skip_special_tokens=False)\n",
    "                print(f\"  - Sample decoded tokens: {decoded[:200]}...\")\n",
    "        return result\n",
    "    \n",
    "    tokenize_function.first_call = True\n",
    "\n",
    "    print(\"\\nüîÑ Tokenizing dataset...\")\n",
    "    tokenized_dataset = hf_dataset.map(tokenize_function, batched=False)\n",
    "    tokenized_dataset.set_format(type=\"torch\")\n",
    "    \n",
    "    # Debug dataset statistics\n",
    "    print(f\"\\nüìä Dataset statistics:\")\n",
    "    print(f\"  - Dataset size: {len(tokenized_dataset)}\")\n",
    "    \n",
    "    # Check if any examples have non-masked labels\n",
    "    total_non_masked = 0\n",
    "    for i in range(min(5, len(tokenized_dataset))):\n",
    "        example = tokenized_dataset[i]\n",
    "        non_masked = (example[\"labels\"] != -100).sum().item()\n",
    "        total_non_masked += non_masked\n",
    "        print(f\"  - Example {i}: {non_masked} non-masked tokens out of {len(example['labels'])}\")\n",
    "    \n",
    "    if total_non_masked == 0:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: No non-masked tokens found in first 5 examples!\")\n",
    "        print(\"    This means the model has nothing to learn from!\")\n",
    "\n",
    "    use_peft = train_kwargs.get(\"use_peft\", False)\n",
    "    peft_config = None\n",
    "\n",
    "    if use_peft:\n",
    "        from peft import LoraConfig\n",
    "        peft_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            target_modules=\"all-linear\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        print(\"‚úÖ PEFT/LoRA config created\")\n",
    "\n",
    "    # Enable gradient checkpointing\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"‚úÖ Gradient checkpointing enabled\")\n",
    "\n",
    "    # Print training configuration\n",
    "    print(f\"\\n‚öôÔ∏è  Training configuration:\")\n",
    "    print(f\"  - Output dir: {train_kwargs['output_dir']}\")\n",
    "    print(f\"  - Epochs: {train_kwargs.get('num_train_epochs', 2)}\")\n",
    "    print(f\"  - Batch size: {train_kwargs.get('per_device_train_batch_size', 1)}\")\n",
    "    print(f\"  - Gradient accumulation: {train_kwargs.get('gradient_accumulation_steps', 8)}\")\n",
    "    print(f\"  - Learning rate: {train_kwargs.get('learning_rate', 2e-5)}\")\n",
    "    print(f\"  - Max seq length: {train_kwargs['max_seq_length']}\")\n",
    "    print(f\"  - Packing: False (disabled)\")\n",
    "    print(f\"  - BF16: True\")\n",
    "\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=train_kwargs[\"output_dir\"],\n",
    "        num_train_epochs=train_kwargs.get(\"num_train_epochs\", 2),\n",
    "        per_device_train_batch_size=train_kwargs.get(\"per_device_train_batch_size\", 1),\n",
    "        gradient_accumulation_steps=train_kwargs.get(\"gradient_accumulation_steps\", 8),\n",
    "        learning_rate=train_kwargs.get(\"learning_rate\", 2e-5),\n",
    "        max_grad_norm=2.0,\n",
    "        logging_steps=5,  # More frequent logging\n",
    "        warmup_ratio=0.03,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_steps=50,\n",
    "        save_total_limit=2,\n",
    "        bf16=True,  # Always use bf16\n",
    "        max_seq_length=train_kwargs[\"max_seq_length\"],\n",
    "        packing=False,  # Disable packing to avoid issues\n",
    "        dataset_kwargs={\n",
    "            \"add_special_tokens\": False,\n",
    "            \"append_concat_token\": False,\n",
    "        },\n",
    "        gradient_checkpointing=True,\n",
    "        optim=train_kwargs.get(\"optim\", \"adamw_torch_fused\"),\n",
    "        logging_first_step=True,  # Log the first step\n",
    "        eval_on_start=False,  # Don't evaluate at start\n",
    "    )\n",
    "\n",
    "    print(\"\\nüèãÔ∏è Starting training...\")\n",
    "    \n",
    "    # Create a custom trainer to add more debugging\n",
    "    class DebugSFTTrainer(SFTTrainer):\n",
    "        def compute_loss(self, model, inputs,\n",
    "                         num_items_in_batch=None, return_outputs=False):\n",
    "    \n",
    "            loss_or_tuple = super().compute_loss(\n",
    "                model, inputs,\n",
    "                num_items_in_batch=num_items_in_batch,\n",
    "                return_outputs=return_outputs\n",
    "            )\n",
    "    \n",
    "            # ‚îÄ‚îÄ Unpack if necessary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            if isinstance(loss_or_tuple, tuple):\n",
    "                loss_value, outputs = loss_or_tuple           # trl ‚â•0.8 path\n",
    "            else:\n",
    "                loss_value, outputs = loss_or_tuple, None     # older trl path\n",
    "    \n",
    "            # Only print once\n",
    "            if not hasattr(self, \"_debug_printed\"):\n",
    "                self._debug_printed = True\n",
    "                print(\"\\nüîç DEBUG: First batch in compute_loss:\")\n",
    "                print(f\"  - Input shape:  {inputs['input_ids'].shape}\")\n",
    "                print(f\"  - Labels shape: {inputs['labels'].shape}\")\n",
    "                print(f\"  - Non‚Äëmasked labels: {(inputs['labels'] != -100).sum().item()}\")\n",
    "                print(f\"  - Loss value: {loss_value.item()}\")\n",
    "    \n",
    "            # ‚îÄ‚îÄ Return what the Trainer expects ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            if return_outputs:\n",
    "                return loss_value, outputs                    # keep the tuple form\n",
    "            else:\n",
    "                return loss_value                             # scalar tensor only\n",
    "    \n",
    "        \n",
    "    trainer = DebugSFTTrainer(\n",
    "        model=model,\n",
    "        args=sft_config,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        peft_config=peft_config,\n",
    "    )\n",
    "\n",
    "    # Train!\n",
    "    print(\"\\nüöÇ Training started...\")\n",
    "    trainer.train()\n",
    "    print(\"‚úÖ Training completed!\")\n",
    "\n",
    "    # Save the model!\n",
    "    print(f\"\\nüíæ Saving model to {sft_config.output_dir}\")\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(sft_config.output_dir)\n",
    "\n",
    "    if use_peft:\n",
    "        from peft import AutoPeftModelForCausalLM\n",
    "        print(\"\\nüîÑ Merging PEFT adapter...\")\n",
    "        # Load PEFT model on CPU\n",
    "        model_ = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=sft_config.output_dir,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        merged_model = model_.merge_and_unload()\n",
    "        merged_model.save_pretrained(sft_config.output_dir, safe_serialization=True, max_shard_size=\"5GB\")\n",
    "        print(\"‚úÖ PEFT adapter merged\")\n",
    "\n",
    "    # Clean up!\n",
    "    import gc\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"\\n‚úÖ Returning output dir: {sft_config.output_dir}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return sft_config.output_dir\n",
    "\n",
    "# Apply the patch\n",
    "lm_local.train_sft_locally = fixed_train_sft_locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 77\n",
      "First 5 classes: ['activate_my_card', 'age_limit', 'apple_pay_or_google_pay', 'atm_support', 'automatic_top_up']\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "import random\n",
    "from dspy.datasets import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Banking77 dataset\n",
    "ds = load_dataset(\"banking77\", split=\"train\")\n",
    "CLASSES = ds.features['label'].names\n",
    "print(f\"Number of classes: {len(CLASSES)}\")\n",
    "print(f\"First 5 classes: {CLASSES[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 500 unlabeled examples\n",
      "First example text: What if there is an error on the exchange rate?...\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data\n",
    "kwargs = dict(fields=(\"text\", \"label\"), input_keys=(\"text\",), split=\"train\")\n",
    "raw_data = [\n",
    "    dspy.Example(x, label=CLASSES[x.label]).with_inputs(\"text\")\n",
    "    for x in DataLoader().from_huggingface(dataset_name=\"banking77\", **kwargs)[:1000]\n",
    "]\n",
    "random.Random(0).shuffle(raw_data)\n",
    "\n",
    "unlabeled_trainset = [dspy.Example(text=x.text).with_inputs(\"text\") for x in raw_data[:500]]\n",
    "print(f\"Prepared {len(unlabeled_trainset)} unlabeled examples\")\n",
    "print(f\"First example text: {unlabeled_trainset[0].text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup authentication\nos.environ[\"HF_HUB_TOKEN\"] = \"hf_[REDACTED]\"\n\nfrom huggingface_hub import login\nlogin(token=\"hf_[REDACTED]\")"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models configured\n"
     ]
    }
   ],
   "source": [
    "# Setup DSPy\n",
    "from typing import Literal\n",
    "from dspy.clients.lm_local import LocalProvider\n",
    "dspy.settings.experimental = True\n",
    "\n",
    "classify = dspy.ChainOfThought(f\"text -> label: Literal{CLASSES}\")\n",
    "\n",
    "student_lm_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "student_lm = dspy.LM(model=f\"openai/local:{student_lm_name}\",\n",
    "                     provider=LocalProvider(), max_tokens=2000)\n",
    "teacher_lm = dspy.LM('openai/gpt-4o-mini', max_tokens=3000)\n",
    "\n",
    "student_classify = classify.deepcopy()\n",
    "student_classify.set_lm(student_lm)\n",
    "\n",
    "teacher_classify = classify.deepcopy()\n",
    "teacher_classify.set_lm(teacher_lm)\n",
    "\n",
    "print(\"Models configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing teacher model output...\n",
      "\n",
      "Input text: What if there is an error on the exchange rate?...\n",
      "Teacher predicted label: card_payment_wrong_exchange_rate\n",
      "Teacher reasoning: The question pertains to an error regarding the exchange rate, which suggests a concern about the accuracy of the rates applied during transactions. This aligns with the label 'card_payment_wrong_exch...\n"
     ]
    }
   ],
   "source": [
    "# First, let's manually check what the teacher generates\n",
    "print(\"Testing teacher model output...\")\n",
    "test_example = unlabeled_trainset[0]\n",
    "teacher_output = teacher_classify(text=test_example.text)\n",
    "print(f\"\\nInput text: {test_example.text[:100]}...\")\n",
    "print(f\"Teacher predicted label: {teacher_output.label}\")\n",
    "print(f\"Teacher reasoning: {teacher_output.reasoning[:200]}...\" if hasattr(teacher_output, 'reasoning') else \"No reasoning available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training kwargs optimized for GPU 1\n",
    "train_kwargs = {\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"bf16\": True,\n",
    "    \"optim\": \"adamw_torch_fused\",\n",
    "    \"max_seq_length\": 2048,\n",
    "    \"packing\": False,\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"use_peft\": False,  # Disable PEFT for now\n",
    "}\n",
    "\n",
    "print(\"Starting DSPy BootstrapFinetune...\")\n",
    "optimizer = dspy.BootstrapFinetune(num_threads=16, train_kwargs=train_kwargs)\n",
    "\n",
    "# Use a smaller subset for testing\n",
    "small_trainset = unlabeled_trainset[:10]  # Even smaller for debugging\n",
    "\n",
    "classify_ft = optimizer.compile(\n",
    "    student_classify,\n",
    "    teacher=teacher_classify,\n",
    "    trainset=small_trainset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_ft.save(\"tst\", save_program = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'DSPY_CACHEDIR'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDSPY_CACHEDIR\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:679\u001b[39m, in \u001b[36m__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'DSPY_CACHEDIR'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"DSPY_CACHEDIR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/12 13:45:51 INFO dspy.clients.lm_local: Grabbing a free port to launch an SGLang server for model /home/maxime/.dspy_cache/finetune/c337d0d9b6c05490_aji0y6_meta-llama-Llama-3.2-1B-Instruct_2025-07-12_13-13-30\n",
      "2025/07/12 13:45:51 INFO dspy.clients.lm_local: We see that CUDA_VISIBLE_DEVICES is 1\n",
      "2025/07/12 13:45:51 INFO dspy.clients.lm_local: SGLang server process started with PID 12919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-12 13:45:56] server_args=ServerArgs(model_path='/home/maxime/.dspy_cache/finetune/c337d0d9b6c05490_aji0y6_meta-llama-Llama-3.2-1B-Instruct_2025-07-12_13-13-30', tokenizer_path='/home/maxime/.dspy_cache/finetune/c337d0d9b6c05490_aji0y6_meta-llama-Llama-3.2-1B-Instruct_2025-07-12_13-13-30', tokenizer_mode='auto', skip_tokenizer_init=False, skip_server_warmup=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='/home/maxime/.dspy_cache/finetune/c337d0d9b6c05490_aji0y6_meta-llama-Llama-3.2-1B-Instruct_2025-07-12_13-13-30', chat_template=None, completion_template=None, is_embedding=False, enable_multimodal=None, revision=None, hybrid_kvcache_ratio=None, impl='auto', host='0.0.0.0', port=35121, nccl_port=None, mem_fraction_static=0.879, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=605132822, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, bucket_time_to_first_token=None, bucket_e2e_request_latency=None, bucket_inter_token_latency=None, collect_tokens_histogram=False, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, ep_size=1, enable_ep_moe=False, enable_deepep_moe=False, enable_flashinfer_moe=False, enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, cuda_graph_max_bs=8, cuda_graph_bs=None, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_nccl_nvls=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, disable_overlap_cg_plan=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', hicache_io_backend='', flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, enable_triton_kernel_moe=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, pdlb_url=None, custom_weight_loader=[], weight_loader_disable_mmap=False)\n",
      "[2025-07-12 13:46:01] Attention backend not set. Use flashinfer backend by default.\n",
      "[2025-07-12 13:46:01] Init torch distributed begin.\n",
      "[2025-07-12 13:46:01] Init torch distributed ends. mem usage=0.00 GB\n",
      "[2025-07-12 13:46:01] Load weight begin. avail mem=23.01 GB\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.43it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.43it/s]\n",
      "\n",
      "[2025-07-12 13:46:02] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=20.67 GB, mem usage=2.34 GB.\n",
      "[2025-07-12 13:46:02] KV Cache is allocated. #tokens: 585758, K size: 8.94 GB, V size: 8.94 GB\n",
      "[2025-07-12 13:46:02] Memory pool end. avail mem=1.64 GB\n",
      "[2025-07-12 13:46:02] Capture cuda graph begin. This can take up to several minutes. avail mem=1.16 GB\n",
      "[2025-07-12 13:46:02] Capture cuda graph bs [1, 2, 4, 8]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Capturing batches (bs=8 avail_mem=1.16 GB):   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Capturing batches (bs=8 avail_mem=1.16 GB):  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.33it/s]\n",
      "Capturing batches (bs=4 avail_mem=1.09 GB):  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.33it/s]\n",
      "Capturing batches (bs=4 avail_mem=1.09 GB):  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:00<00:00,  2.32it/s]\n",
      "Capturing batches (bs=2 avail_mem=1.06 GB):  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:00<00:00,  2.32it/s]\n",
      "Capturing batches (bs=2 avail_mem=1.06 GB):  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:01<00:00,  3.11it/s]\n",
      "Capturing batches (bs=1 avail_mem=1.04 GB):  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:01<00:00,  3.11it/s]\n",
      "Capturing batches (bs=1 avail_mem=1.04 GB): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  3.60it/s]\n",
      "Capturing batches (bs=1 avail_mem=1.04 GB): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.94it/s]\n",
      "[2025-07-12 13:46:03] Capture cuda graph end. Time elapsed: 1.38 s. mem usage=0.15 GB. avail mem=1.01 GB.\n",
      "[2025-07-12 13:46:04] max_total_num_tokens=585758, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2288, context_len=131072, available_gpu_mem=1.01 GB\n",
      "[2025-07-12 13:46:04] INFO:     Started server process [12919]\n",
      "[2025-07-12 13:46:04] INFO:     Waiting for application startup.\n",
      "[2025-07-12 13:46:04] INFO:     Application startup complete.\n",
      "[2025-07-12 13:46:04] INFO:     Uvicorn running on http://0.0.0.0:35121 (Press CTRL+C to quit)\n",
      "[2025-07-12 13:46:05] INFO:     127.0.0.1:54166 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "[2025-07-12 13:46:05] INFO:     127.0.0.1:54172 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-07-12 13:46:05] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, #token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, timestamp: 2025-07-12T13:46:05.801853\n",
      "[2025-07-12 13:46:06] INFO:     127.0.0.1:54180 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-07-12 13:46:06] The server is fired up and ready to roll!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/12 13:46:10 INFO dspy.clients.lm_local: Server ready on random port 35121! Logs are available via lm.get_logs() method on returned lm.\n"
     ]
    }
   ],
   "source": [
    "classify_ft.get_lm().launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ DSPy BootstrapFinetune completed!\n",
      "\n",
      "üß™ Testing fine-tuned model...\n",
      "Test input: What if there is an error on the exchange rate?...\n",
      "Predicted label: exchange_rate\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n‚úÖ DSPy BootstrapFinetune completed!\")\n",
    "\n",
    "# Test the result\n",
    "print(\"\\nüß™ Testing fine-tuned model...\")\n",
    "test_text = unlabeled_trainset[0].text\n",
    "result = classify_ft(text=test_text)\n",
    "print(f\"Test input: {test_text[:100]}...\")\n",
    "print(f\"Predicted label: {result.label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'text': 'Which fiat currencies do you currently support? Will this change in this future?', 'label': 'fiat_currency_support'}) (input_keys={'text'})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devset = raw_data[500:600]\n",
    "devset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = (lambda x, y, trace=None: x.label == y.label)\n",
    "evaluate = dspy.Evaluate(devset=devset, metric=metric, display_progress=True, display_table=5, num_threads=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.00 / 20 (65.0%):  19%|‚ñà‚ñà‚ñä            | 19/100 [00:01<00:05, 14.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/12 14:03:36 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 67.00 / 100 (67.0%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 17.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/12 14:03:39 INFO dspy.evaluate.evaluate: Average Metric: 67 / 100 (67.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>example_label</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>&lt;lambda&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which fiat currencies do you currently support? Will this change i...</td>\n",
       "      <td>fiat_currency_support</td>\n",
       "      <td>The question is about the support for fiat currencies, which relat...</td>\n",
       "      <td>fiat_currency_support</td>\n",
       "      <td>‚úîÔ∏è [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I didn't receive my money earlier and it says the transaction is s...</td>\n",
       "      <td>pending_cash_withdrawal</td>\n",
       "      <td>The user is inquiring about a situation where they did not receive...</td>\n",
       "      <td>cash_withdrawal_not_recognised</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what currencies do you accept?</td>\n",
       "      <td>fiat_currency_support</td>\n",
       "      <td>The question is about the currencies that the service accepts, whi...</td>\n",
       "      <td>fiat_currency_support</td>\n",
       "      <td>‚úîÔ∏è [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where can I find your exchange rates?</td>\n",
       "      <td>exchange_rate</td>\n",
       "      <td>The user is inquiring about where to find exchange rates, which re...</td>\n",
       "      <td>exchange_rate</td>\n",
       "      <td>‚úîÔ∏è [True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>why hasnt my card come in yet?</td>\n",
       "      <td>card_arrival</td>\n",
       "      <td>The user is inquiring about the status of their card, specifically...</td>\n",
       "      <td>card_arrival</td>\n",
       "      <td>‚úîÔ∏è [True]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    text  \\\n",
       "0  Which fiat currencies do you currently support? Will this change i...   \n",
       "1  I didn't receive my money earlier and it says the transaction is s...   \n",
       "2                                         what currencies do you accept?   \n",
       "3                                  Where can I find your exchange rates?   \n",
       "4                                         why hasnt my card come in yet?   \n",
       "\n",
       "             example_label  \\\n",
       "0    fiat_currency_support   \n",
       "1  pending_cash_withdrawal   \n",
       "2    fiat_currency_support   \n",
       "3            exchange_rate   \n",
       "4             card_arrival   \n",
       "\n",
       "                                                               reasoning  \\\n",
       "0  The question is about the support for fiat currencies, which relat...   \n",
       "1  The user is inquiring about a situation where they did not receive...   \n",
       "2  The question is about the currencies that the service accepts, whi...   \n",
       "3  The user is inquiring about where to find exchange rates, which re...   \n",
       "4  The user is inquiring about the status of their card, specifically...   \n",
       "\n",
       "                       pred_label   <lambda>  \n",
       "0           fiat_currency_support  ‚úîÔ∏è [True]  \n",
       "1  cash_withdrawal_not_recognised             \n",
       "2           fiat_currency_support  ‚úîÔ∏è [True]  \n",
       "3                   exchange_rate  ‚úîÔ∏è [True]  \n",
       "4                    card_arrival  ‚úîÔ∏è [True]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div style='\n",
       "                text-align: center;\n",
       "                font-size: 16px;\n",
       "                font-weight: bold;\n",
       "                color: #555;\n",
       "                margin: 10px 0;'>\n",
       "                ... 95 more rows not displayed ...\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "67.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(classify_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Debug: Manually inspect the training data\n",
    "\n",
    "Let's manually look at what DSPy is generating for training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's manually inspect the training data file\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# Find the most recent training data file\n",
    "data_files = glob.glob(\"/home/maxime/.dspy_cache/finetune/*.jsonl\")\n",
    "if data_files:\n",
    "    latest_file = max(data_files, key=os.path.getctime)\n",
    "    print(f\"Latest training data file: {latest_file}\")\n",
    "    \n",
    "    # Read first few examples\n",
    "    with open(latest_file, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 3:  # Only show first 3 examples\n",
    "                break\n",
    "            example = json.loads(line)\n",
    "            print(f\"\\n--- Example {i+1} ---\")\n",
    "            print(json.dumps(example, indent=2))\n",
    "else:\n",
    "    print(\"No training data files found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}