{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Hacking DSPy into doing Automatic System Prompt Optimization\"\n",
        "date: 2025-07-21\n",
        "author: Maxime Rivest\n",
        "description: \"In this tutorial, I will show you how to make DSPy optimize a System Prompt Automatically.\"\n",
        "draft: false\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    toc-location: right\n",
        "    code-tools: true\n",
        "    reference-location: margin\n",
        "include-in-header:\n",
        "  text: |\n",
        "    <style>\n",
        "    .cell-output-stdout {\n",
        "      overflow-y: scroll;\n",
        "      max-height: 300px;\n",
        "    }\n",
        "    </style>\n",
        "title-block-banner: false\n",
        "title-block-style: none\n",
        "execute:\n",
        "  echo: true  \n",
        "  #cache: true\n",
        "  #freeze: true\n",
        "---\n",
        "\n",
        "\n",
        "::: {.callout-tip collapse=\"true\"}\n",
        "## Setting up\n",
        "\n",
        "For this tutorial, you will only need to install dspy and setup a LLM connections. I will be using several LLMs to demonstrate how easy it is to switch between them and show the student/teacher concept. You can however set only one up if you want. If you use a locally hosted model, (you can!) simply skip the setting up of the API key. .\n",
        "\n",
        "For this tutorial, I have will use Kimi-K2 hosted by Groq [Click here to get a groq api key](https://console.groq.com/keys) and Llama models from OpenRouter [Click here to get a OpenRouter key](https://openrouter.ai/settings/keys).\n",
        "\n",
        "::: {.callout-note icon=false appearance=\"simple\" collapse=\"true\"}\n",
        "## python library requirements\n",
        "I like to use uv to install my libraries.\n"
      ],
      "id": "8a5381ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "#| code-fold: false\n",
        "#| code-summary: \"\"\n",
        "!uv pip install dspy>=2.6.27"
      ],
      "id": "69c4e125",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.callout-note icon=false appearance=\"simple\" collapse=\"true\"}\n",
        "## api key setup\n",
        "I generally setup my key permanently but you can also do this to set it up just for here and now.\n",
        "\n",
        "```{{python}}\n",
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = \"[REDACTED]\"\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = \"[REDACTED]\"\n",
        "```\n",
        "\n",
        "::: {.callout-note icon=false appearance=\"simple\" collapse=\"true\"}\n",
        "## Make GROQ_API_KEY permanent\n",
        "\n",
        "Replace GROQ_API_KEY with OPENROUTER_API_KEY to set openrouter key permanently on your system.\n",
        "\n",
        "###### Linux / macOS\n",
        "Append to your shell start-up file (pick the one you actually use):\n",
        "\n",
        "```bash\n",
        "echo \"export GROQ_API_KEY='gsk_[REDACTED]'\" >> ~/.bashrc\n",
        "# or ~/.zshrc, ~/.profile, etc.\n",
        "source ~/.bashrc   # reload once\n",
        "```\n",
        "\n",
        "###### Windows – CMD\n",
        "```cmd\n",
        "setx GROQ_API_KEY \"gsk_[REDACTED]\"\n",
        "```\n",
        "Close and reopen the terminal.\n",
        "\n",
        "###### Windows – PowerShell\n",
        "```powershell\n",
        "[Environment]::SetEnvironmentVariable(\"GROQ_API_KEY\", \"gsk_[REDACTED]\", \"User\")\n",
        "```\n",
        "Refresh with `refreshenv` or open a new window.\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "## Making an automatic System Prompt tool\n",
        "\n",
        "In this tutorial, I’ll show you how I’ve modified and customized DSPy to make it handle system prompt optimization. Usually DSPy is doing program optimization. DSPy is very much batteries included, giving you tons of tools for everything. It’s general, and it gives you a framework for how to do things, which is powerful and useful. But that framework is about AI programming, not about system prompt optimization. That is why we will need to do some customization to DSPy. Don't worry, DSPy was built in a way that lets us do it without too much work.\n",
        "\n",
        "The nice thing about having to customize DSPy is that by the end you'll walk away with two things. First, a way to automatically optimize system prompts. Second, you'll have opened the hood: you'll understand better how DSPy works and this will help you use DSPy more proficiently when you actually want to do AI programs.\n",
        "\n",
        "So by the end of this tutorial we will have built this simple yet powerful automatic system prompt optimization utility and understood why we had to do what we did.\n",
        "\n",
        "```{{python}}\n",
        "optimzed_system_prompt = optimize(\n",
        "    training_inputs = [\"User prompt example 1\", \"...\", \"User prompt exampl n\"],\n",
        "    training_outputs = [\"Desirable Assistant's example 1\", \"...\", \"Desirable Assistant's example 1\"],\n",
        "    llm_judge = \"Return a 1 if it's good and a 0 if it's bad.\"\n",
        ")\n",
        "```\n",
        "\n",
        "Our `optimize` function will also be able to optionally take a starting system prompt, a max few-shots, and teacher and student model identifiers. Here is a mock-up of that:\n",
        "\n",
        "```{{python}}\n",
        "optimzed_system_prompt = optimize(\n",
        "    training_inputs = [\"User prompt example 1\", \"...\", \"User prompt exampl n\"],\n",
        "    training_outputs = [\"Desirable Assistant's example 1\", \"...\", \"Desirable Assistant's example 1\"],\n",
        "    llm_judge = \"Return a 1 if it's good and a 0 if it's bad.\",\n",
        "    system_prompt = \"You are a model that perform well...\",\n",
        "    max_few_shots = 2,\n",
        "    teacher_model = \"a-smart-model\",\n",
        "    student_model = \"a-cheap-model\"\n",
        ")\n",
        "```\n",
        "\n",
        "Now that we have our vision, let's get going!\n",
        "\n",
        "## The task\n",
        "\n",
        "All throughout this tutorial our task will be to make an English to Quebec-French translator.\n",
        "\n",
        "The first DSPy optimizer that we want to use is `dspy.MIPROv2`. This optimizer can write (or improve) a program's instructions. Let's analyze the code below to learn what parts we must prepare to reach that goal of running MIPROv2 on task.\n",
        "\n",
        "First we pass `translation_judge` to the optimizer initialisation. This should be a function that must return a score between 0 (bad) to 1 (good). In DSPy these are called metrics. Almost every DSPy optimizer requires a metric. After we have 2 `max_..._demos` which are set to 0, this is because as a first run we would like to only optimise the text of the system prompt without adding few-shot examples. MIPROv2 can search for few-shot examples that would improve a program's performance.\n",
        "\n",
        "```{{python}}\n",
        "optimizer = dspy.MIPROv2(translation_judge, max_bootstrapped_demos = 0, max_labeled_demos = 0)\n",
        "my_program_optimized = optimizer.compile(my_program, trainset=trainset)\n",
        "```\n",
        "\n",
        "Second line of code, inside the `compile` method, we must give a DSPy `program`. This is not a string; it cannot be a system prompt. We will thus need to wrap up our system prompt + user/assistant simple LLM call into a lightweight program. And, finally, we have the trainset. In DSPy, this must be a list of `dspy.Example` objects. This is the object that all of DSPy's internals are using, so there is no way around it; we must format our input/output training set as `dspy.Example`.\n",
        "\n",
        "In summary, we need:\n",
        "1. a metric\n",
        "2. a program\n",
        "3. a training set\n",
        "    \n",
        "and we must format those appropriately.\n",
        "\n",
        "Let's first tackle the training set as it is quite straightforward\n",
        "\n",
        "## Training set\n",
        "\n",
        "The `Example()` object can take any arguments. You can think of those as column names in a dataframe or \"keys\" in JSON. It is usually pretty important to consider these names thoughtfully and normally DSPy will present them to the LLM as part of the prompts. In our case, that is a behavior from DSPy that we will change, so it does not matter what we call them. I decided to go with something very general. The `prompt` will be the user message and the `generation` will be the assistant message.\n"
      ],
      "id": "559b46f5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dspy\n",
        "\n",
        "examples = [\n",
        "    dspy.Example(prompt=\"I'm going to the convenience store.\", generation=\"Je m'en vais au dépanneur.\"),\n",
        "    dspy.Example(prompt=\"It's really cold out today.\", generation=\"Il fait frette en maudit aujourd'hui.\"),\n",
        "    dspy.Example(prompt=\"Can you help me move this weekend?\", generation=\"Tu peux m'aider à déménager ce weekend?\"),\n",
        "    dspy.Example(prompt=\"We were stuck in traffic for two hours.\", generation=\"On était pognés dans le trafic pendant deux heures.\"),\n",
        "    dspy.Example(prompt=\"She's my girlfriend.\", generation=\"C'est ma blonde.\"),\n",
        "    dspy.Example(prompt=\"That car is so cool!\", generation=\"C'est ben l'fun ce char-là!\"),\n",
        "    dspy.Example(prompt=\"I'll call you tonight.\", generation=\"Je vais t'appeler ce soir.\"),\n",
        "    dspy.Example(prompt=\"He's always bragging.\", generation=\"Il se vente tout l'temps.\"),\n",
        "    dspy.Example(prompt=\"We grabbed a coffee at Tim's.\", generation=\"On a pris un café au Tim.\"),\n",
        "    dspy.Example(prompt=\"Close the window, it's chilly.\", generation=\"Ferme la fenêtre, y fait frette.\"),\n",
        "    dspy.Example(prompt=\"I have an appointment at 3.\", generation=\"J'ai un rendez-vous à trois heures.\"),\n",
        "    dspy.Example(prompt=\"They're celebrating their birthday.\", generation=\"Ils fêtent leur fête.\"),\n",
        "    dspy.Example(prompt=\"I parked in the back.\", generation=\"J'ai stationné dans l'fond.\"),\n",
        "    dspy.Example(prompt=\"The metro is packed.\", generation=\"Le métro est plein à craquer.\"),\n",
        "    dspy.Example(prompt=\"We watched a movie last night.\", generation=\"On a écouté un film hier soir.\"),\n",
        "    dspy.Example(prompt=\"I need to do my groceries.\", generation=\"J'dois faire mon épicerie.\"),\n",
        "    dspy.Example(prompt=\"Don't forget your boots.\", generation=\"Oublie pas tes bottes.\"),\n",
        "    dspy.Example(prompt=\"It's snowing again.\", generation=\"Il neige encore.\"),\n",
        "    dspy.Example(prompt=\"I'll take the bus.\", generation=\"J'va prendre l'bus.\"),\n",
        "    dspy.Example(prompt=\"We're out of milk.\", generation=\"On est à court de lait.\"),\n",
        "]"
      ],
      "id": "d28c4be4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we are done with our training set we must do 1 more little thing:\n"
      ],
      "id": "8868a00a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trainset = [x.with_inputs('prompt') for x in examples]"
      ],
      "id": "976e9192",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This, again, is something we have to do because of DSPy's general powerful nature. Briefly, it is used by DSPy's code internally to know what fields of the Example object are input fields for the LLM. It helps internal development to separate inputs from outputs. In our case, we just need to know that we have to do it, and so we do.\n",
        "\n",
        "Let's move on to the Metric now!\n",
        "\n",
        "## Metric\n",
        "\n",
        "Our first metric will be somewhat dumb and a little bit bad. That is because it is hard to have code that measures the quality of a translation. Despite that, we will get pretty good results, you will see.\n",
        "\n",
        "In essence, all this code does is search for some very common French words that are not also common English words. If any of the words are found, the function returns a 1; otherwise it returns a 0.\n"
      ],
      "id": "3d35afe6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import re\n",
        "\n",
        "def is_french(text):\n",
        "    # Naive French detector: check for common French words/accents\n",
        "    french_markers = [\n",
        "        r\"\\b(le|la|les|un|une|des|du|de|et|à|est|sont|avec|pour|sur|par|mais|ou|où|que|qui|quand|comment|nous|vous|ils|elles|ça|ce|cette|ces)\\b\",\n",
        "        r\"[éèêàùçîôâœëïü]\",\n",
        "    ]\n",
        "    return any(re.search(marker, text.lower()) for marker in french_markers)\n",
        "\n",
        "def translation_judge(example, prediction, trace=None):\n",
        "    \"\"\"\n",
        "    Return 1.0 if the output looks French, else 0.0.\n",
        "    Doing the cast explicitly guarantees we never hand DSPy a None.\n",
        "    \"\"\"\n",
        "    output = prediction.get(\"generation\", \"\") or \"\"\n",
        "    try:\n",
        "        return float(is_french(output))\n",
        "    except Exception:\n",
        "        # Anything weird is just a miss\n",
        "        return 0.0"
      ],
      "id": "16c5c98d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how `translation_judge` takes 3 arguments: `example`, `prediction`, and `trace`. \n",
        "\n",
        "* `example` will essentially be an instance of the `Example()` object as we defined above. \n",
        "* `prediction` will be the parsed LLM output. Usually DSPy can do a lot here, but we will modify and simplify that part too.\n",
        "* `trace` can be ignored except when we want models to generate good examples themselves. This is called bootstrapping, and in that case, if `trace` is not `None`, we must return a boolean for whether the LLM-generated example is good (1) or not (0). This could be used, for instance, to make our list of translation pairs longer.\n",
        "\n",
        "Moving on the the program now!\n",
        "\n",
        "## Program\n",
        "\n",
        "The simplest program you can build in DSPy is one with only one input, one output, and empty instructions using `Predict`. A core concept of DSPy is around that signature, but since we do not want to do program optimization I'll not go into it (see [this post](https://maximerivest.com/posts/dspy-one-hour-guide.html) for a simple introduction to DSPy).\n"
      ],
      "id": "f6263d1d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class signature(dspy.Signature):\n",
        "    \"\"\" \n",
        "    \n",
        "    \"\"\"\n",
        "    prompt = dspy.InputField()\n",
        "    generation = dspy.OutputField()\n",
        "\n",
        "initial_program = dspy.Predict(signature)"
      ],
      "id": "d653bde1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The most interesting part for you to note is that `initial_program` is now callable, and if we call it, we will get an LLM response, provided we set up an LLM like this:\n"
      ],
      "id": "46b398fe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "kimi = dspy.LM(\"groq/moonshotai/kimi-k2-instruct\")\n",
        "dspy.configure(lm = kimi)\n",
        "initial_program(prompt = \"Hello, how are you?\")"
      ],
      "id": "5d18f3f4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But we have a few problems.\n"
      ],
      "id": "55416afd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "initial_program.inspect_history()"
      ],
      "id": "741d3d5b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above command prints the previous interaction we had with the LLM. In that interaction, the system prompt was:\n",
        "\n",
        "\n",
        "```{text}\n",
        "Your input fields are:\n",
        "1. `prompt` (str):\n",
        "Your output fields are:\n",
        "1. `generation` (str):\n",
        "All interactions will be structured in the following way, with the appropriate values filled in.\n",
        "\n",
        "[[ ## prompt ## ]]\n",
        "{prompt}\n",
        "\n",
        "[[ ## generation ## ]]\n",
        "{generation}\n",
        "\n",
        "[[ ## completed ## ]]\n",
        "In adhering to this structure, your objective is:\n",
        "```\n",
        "\n",
        "\n",
        "And the user message was:\n",
        "\n",
        "\n",
        "```{text}\n",
        "[[ ## prompt ## ]]\n",
        "Hello, how are you?\n",
        "\n",
        "Respond with the corresponding output fields, starting with the field `[[ ## generation ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
        "```\n",
        "\n",
        "\n",
        "And the assistant was:\n",
        "\n",
        "\n",
        "```{text}\n",
        "[[ ## generation ## ]]\n",
        "I'm doing well, thank you for asking! How can I help you today?\n",
        "\n",
        "[[ ## completed ## ]]\n",
        "```\n",
        "\n",
        "\n",
        "A lot of stuff was added, and if we run an optimizer as it is, we will be optimizing the LLM's performance in that prompt template. This is a little too different from the vanilla we would have expected, which is:\n",
        "`sp = \"\"`,\n",
        "`user = \"Hello, how are you?\"`,\n",
        "and the assistant response could have been something like\n",
        "`assistant = \"I'm doing well, thank you for asking! How can I help you today?\"`.\n",
        "The culprit for the additions is DSPy's adapter. The adapter is amazing at turning a DSPy signature into an AI program, but right now, it's in the way.\n",
        "\n",
        "Let's replace DSPy's adapter with our own simplified version.\n",
        "\n",
        "## Making a Simple Custom Adapter\n",
        "\n",
        "Adapters are DSPy's interface to the LLMs. They are called with a few pieces of information, and DSPy expects a parsed LLM generation to be returned. The following is the simplest we can make an adapter. We are taking in the LM that DSPy's internals want us to use, keyword arguments if any, a signature, demos, and inputs.\n",
        "\n",
        "The signature can contain only 3 things: instructions, inputs, and outputs. In our case, we have \"canned\" the signature, so we also know that the input is named `prompt` and the output is named `generation`, simplifying our requirements for our adapter substantially from what DSPy usually has to worry about.\n"
      ],
      "id": "0da1f163"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define the SimplestAdapter as before\n",
        "class SimplestAdapter(dspy.Adapter):\n",
        "    def __call__(self, lm, lm_kwargs, signature, demos, inputs):\n",
        "        print(inputs)\n",
        "        system_content = signature.instructions\n",
        "        if demos:\n",
        "            system_content\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_content},\n",
        "            {\"role\": \"user\", \"content\": inputs[\"prompt\"]},\n",
        "        ]\n",
        "        outputs = lm(messages=messages, **lm_kwargs)\n",
        "        return [{\"generation\": outputs[0]}]\n",
        "\n",
        "# Do NOT call dspy.configure(adapter=SimplestAdapter())\n",
        "# Subclass Predict to use the custom adapter only for this instance\n",
        "class MyPredict(dspy.Predict):\n",
        "    def forward(self, **kwargs):\n",
        "        adapter = SimplestAdapter()\n",
        "        with dspy.settings.context(adapter=adapter):\n",
        "            return super().forward(**kwargs)"
      ],
      "id": "60625381",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also have to subclass `dspy.Predict` so that we are able to make a program that uses our adapter. Usually in DSPy, the adapter is set globally or within a scoped context, but in both cases, the adapter is applied recursively. This has the effect of making some DSPy programs inside the optimizer use our simple adapter, causing them all to break. And breaking everything is generally not good...\n",
        "\n",
        "\n",
        "## Automatically Generating a System Prompt\n",
        "\n",
        "We are now ready to run the optimizer!!!\n"
      ],
      "id": "60e2b1c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "optimizer = dspy.MIPROv2(translation_judge, max_bootstrapped_demos = 0, max_labeled_demos = 0)\n",
        "my_program_optimized = optimizer.compile(my_program, trainset=trainset, requires_permission_to_run = False)"
      ],
      "id": "da65ca49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test the program right away:\n"
      ],
      "id": "6fd74903"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "my_program_optimized(prompt = \"Hello, how are you?\")"
      ],
      "id": "b87e5723",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Good! It's a translation and not a response to our salutation. Let's inspect the messages.\n"
      ],
      "id": "ef64e7fc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "my_program_optimized.inspect_history()"
      ],
      "id": "569a5578",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And this confirms that our adapter works! this is a completely 'vanilla' set of messages.\n",
        "\n",
        "## What can we optimize in a System Prompt?\n",
        "\n",
        "DSPy’s optimizers work along three levers:\n",
        "\n",
        "1. **Few-shot examples**  \n",
        "   Find or synthesize the best 0-to-N demonstrations to show to show to the LLM. This is a very common optimization technic, DSPy usually add those as user-assistant message pairs and so we will have to do some modifications to get that all into the system prompt's string.\n",
        "\n",
        "2. **Instruction text**  \n",
        "   In DSPy there is a component called `Instructions`, that is a string that is generally added to a system prompt template and that string can be optimized and rewritten by llms. Normally, DSPy does not rewrite the whole system prompt as it keeps some sections of the system prompt to tell the llm about formatting so that DSPy can easily parse the llms output. We will rewrite a custom simplified version of that so the system prompt contains only our system prompt plus, optionally, to few-shot examples, just mentionned.\n",
        "\n",
        "3. **Model weights** (when you’re running an open-weights model)  \n",
        "   When you are running open weights models you can do lora or full model weight fine tuning. We won't get into that here, but if you have the right GPU setup, you would only need to change 1 line of code.\n",
        "\n",
        "Most people only need the first two. DSPy was designed to keep you *inside* its program-centric world, but we’ll trick it into treating the system prompt as the *entire* program.\n",
        "\n",
        "\n",
        "DSPy has quite a reputation for its automatic prompt optimization capability. Despite that, DSPy is relatively hard to use to optimize a system prompt. DSPy is the implementation of a new Paradigm (one where you do not write prompt you rather focus on your program ), so it does not focus on optimizing a prompt but it rather focus on optimizing a program. Although, I very strongly recommend that you learn DSPy's signature, AI programming and the Intent-Oriented Pragramming paradigm DSPy, sometimes you just want a better system prompt. \n",
        "\n",
        "In this tutorial, I will show you how to optimize as system prompt given a trainset set. The system prompt will be rewritten automatically by an LLM in a loop for several steps.\n",
        "\n",
        "\n",
        "\n",
        "## Optimizing a non existent system prompt\n",
        "\n",
        "As a first task, we will start with an empty system prompt and we will have dspy's optimizer deduce the system prompt based on the training set\n",
        "\n",
        "\n",
        "\n",
        "Usually the first thing to do whenever you work with dspy is to first configure you llm connection.\n"
      ],
      "id": "c488ace2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dspy\n",
        "\n",
        "kimi = dspy.LM(\"groq/moonshotai/kimi-k2-instruct\")\n",
        "dspy.configure(lm = kimi)"
      ],
      "id": "588a459c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-tip collapse=\"true\"}\n",
        "## Calling llms in DSPy\n"
      ],
      "id": "fcac3df4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "kimi(\"Hello\")"
      ],
      "id": "318b4c76",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although, convenient. This is never really used, when you are using DSPy according to it's paradigm, in DSPy you would be using and calling a program instead. More like that:\n"
      ],
      "id": "a835b792"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class signature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    You are a Pirate\n",
        "    \"\"\"\n",
        "    prompt = dspy.InputField()\n",
        "    generation = dspy.OutputField()\n",
        "\n",
        "my_program = dspy.Predict(signature) \n",
        "\n",
        "my_program(prompt = \"Hello :)\")"
      ],
      "id": "ff03d72d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is out of scope to explain all about predict and signatures here as my goal is to simply get you to do automatic system prompt optimization. So let's now focus on that.\n",
        "\n",
        ":::\n",
        "\n",
        "For optimization we need a training set. DSPy expects the training set to be a list of `Example` dspy object so we will create our training set like that:\n"
      ],
      "id": "f484fc7b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "examples = [\n",
        "    dspy.Example(prompt=\"I'm going to the convenience store.\", generation=\"Je m'en vais au dépanneur.\"),\n",
        "    dspy.Example(prompt=\"It's really cold out today.\", generation=\"Il fait frette en maudit aujourd'hui.\"),\n",
        "    dspy.Example(prompt=\"Can you help me move this weekend?\", generation=\"Tu peux m'aider à déménager ce weekend?\"),\n",
        "    dspy.Example(prompt=\"We were stuck in traffic for two hours.\", generation=\"On était pognés dans le trafic pendant deux heures.\"),\n",
        "    dspy.Example(prompt=\"She's my girlfriend.\", generation=\"C'est ma blonde.\"),\n",
        "    dspy.Example(prompt=\"That car is so cool!\", generation=\"C'est ben l'fun ce char-là!\"),\n",
        "    dspy.Example(prompt=\"I'll call you tonight.\", generation=\"Je vais t'appeler ce soir.\"),\n",
        "    dspy.Example(prompt=\"He's always bragging.\", generation=\"Il se vente tout l'temps.\"),\n",
        "    dspy.Example(prompt=\"We grabbed a coffee at Tim's.\", generation=\"On a pris un café au Tim.\"),\n",
        "    dspy.Example(prompt=\"Close the window, it's chilly.\", generation=\"Ferme la fenêtre, y fait frette.\"),\n",
        "    dspy.Example(prompt=\"I have an appointment at 3.\", generation=\"J'ai un rendez-vous à trois heures.\"),\n",
        "    dspy.Example(prompt=\"They're celebrating their birthday.\", generation=\"Ils fêtent leur fête.\"),\n",
        "    dspy.Example(prompt=\"I parked in the back.\", generation=\"J'ai stationné dans l'fond.\"),\n",
        "    dspy.Example(prompt=\"The metro is packed.\", generation=\"Le métro est plein à craquer.\"),\n",
        "    dspy.Example(prompt=\"We watched a movie last night.\", generation=\"On a écouté un film hier soir.\"),\n",
        "    dspy.Example(prompt=\"I need to do my groceries.\", generation=\"J'dois faire mon épicerie.\"),\n",
        "    dspy.Example(prompt=\"Don't forget your boots.\", generation=\"Oublie pas tes bottes.\"),\n",
        "    dspy.Example(prompt=\"It's snowing again.\", generation=\"Il neige encore.\"),\n",
        "    dspy.Example(prompt=\"I'll take the bus.\", generation=\"J'va prendre l'bus.\"),\n",
        "    dspy.Example(prompt=\"We're out of milk.\", generation=\"On est à court de lait.\"),\n",
        "]"
      ],
      "id": "35ad30b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You need to tell DSPY what are the input fields. So let loop through all DSPy Example (those we just created) and use the `.with_inputs` method to tell DSPy that `'prompt'` is our input field. Again, DSPy is very general and powerfull but we will use it in a little bit of a strange way here so some things won't be quite natural. In this specific case, `with_inputs` is relevant in dspy because we could use any legal python variable name for our input and we could have used more then one input.\n"
      ],
      "id": "336ca59d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trainset = [x.with_inputs('prompt') for x in examples]"
      ],
      "id": "e3bd854e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our training set we\n"
      ],
      "id": "548a8fba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use MyPredict instead of dspy.Predict\n",
        "class signature(dspy.Signature):\n",
        "    prompt = dspy.InputField()\n",
        "    generation = dspy.OutputField()\n",
        "\n",
        "system_prompt = \" \"\n",
        "initial_system = MyPredict(signature.with_instructions(system_prompt))"
      ],
      "id": "524cb5d7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def format_demos(demos):\n",
        "    \"\"\"\n",
        "    Wrap every demo once – no duplicated header lines.\n",
        "    \"\"\"\n",
        "    parts = [\"Here are examples of your expected behavior.\",\n",
        "             \"<examples>\"]\n",
        "    for i, demo in enumerate(demos, 1):\n",
        "        parts += [\n",
        "            f\"<example_{i}>\",\n",
        "            \"User:\",\n",
        "            demo[\"prompt\"],\n",
        "            \"Assistant:\",\n",
        "            demo[\"generation\"],\n",
        "            f\"</example_{i}>\",\n",
        "        ]\n",
        "    parts.append(\"</examples>\")\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "# Define the SimplestAdapter as before\n",
        "class SimplestAdapter(dspy.Adapter):\n",
        "    def __call__(self, lm, lm_kwargs, signature, demos, inputs):\n",
        "        print(inputs)\n",
        "        system_content = signature.instructions\n",
        "        if demos:\n",
        "            system_content += \"\\n\" + format_demos(demos)\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_content},\n",
        "            {\"role\": \"user\", \"content\": inputs[\"prompt\"]},\n",
        "        ]\n",
        "        outputs = lm(messages=messages, **lm_kwargs)\n",
        "        return [{\"generation\": outputs[0]}]\n",
        "\n",
        "# Do NOT call dspy.configure(adapter=SimplestAdapter())\n",
        "\n",
        "# Subclass Predict to use the custom adapter only for this instance\n",
        "class MyPredict(dspy.Predict):\n",
        "    def forward(self, **kwargs):\n",
        "        adapter = SimplestAdapter()\n",
        "        with dspy.settings.context(adapter=adapter):\n",
        "            return super().forward(**kwargs)\n"
      ],
      "id": "74b735b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "initial_system.parameters(\"instructions\", \"You are a pirate\")"
      ],
      "id": "0e198536",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "initial_system(prompt=\"Hi how are you?\")"
      ],
      "id": "c2c03b68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import re\n",
        "\n",
        "def is_french(text):\n",
        "    # Naive French detector: check for common French words/accents\n",
        "    french_markers = [\n",
        "        r\"\\b(le|la|les|un|une|des|du|de|et|à|est|sont|avec|pour|sur|par|mais|ou|où|que|qui|quand|comment|nous|vous|ils|elles|ça|ce|cette|ces)\\b\",\n",
        "        r\"[éèêàùçîôâœëïü]\",\n",
        "    ]\n",
        "    return any(re.search(marker, text.lower()) for marker in french_markers)\n",
        "\n",
        "def translation_judge(example, prediction, trace=None):\n",
        "    \"\"\"\n",
        "    Return 1.0 if the output looks French, else 0.0.\n",
        "    Doing the cast explicitly guarantees we never hand DSPy a None.\n",
        "    \"\"\"\n",
        "    output = prediction.get(\"generation\", \"\") or \"\"\n",
        "    try:\n",
        "        return float(is_french(output))\n",
        "    except Exception:\n",
        "        # Anything weird is just a miss\n",
        "        return 0.0"
      ],
      "id": "a89112ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer = dspy.MIPROv2(translation_judge, max_bootstrapped_demos = 0, max_labeled_demos = 0)\n",
        "my_program_optimized = optimizer.compile(my_program, trainset=trainset)"
      ],
      "id": "4c204a29",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "my_program_optimized(prompt = \"Hi how are you?\")"
      ],
      "id": "8c403931",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "my_program_optimized.inspect_history()"
      ],
      "id": "f0a8734b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer = dspy.MIPROv2(translation_judge)\n",
        "my_program_optimized = optimizer.compile(my_program, trainset=trainset)"
      ],
      "id": "994142fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "my_program_optimized(prompt = \"Hi how are you?\")"
      ],
      "id": "66454691",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "my_program_optimized.inspect_history()"
      ],
      "id": "b3901586",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer = dspy.SIMBA(metric = translation_judge, bsize = 8)\n",
        "my_program_optimized = optimizer.compile(my_program, trainset=trainset)"
      ],
      "id": "52b208ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "my_program_optimized(prompt = \"Hi how are you?\")"
      ],
      "id": "2086ea88",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "my_program_optimized.inspect_history()"
      ],
      "id": "2d88b973",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "evaluator = dspy.Evaluate(devset = trainset, metric=translation_judge)\n",
        "evaluator(my_program_optimized)"
      ],
      "id": "14655e3d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "evaluator = dspy.Evaluate(devset = trainset, metric=translation_judge)\n",
        "evaluator(my_program)"
      ],
      "id": "067049d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer = dspy.MIPROv2(translation_judge, max_bootstrapped_demos = 2, max_labeled_demos = 2)\n",
        "my_program_optimized_with_demo = optimizer.compile(my_program, trainset=trainset)"
      ],
      "id": "1ca650db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "my_program_optimized_with_demo(prompt = \"Hi how are you?\")"
      ],
      "id": "860fcacf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "my_program_optimized_with_demo.inspect_history()"
      ],
      "id": "b02bec88",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer = dspy.BootstrapFewShotWithOptuna(translation_judge, max_bootstrapped_demos = 10, max_labeled_demos = 10, max = 3)\n",
        "my_program_optimized_with_demo = optimizer.compile(my_program, trainset=trainset, max_demos=3)"
      ],
      "id": "3efeced0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "my_program_optimized_with_demo(prompt = \"Hi how are you today?\")"
      ],
      "id": "141f566a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "my_program_optimized_with_demo.inspect_history()"
      ],
      "id": "dc332aec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class QuebecTranslationJudge(dspy.Signature):\n",
        "    \"\"\"You are an expert Quebec French linguist. For each English sentence and its proposed French translation, evaluate the translation on a scale of 1 to 5 based on the following criteria, with 5 being a perfect, natural-sounding translation.\n",
        "\n",
        "1.  **Accuracy**: Does the French convey the same meaning as the English?\n",
        "2.  **Register**: Is the tone appropriately informal/colloquial (not formal textbook French)?\n",
        "3.  **Regional Vocabulary**: Does it use authentic Quebec French terms (e.g., \"dépanneur\", \"frette\", \"char\")?\n",
        "4.  **Contractions**: Are natural Quebec French contractions used (e.g., \"j'va\", \"t'sais\", \"y fait\")?\n",
        "5.  **Proper Nouns & Anglicisms**: Are names (e.g., \"Tim's\") and common anglicisms (e.g., \"weekend\") handled appropriately for Quebec French?\n",
        "\n",
        "Provide brief feedback on any issues and output only the final numerical score.\n",
        "\n",
        "IMPORTANT IF MEANING IS CHANGED SET TO 0.\n",
        "\"\"\"\n",
        "\n",
        "    english_sentence = dspy.InputField(desc=\"The original sentence in English.\")\n",
        "    french_translation = dspy.InputField(desc=\"The proposed translation in Quebec French.\")\n",
        "    feedback = dspy.OutputField(desc=\"Brief feedback on the translation's quality.\")\n",
        "    score = dspy.OutputField(desc=\"A single integer from 1 to 5.\")\n",
        "\n",
        "# If you have a capable model configured globally, just do this:\n",
        "llm_judge = dspy.Predict(QuebecTranslationJudge)\n",
        "\n",
        "def translation_judge(example, prediction, trace=None):\n",
        "    \"\"\"\n",
        "    An LLM-based metric that judges translation quality.\n",
        "    It robustly parses the score and normalizes it to a 0.0-1.0 scale.\n",
        "    \"\"\"\n",
        "    english_sentence = example.prompt\n",
        "    # Ensure the prediction's output is not empty\n",
        "    french_translation = prediction.get(\"generation\", \"\")\n",
        "    if not french_translation:\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        # Call the LLM judge to get a score\n",
        "        result = llm_judge(\n",
        "            english_sentence=english_sentence,\n",
        "            french_translation=french_translation\n",
        "        )\n",
        "        # Parse the score and normalize it to a 0.0-1.0 range\n",
        "        # (e.g., a score of 5 becomes 1.0, 1 becomes 0.2)\n",
        "        score = float(result.score)\n",
        "        return score / 5.0\n",
        "    except (ValueError, AttributeError, TypeError):\n",
        "        # If the LLM fails to output a valid score, return 0.0\n",
        "        return 0.0"
      ],
      "id": "dfe084a0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer = dspy.BootstrapFewShotWithOptuna(translation_judge, max_bootstrapped_demos = 10, max_labeled_demos = 10, max_rounds = 3)\n",
        "my_program_optimized_with_demo2 = optimizer.compile(my_program, trainset=trainset, max_demos=3)"
      ],
      "id": "4847da64",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "my_program_optimized_with_demo2(prompt = \"Hi how are you today?\")"
      ],
      "id": "d608d2f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "my_program_optimized_with_demo2.inspect_history()"
      ],
      "id": "ed008d0a",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "maximerivest-blog",
      "language": "python",
      "display_name": "Python (maximerivest-blog)",
      "path": "/home/maxime/.local/share/jupyter/kernels/maximerivest-blog"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}